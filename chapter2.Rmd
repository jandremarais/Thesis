# The Multi-Label Framework

> The new plan for this chapter is to introduce all the ml concepts necessary to understand before fitting the model. First we need to define the ML learning objective. Then we will look at ML data set properties. Important: class imbalance, label correlation (should I talk about exploiting it here?). Then we will discuss evaluation metrics. This chapter might become very long.

## Introduction






## Benchmark Data Sets

The progress of areas in machine/statistical learning is highly dependent on the availability of quality and diverse benchmark data sets. This enables researchers to compare their methods in a wide variety of environments. Recently, a decent amount of ML data sets has been published, but not without critique. [@Luaces] argues that the MULAN[^mulan] ML data set repository does not have data sets that are truely ML and that most of the data sets are very similar to each other. Most of the data sets have low cardinality and low label dependence. The problem with this is that these data sets may not show the true performance of ML algorithms. In [@Gibaja2015] the authors also comments on the lack of thorough, comparative empirical studies on these benchmark sets.

[^mulan]: A Java library for ML learning - http://mulan.sourceforge.net/datasets-mlc.html.

Some of the most popular and recent ML benchmark data sets will be introduced here along with their properties. This will give us some form of a reference to compare our data set of satellite images against.

+ [@Read2011a] defines a complexity measure as $n\times p\times K$

+ [@Gibaja2015a] long list of datasets. Other than MULAN: Plant and Human, Slashdot, LangLog, IMDB
+ [@Sorower]
+ https://manikvarma.github.io/downloads/XC/XMLRepository.html
+ yelp dataset: http://www.ics.uci.edu/~vpsaini/
+ also new yt8m

## Sampling and Resampling

+ Simulating [@TorresTomas2014] (also gives citations to other papers)
+ partitioning mentioned in [@Gibaja2015] - referred to [@Sechidis]
+ [@Luaces] Therefore they created a ML data generator to simulate ML data on which algorithms can be evaluated.

## Class Imbalance

+ [@Charte2015]

Maybe include the following headers here:

## Learning Objective

## Evaluation Metrics





### Theoretical Results

+ evaluate perfomance on many metrics for fairness
+ something on label dependence link that will be discussed in next chapter
+ minimisation of surrogate loss functions and consistency
+ consistency [@Zhou2011]:

They were the first to do a theoretical study on the consistency of multi-label learning algorithms, focusing on the ranking loss and the hamming loss. A learning algorithm is said to be consistent if its expected risk converges to the Bayes risk as the size of the training data increases. They found that any convex surrogate loss is inconsistent with the ranking loss and therefore proposed a partial ranking loss (which is consistent with some surrogate loss functions) as an alternative. They also show how some recent multi-label algorithms are inconsistent in terms of the hamming loss and provides a discussion on the consistency of approaches which transforms the multi-label problem into a set of binary classification tasks.

## Label Dependence

