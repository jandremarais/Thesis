# Experiments and Results \label{chp:results}

\chapterprecishere{"For us, the most important part of rigor is better empiricism, not more mathematical theories."\par\raggedleft--- \textup{Ali Rahimi and Ben Recht}, NIPS 2017}

## Introduction

We want all of this experiments to be as reproducible as possible.

Old introduction:

> The main aim of this chapter is to empirically compare some of the deep learning for multi-label image classification approaches proposed in the literature in a standardised fashion. We will also attempt to empirically answer some of the questions that arose in the literature study. 

> Multi-label image classification with CNNs is still a relatively new research area. No work has been done to provide an extensive and robust comparison of the existing approaches in the literature. Typically, when a new approach is proposed it is empirically compared to other previous proposed approaches. But these evaluations of the approaches are not in a standardised fashion. The base networks and optimisation procedures are just some of the learning components that vary accross the proposed approaches. This makes it difficult to determine whether or not a proposed approach performs empirically better than another because of its ability to model multi-label images or because of the latest general developments of training CNNs. 

> Take the Spatial Regularisation Network (SRN) in the previous chapter as an example. The SRN is an extension of a base CNN that is supposed to help exploiting spatial relations amongs labels. The SRN shows favourable empirical results over all other proposed approaches. However, it also uses a much deeper base CNN (ResNet-101) than the other approaches in the literature. This makes it difficult to determine whether or not the performance boost comes from the SRN or the deeper CNN. 

> For this reason, we want to provide a standardised and robust comparison of the some the most promising approaches in the literature. To standardise the comparisons, we will evaluate the chosen approaches using the same base CNN and optimisation procedure. To ensure robustness, we will evaluate the methods on two very distinct multi-label image datasets (described in Appendix \ref{app:data}), using multiple diverse evaluation metrics and using cross-validation for a better estimate of generalisation ability which will also allow us to report standard deviations of errors.

> There are 4 main question we attempt to answer in this chapter. They are:

> 1. How do the different loss functions act as a surrogate for the micro and macro F-score? (bce vs weighted bce vs rank loss vs retina loss)

> 2. Does multi-level predictions help to detect small objects?

> 3. Which extension works best to explicitly model label correlations? CG vs chaining vs SE-module

> 4. How does learnable label calibration modules compare to brute force search?

> After getting closer to the answers of these questions we will train a final model taking into considerations the empirical findings to see how accurate we can get on both datasets.

End of old introduction.

## Evaluation of Approaches?

### Evaluatution Metrics

We chose the following metrics to measure the performance of the model on the data:

- Label-based macro $F_{1}$-score ($F_{1}^{\text{macro}}$),
- Label-based micro $F_{1}$-score ($F_{1}^{\text{micro}}$),
- example-based average precision (AP), and 
- Label-based macro ROC-AUC

By using these four metrics we will get an all-round estimate of the performance the models.
This is a diverse set of metrics.
Includes label-based and example-based metrics, $F$-score, AP and ROC-AUC metrics and micro- and macro-average metrics.
The $F$-score metric variants are popular choices for evaluating MLC models.
The AP metric is common in the Computer Vison domain.
The ROC-AUC is chosen mainly to be able to compare the models to other work reported on this dataset.
ROC-AUC is also a convenient option since it is independent of the classification threshold chosen.
When applicable, we will inspect the performance of the models on a per label basis. 

For the $F$-scores, we will need to threshold the outputs coming from the CNN.
We will not search for the optimal threshold for every experiment.
Rather use a standard threshold of 0.5 when comparing between models.
BUt if we want to squeeze out more accuracy, we will search for the optimal threshold.

When possible, the chosen set of metrics will be reported after each epoch in the form of line graphs.
We do it this way because the point of convergence for the loss function being trained on might not be the same as the metric reported.
Thus, if we only report the performance of the final (converged) model, we might not see the best possible performance for each of the metrics.
The performance of the best (and/or final - I must still choose) models for each training phase will be reported in tabular form.

The final model evaluations will be reported on both the validation and testing sets. 
No model selection will be done on the test set evaluations.

Where possible we will include the time taken to train until convergence. 
Also time taken to make a prediciton for a single image.


### Validation Approach

The data is split into a training, validation and test set.
Since our dataset is large and our computing resources limited, we are comfortable not to use cross-validation.
We can do a cross-validation of the final model to have a better estimate of it's performance.
We will use the exact same split as in (paper) for fairer comparisons.
The split was made randomly by patient in the following ratios: 70% training, 10% validation, 20% testing.
There is no overlap between the patients in different splits to ensure uniqueness of the validation and testing examples.


## Training Procedure

1Cycle policy with learning rate finder.
Adam optimiser.


### How will the policy parameters be chosen? 

See paper. 
Parameters: 

- ratio between minimum and maximum learning rate, 
- decay rate
- momentum decay
- weight decay?


### Fine-Tuning or Global Tuning

Prefer fine-tuning where possible to save time.
Will not be as accurate as global tuning.
Can precompute the activations before the classification head to be tuned, which saves a lot of repetitive computing.
Will do complete global tuning for specific models where appropriate.
Will use appropriate data augmentation techniques when precomputed activations are not used.
Give a more detailed explanation.


## Model Architecture

Will have an initial experiment to compare different architectures.
The majority of the experiments we will run using the smallest version of the chosen architecture type to reduce computational demand.
We assume the conclusions are applicable to the larger models unless specified otherwise.

Sometime we will use smaller versions of the x-ray, say 128x128 to make the experiments run faster.
Which again we assume the same conclusions will hold for the original x-ray sizes.


## Base Experiments

### Validation Split Experiment

Will it bias results to split data randomly?


### Architecture Experiment

Which of the following architectures perform the best on our data ResNets, DenseNets, SEResNet, DarkNet?


### Transfer Learning Experiment

Does it help to do transfer learning vs training from scratch?
Does it help to pretrain on another x-ray based dataset?


## Multi-Label Experiments

### Loss Function Experiment

By training on which loss function will result in the best metrics?

The goal of this experiment is to find out which multi-label loss function is a more suitable surrogate for our chosen multi-label evaluation metrics. We compare the binary cross-entropy loss, weighted binary cross-entropy loss, focal loss and LSEP loss. We only use one ranking based loss function since there was significant proof given in [@Li2017] that LSEP loss outperforms the other ranking based loss functions. We will also experiment with the focal loss weighted as in $\text{W-CE}$. This is the first time the focal loss is used with multi-label image classification and the first time LSEP loss is compared to cross-entropy based loss functions.


### Classification Head Experiment

Which classification head architecture obtains the best results?
Does it learn label dependence?


## Thresholding Experiment

Can the network learn the optimal threshold?


## Other Experiments

### Spatial Pyramid Pooling

Does it help to make predictions from multiple layers of the CNN?



## Summary

Write summary here.









