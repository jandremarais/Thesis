# Convolutional Neural Networks
\label{chp:cnn}

## Introduction

## ConvNet Layers

### Convolutional Layer

### Pooling Layer

+ mixed pool: https://pdfs.semanticscholar.org/de66/4f22dd4c7b4c15ac4a52513004aee55765ff.pdf and https://arxiv.org/pdf/1509.08985.pdf, can try implementation from https://github.com/fchollet/keras/issues/2816
+ maxout?

### Normalisation Layer

### Fully Connected Layer

+ also optiion to replace with conv layer

## ConvNet Architectures

### Layer Patterns

### Layer Sizing Patterns

### Famous Architectures

+ AlexNet
+ VGG

#### Residual Networks

Residual Networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of performance improvement is at a cost of almost double the depth. Therefore training these deep residual networks has a problem of diminishing feature reuse, which make them very slow to train. [@Zagoruyko2016] showed that wide, shallow networks work significantly better than their deeper variants, in terms of accuracy and training time.

See code at: https://github.com/titu1994/Wide-Residual-Networks or https://github.com/asmith26/wide_resnets_keras


+ DenseNet
+ Inception (?)
+ Dirac Nets (?)

## Visualizing CNN's

+ https://github.com/raghakot/keras-vis
+ http://yosinski.com/deepvis

### Activations and First Layer Weights

### Images with Maximum Activation

### t-SNE Embedding

### Occluding

+ more resources: http://cs231n.github.io/understanding-cnn/

## Transfer Learning

### Feature Extractor

### Fine-Tuning

### Pretrained Models

+ probably also something on attention mechanisms for understanding SRNs:

    + Show, attend and tell: Neural image caption generation with visual attention
    + Stacked attention networks for image question answering.
    + Learning transferrable knowledge for semantic segmentation with deep convolutional neural network