---
output: 
  pdf_document:
    citation_package: natbib
bibliography: library.bib
---    

# Multi-Label Convolutional Neural Networks
\label{chp:ml_cnn}

## Introduction

First discuss the domain of MLC in general (only the aspects transferable to Deep Learning) and then move on to look at approaches of ML with ConvNets

Multi-label (ML) learning belongs to the supervised learning paradigm and can be viewed as a generalisation of the traditional single-label learning problem. Suppose the data set to be analysed consists of a set of observations each representing a real-world object such as an image or a text document. In the single-label context each object is restricted to belonging to a single, mutually exclusive class, *i.e.* each observation is associated with a single label. One can quite effortlessly come up with tasks that will not fit into this framework: an image annotation problem where each image contains more than one semantic object, a text classification task where each document has multiple topics or an acoustic classification task where the recordings contain the sounds of multiple bird species. Therefore the need for a ML learner that can assign a set of labels to an observation. Let $\mathcal{L}=\{l_{1},l_{2},\dots,l_{K}\}$ denote the complete set of possible labels that can be assigned to an observation. Whereas a single-label learner aims to find which single label $l_{k}$, $k=1,2,\dots,K$, belongs to a given observation, a ML learner is capable of assigning a set of labels $L \subseteq \mathcal{L}$ to the observation. 

According to [@Zhang2014], ML learning can be considered a sub-problem of a wider framework, called multi-target learning, covering all problems where an observation is associated with multiple outputs. When the output variables are binary, it is a ML learning problem. But problems also exist where the output variables are multi-class or numerical and in these settings the problems are respectively known as multi-dimensional learning and multi-output regression. It is also possible that the output variables are combinations of the aforementioned types. 

As should be expected, the ML framework has a few concepts novel to the single-label case which should be reviewed before looking at the algorithms for ML learning. In this chapter, the core notation for the thesis will be introduced and a clear definition of the task of ML learning will be given. Then, a deep look is taken into the unique properties of ML data and how these might affect the performance of classifiers. The concepts of label correlation and class imbalance will also be introduced, however, how to deal with these will be discussed in the next chapter (for now). Finally, we will get to the evaluation metrics of ML algorithms. This is an important topic in ML learning, often neglected in the literature [cite]. After completing this chapter, the reader will have a good basis to be able to move on to the discussion of ML learning algorithms.

## Notation

The following notation will be used throughout the thesis. Define the input matrix as
$$
X=
\begin{bmatrix}
x_{11} & x_{12} & \dots & x_{1p} \\
x_{21} & x_{22} & \dots & x_{2p} \\
\vdots & \vdots & \ddots & \vdots \\
x_{n1} & x_{n2} & \dots & x_{np}
\end{bmatrix} =
\begin{bmatrix}
\boldsymbol{x}_{1}^{\intercal} & \boldsymbol{x}_{2}^{\intercal} & \dots & \boldsymbol{x}_{n}^{\intercal}
\end{bmatrix},
$$
where $n$ is the number of observations and $p$ is the number of features. $\boldsymbol{x}_{i}^{\intercal}$ represents the $p$-dimensional vector that forms the $i$-th row of $X$. For a text classification problem, $x_{ij}$ might indicate the number of times a word $j$ appeared in document $i$. Define the label or output matrix as 
$$
Y = 
\begin{bmatrix}
y_{11} & y_{12} & \dots & y_{1K} \\
y_{21} & y_{22} & \dots & y_{2K} \\
\vdots & \vdots & \ddots & \vdots \\
y_{n1} & y_{n2} & \dots & y_{nK}
\end{bmatrix} =
\begin{bmatrix}
\boldsymbol{y}_{1}^{\intercal} & \boldsymbol{y}_{2}^{\intercal} & \dots & \boldsymbol{y}_{K}^{\intercal}
\end{bmatrix} = 
\begin{bmatrix}
\boldsymbol{Y}_{(1)} & \boldsymbol{Y}_{(2)} & \dots & \boldsymbol{Y}_{(K)}
\end{bmatrix},
$$
where $K$ is the size of the label set $\mathcal{L}$. $Y$ only contains zeros and ones, *i.e.* $y_{ik}=1$ if label $l_{k}$, $k=1,\dots,K$, is present for observation $i$ and $y_{ik}=0$ if it is absent. Thus $\boldsymbol{Y}_{(k)}$ is a $n$-dimensional binary vector indicating which observations are associated with label $l_{k}$. A ML data set will be defined as $D=\begin{bmatrix} X & Y \end{bmatrix}$, which contains the $n$ input-output pairs, $\{\left(\boldsymbol{x}_{i},\boldsymbol{y}_{i}\right)|i=1,\dots,n\}$. Note that, $\boldsymbol{y}_{i}=(y_{1},y_{2},\dots,y_{K})$, $y_{k}\in \{0,1\}$, used here is the label vector, however, it is also common to use the label set notation, *i.e.* $L_{i} \subseteq \mathcal{L}$, where $\mathcal{L}$ is the complete label set and $L_{i}$ is the set of relevant labels for observation $i$.

## The Task of Multi-Label Learning

A more formal definition of the ML learning task will be given in the following chapter. However, it is important to note that we will define the utlimate task of ML learning as the assigning of multiple labels to an observation. ML learning covers to very similar approaches, namely, ML classification and ML ranking. ML classification algorithms output whether or not labels are relevant to an observation (binary) and ML ranking algorithms outputs a real-valued score assigned to each label indicating its relative importance to an observation. Thus with ML ranking, for each observation we seek a list of labels ordered by their scores representing the confidence in how relevant they are to the specific observation. Many classifiers base their final (categorical) prediction on the thresholding of the real-valued output of the algorithm and thus can also be used for ranking. Similarly, ranking algorithms can also be used for classification if a thresholding function is applied to the real-valued output. (see [@Zhang2014] for a more brief description)

+ mathematical definition with notation of the task of ML learning.
+ real-valued output + thresholding function (ranking vs classification)

The task of ML classification is to find a function $h$ that accurately maps the observations contained in $X$ to the label matrix $Y$, i.e., $h:X\to Y$, so that given a new observation, $h$ can determine which labels belong to it. The accuracy aforementioned is a topic that will be discussed shortly. The measurement thereof is another unique problem for ML classification.

On the other hand, the goal of ML ranking is to find a function $f:X\to G$, where $G$ is a similar matrix to $Y$, but with the $g_{ij}$ a real value representing the relative confidence score that label $j$ is relevant to observation $i$. $f$ is found by optimising a ranking metric, also discussed shortly. From the confidence scores of observation $i$, $f(\boldsymbol{x}_{i})$, a ranking $\boldsymbol{r}_{i}$ can be obtained, giving the rank of labels in descending order of $f(\boldsymbol{x}_{i})$.

> mention the calibration factor of [@Zhang2014]. Finding $z_{i}$ from $r_{i}$

$h$ will be referred to as the ML classifier and $f$ as the ML ranker. When ML learner will be a collective term covering both $h$ and $f$. Before different ML learners can be discussed, an understanding of how the output of these algorithms are evaluated is necessary, since fitting $f$ of $h$ envolves optimising an evaluation metric. (always?)

Let $\mathcal{D}=\{(\boldsymbol{x}_{i},\boldsymbol{y}_{i})\}_{i=1}^{n}$ define a multi-label dataset. $\boldsymbol{x}$, is the feature/input/instance vector of an observation and is given by a $p$-dimensional real-valued vector, $\boldsymbol{x}=(x_{1},x_{2},\dots,x_{p})$, *i.e.* $\boldsymbol{x}\in \mathcal{R}^{p}$. Each instance, $\boldsymbol{x}$ is associated with a subset of labels $L\in 2^{\mathcal{L}}$, where $2^{\mathcal{L}}$ represents the powerset of the full set of labels, $\mathcal{L}=\{l_{1},l_{2},\dots,l_{K}\}$. The subset $L$ is represented as an indicator vector $\boldsymbol{y}=(y_{1},y_{2},\dots,y_{K})$, where $y_{k}=1$ if $l_{k}\in L$ or else $y_{k}=0$, for $k=1,2,\dots,K$. We assume examples in $\mathcal{D}$ to be independently and identically distributed (*i.i.d.*) from $P(\boldsymbol{X},\boldsymbol{Y})$. Let $h$ define a multi-label classifier, which is a mapping, 
$$
h:\boldsymbol{X}\to \boldsymbol{Y}
$$
(not sure about this notation). The risk of $h$ is defined as the expected loss over the joint distribution $P(\boldsymbol{X},\boldsymbol{Y})$:
$$
R_{L}(h)=E_{\boldsymbol{X}\boldsymbol{Y}}\left[L\left(\boldsymbol{Y},h(\boldsymbol{X})\right)\right],
$$
where $L(.)$ is a multi-label loss function. The MLC task boils down to given training data, $\mathcal{D}$, drawn independently from $P(\boldsymbol{X},\boldsymbol{Y})$, learn a classifier $h$ that minimizes the risk with respect to a specific loss function, *i.e.*
$$
h^{*}=\arg\min_{h}E_{\boldsymbol{X}\boldsymbol{Y}}\left[L\left(\boldsymbol{Y},h(\boldsymbol{X})\right)\right]=\arg\min_{h}E_{\boldsymbol{X}}\left[E_{\boldsymbol{Y}|\boldsymbol{X}}\left[L\left(\boldsymbol{Y},h(\boldsymbol{X})\right)\right]\right],
$$
where $h^{*}$ is the so-called risk-minimizing model and can be determined in a pointwise way by the risk minimizer,
$$
h^{*}(\boldsymbol{x})=\arg\min_{\boldsymbol{y}}E_{\boldsymbol{Y}|\boldsymbol{X}}\left[L(\boldsymbol{Y},\boldsymbol{y}))\right].
$$
Note, here we allow $h(\boldsymbol{x})$ to take on real values, *i.e.* $h(\boldsymbol{x})\in\mathcal{R}^{K}$, for the sake of generality. This is to cover multi-label ranking functions and multi-label classifiers that output real real values before thresholding.

### Theoretical Results

+ evaluate perfomance on many metrics for fairness
+ minimisation of surrogate loss functions and consistency
+ consistency [Zhou2011]:

They were the first to do a theoretical study on the consistency of multi-label learning algorithms, focusing on the ranking loss and the hamming loss. A learning algorithm is said to be consistent if its expected risk converges to the Bayes risk as the size of the training data increases. They found that any convex surrogate loss is inconsistent with the ranking loss and therefore proposed a partial ranking loss (which is consistent with some surrogate loss functions) as an alternative. They also show how some recent multi-label algorithms are inconsistent in terms of the hamming loss and provides a discussion on the consistency of approaches which transforms the multi-label problem into a set of binary classification tasks.

+ more theoretical work at [@Gasse2015]. Mentions: Finding theoretically correct algorithms for other non label-wise decomposable loss functions is still a great challenge.

+ more theory: Optimizing the F-Measure in Multi-Label Classification: Plug-in Rule Approach versus Structured Loss Minimization

Other solutions: exploit correlation of labels from both types conditional and unconditional dependencies, features selection methods that are designed especially to handle multi label datasets, and having new stratification methods that are suitable to the nature of multi label datasets (copied from [@Alazaidah2016])

+  most of these algorithms suffer from high complexity in the learning process [10]. Based on that, the true challenge is to exploit high order labels correlations locally and maintain a linear complexity at the same time [2].(copied from [@Alazaidah2016])

## Multi-Label Indicators

As with all supervised learning problems, no one ML algorithm performs optimally on all problems. It is common practice in classical single output supervised learning to first consider, for example, the number of features ($p$) and the number of observations ($n$) in a data set before deciding on which model(s) to fit to the data. The same naturally holds for a ML problem but with added complexity. The multiple outputs of the data introduces many more factors to consider before continuing to the modelling phase. Some ML data sets have only a few labels per observation, while others have plenty. In some ML data sets the number of label combinations is small, whereas in others it can be very large. Some labels appear more frequently than others. Moreover, the labels can be correlated or not. These characteristics can have a serious impact on the performance of a ML classifier. This is the reason why several specific indicators have been designed to assess ML data set properties.

The two standard measures for the multi-labeledness of a data set are *label cardinality* and *label density*, introduced by [@Tsoumakas]. The label cardinality of a ML data, $D$, set is the average number of labels per observation:

$$
LCard(D)=\frac{1}{n}\sum_{i=1}^{n}\sum_{k=1}^{K}y_{ik}.
$$
This measure can be normalised to be independent of the label set size, which results in the label density indicator:

$$
LDens(D)=\frac{1}{K}LCard(D)=\frac{1}{nK}\sum_{i=1}^{n}\sum_{k=1}^{K}y_{ik}.
$$
According to [@Tsoumakas] it is important to distinguish between these two measures, since two data sets with the same label cardinality but with a great difference in the number of labels might not exhibit the same properties and cause different behaviour to the ML classification methods. These two measures give a good indication of the label frequency of a data set, but we are also interested in the uniformity and regularity of the labeling scheme. The authors of [@Read2011a] suggested measuring the proportion of distinct label sets and the proportion of label sets with the maximum frequency. Consider the number of distinct label sets, also referred to as the label diversity [@Zhang2014], which can be defined as:

> there are multiple ways this is defined in the literature - still need to decide on which one I want to use

$$
LDiv(D)=|\{Y|\exists \boldsymbol{x}:(\boldsymbol{x},Y)\in D\}|,
$$
by [@Zhang2014]. ([@Read2011a] uses $\exists !$ instead of $\exists$ and $Y$ as a vector $\boldsymbol{y}$. I want to consider a way of defining it in matrix notation. Maybe with an indicator function. Some papers define it as $DL$ instead of $LDiv$.) The proportion of distinct label sets in $D$ is then

$$
PLDiv\{/PUniq/PDL\}(D)=\frac{1}{n}LDiv(D).
$$

The proportion of label sets with the maximum frequency is defined by [@Read2011a] as:

$$
PMax(D)=\max_{\boldsymbol{y}}\frac{\mathrm{count}(\boldsymbol{y},D)}{n},
$$
where $\mathrm{count}(\boldsymbol{y},D)$ is the frequency that label combination $\boldsymbol{y}$ is found in data set $D$. This represents the proportion of observations associated with the most frequently occurring label sets. High values of $PLDiv$ and $PMax$ indicate an irregular and skewed labeling scheme, respectively, *i.e.* a relatively high number of observations are associated with infrequent label sets and a relatively high number of observations are associated with the most common label sets. (*think about this again*) When this is the case, and the labels are modelled separately, the classifiers will suffer from the class imbalance problem, a common problem in supervised classification tasks. More detail about this will be adressed shortly.

Very little research has been done on how all these ML indicators affect the performance of a ML classifier. [@Chekina2011] made a worthy attempt. Their goal was to find a way of determining which ML algorithm to use given a data set with specific properties and with a specific evaluation metric to optimise. They approached this problem by training a so called meta-learner on a meta-data set containing the performance of multiple ML algorithms on benchmark data sets with different properties. This trained meta-learner is then able to predict which ML algorithm is most likely to give the best results in terms of a specific evaluation metric, given the properties of the data set to be analysed. Although we will not use their meta-learner for this thesis, we will consider some of the additional findings in their research. They found that the following properties (among others) of a ML data set was important to their trained meta-model (which was based on classification trees) in predicting which ML algorithm is most appropriate: $K$; $LDiv(D)$; $LCard(D)$; the standard deviation, skewness and kurtosis of the number of labels per observation in $D$; number of unconditionally dependent label pairs (based on what?); average of $\chi^{2}$ -scores of all dependent label pairs; number of classes with less than 2, 5 and 10 observations; ratio of classes with less than 2, 5, 10 and 50 observations; average, minimal and maximal entropy of labels (def of entropy?); average observations per class. This strengthens the argument that it is important to take ML indicators into account before the training process.

> Some rules that they found that I might refer to later:

+ for micro-AUC target evaluation measure if label cardinality of training data is above 3.028 then the 2BR method (among the single-classifiers) should be used.
+ Another example for an extracted rule is for ranking loss evaluation measure: if minimum of label entropies is zero (i.e. there is at least one certain label in the training set), number of labels is less than 53 and skewness of label cardinality is below or equal to 2.49 then the EPS method (among ensembles) should be used.

## Evaluation Metrics

[@Tsoumakasc] first to categorise into label-based and example-based.

The evaluation of the performance of ML algorithms is another distinct problem to this setting. Compared to the single-label case, many more evaluation metrics exist, with subtle or obvious differences in their measurement. According to [@Madjarov2012] it is essential to evaluate a ML algorithm on multiple and contrasting measures because of the additional degrees of freedom introduced by the ML setting. In addition, care should be taken when reporting multiple measures and with their interpretation. Since some of the measures are contrasting it is dangerous to report multiple metrics and conclude that on average one learner is better than the other. This was highlighted in [@Dembcz2012], where the authors suggested that when evaluating the performance of a ML learner, it should be made clear which metric(s) it is aiming to optimise, otherwise the results can be misleading. It is impossible (?) for a learner to have superior performance over others in terms of all the multi-label evaluation metrics simultaneously.

The evaluation measures of predictive performance of multi-label learnerss can be divided into two groups: example-based and label-based measures. Example-based measures compares the actual versus the predicted labels for each observation and then computes the average across all the observations in the dataset. Where label-based measures computes the predictive performance on each label separately and then averages across all labels [@Madjarov2012]. For both groups the measures can further be partitioned into metrics from a classification persepective and measures from a ranking perspective, *i.e.* metrics for $h$ and metrics for $f$ respectively. The most commonly used metrics in each of the groups will be introduced here.

### Brief Taxonomy

+ more complicated than single label metrics
+ introduce example based vs label based
+ for classification and ranking
+ diagram / table + where they are used

```{r eval-tax, includ = FALSE, eval=FALSE}
grViz('figures/eval-tax.gv') %>% export_svg() %>% 
  charToRaw %>% rsvg %>% png::writePNG('figures/eval-tax.png')
```

![Categorisation of the taxonomy of MLL evaluation metrics \label{fig:eval-tax}](figures/eval-tax.png)

+ \autoref{fig:eval-tax} is just an example. The image quality is lacking.

### Example-based Metrics

+ subset accuracy; hamming loss; accuracy; precision; recall; one-error; coverage; ranking loss; average precision
+ definition + brief interpretaion where it is unclear

For the following definitions, let $y_{i}$ be the set of true labels for observation $\boldsymbol{x}_{i}$ and $z_{i}$ the set of predicted labels for the same observation, obtained from the predicted indicator vector of $\hat{h}(\boldsymbol{x}_{i})$. The Hamming loss is then defined as
$$
\text{hloss}(h)=\frac{1}{n}\sum_{i=1}^{n}\frac{1}{K}|z_{i}\triangle y_{i}|,
$$
where $\triangle$ stands for the symmetric difference and $|.|$, the size of the set. For example, $|\{1,2,3\}\triangle\{3,4\}|=|\{1,2,4\}|=3$. Thus the Hamming loss counts the number of labels not in the intersection of the predicted subset of labels and the true subset of labels, as a fraction of the total size of the labelset, averaged across each observation in the dataset. When $h$ returns perfect predictions for each observation in the dataset, $\text{hloss}(h)=0$, and if $h$ predicts for each observation that it belongs to all the labels except for its the true labels, $\text{hloss}(h)=1$.

Accuracy is defined as
$$
\text{accuracy}(h)=\frac{1}{n}\sum_{i=1}^{n}\frac{|z_{i}\cap y_{i}|}{|z_{i}\cup y_{i}|}.
$$
Thus for each observation the number of correctly predicted labels is calculated as a proportion of the sum of the correctly and incorrectly predicted labels. These quantities are then averaged over each observation in the dataset. If the $h$ perfectly predicts the relevant subset of labels for each observations, $\text{accuracy}(h)=1$. If $h$ does not manage to predict a single correct label for any observation, $\text{accuracy}(h)=0$.

The precision and recall are respectively defined as
$$
\text{precision}(h)=\frac{1}{n}\sum_{i=1}^{n}\frac{|z_{i}\cap y_{i}|}{|z_{i}|},
$$
and
$$
\text{recall}(h)=\frac{1}{n}\sum_{i=1}^{n}\frac{|z_{i}\cap y_{i}|}{|y_{i}|}.
$$
Precision calculates the average proportion of correctly predicted labels in terms of the number of labels predicted, across all the observations in the dataset. Recall calculates a similar average, with the only difference that the proportion is calculated in terms of the number of true labels per observation. Both these metrics lie in the range $[0,1]$ with larger values desirable.

The harmonic mean between the precision and the recall is called the $F_{1}$-score and is defined as
$$
F_{1}=\frac{1}{n}\sum_{i=1}^{n}\frac{2|z_{i}\cap y_{i}|}{|z_{i}|+|y_{i}|}.
$$
The perfect classifier will result in a $F_{1}$-score of 1 and the worst possible score is zero.

The subset accuracy or classification accuracy is defined as
$$
\text{subsetacc}(h)=\frac{1}{n}\sum_{i=1}^{n}I(z_{i}=y_{i}),
$$
where $I(.)$ is the indicator function. This the subset accuracy is the proportion of observations that were perfectly predicted by $h$.

The above are all performance measures of ML classifiers. If the ML learner outputs real-valued confidence scores, these ranking metrics can be used to evaluate the learner's performance:

One-error:

Coverage:

Ranking Loss:

Average Precision:

### Label-based Metrics

+ micro vs macro ito tp, tn, fp, fn
+ auc example

The idea with label-based measures is to compute a single-lable metric for each label based on the number of true positives ($TP$), true negatives ($TN$), false positives ($FP$) and false negatives ($FN$) made by the classifier on a dataset and then obtaining an average of the values [@Gibaja2014]. Note, $TN_{k}$, $TP_{k}$, $FN_{k}$ and $FP_{k}$ denote the quantities for label $l_k$, $k=1,2,\dots,K$. Thus $TP_{k}+TN_{k}+FP_{k}+FN_{k}=n$. Let $B$ be any binary classification metric, i.e. $B\in \{\text{accuracy},\text{precision},\text{recall},F_{1}\}$. $B$ can be written in terms of $TN_{k}$, $TP_{k}$, $FN_{k}$ and $FP_{k}$, for example 
$$
\text{accuracy}(TN_{k}, TP_{k}, FN_{k}, FP_{k})=\frac{TP_{k}+TN_{k}}{TP_{k}+TN_{k}+FP_{k}+FN_{k}}.
$$
$B$ is then calculated for each label and then an average is calulated. The averaging can be done either by the micro or the macro approach. The micro approach considers predictions of all observations together and then calculates the measure across all labels, i.e.
$$
B_{micro}=B\left(\sum_{k=1}^{K}TP_{k},\sum_{k=1}^{K}TN_{k},\sum_{k=1}^{K}FP_{k},\sum_{k=1}^{K}FN_{k}\right).
$$
Whereas the macro approach computes one metric for each label and then the values are averaged over all the labels, i.e.
$$
B_{macro}=\frac{1}{K}\sum_{k=1}^{K}B(TP_{k},TN_{k},FP_{k},FN_{k}).
$$
Note, also that $\text{accuracy}_{micro}(h)=\text{accuracy}_{macro}(h)$ and that $\text{accuracy}_{micro}(h)+\text{hloss}(h)=1$, since Hamming loss is the average binary classification error.

Again, all of the above mentioned metrics are from a classification perspective. An example of a label-based metric from a ranking persepective is the macro- and micro-averaged AUC:

Most multi-label classifiers learn from the training observations by explicity or implicitly optimising one specific metric [@Zhang2014]. That is why in [@Dembcz2012] the authors reccomended specifying which of the metrics a new proposed algorithm aims to optimise in order to show if it is succesful. But at the same time it is important to test the algorithm on numerous metrics for fair comparisons against other algorithms [@Zhang2014], [@Madjarov2012]. It might be that a algorithm does very well in terms of the Hamming loss, but performs poorly according to the subset accuracy, or vice versa, as shown in [@Dembcz2012]. In [@Tsoumakasc] they claim that the Hamming loss reported together with the micro-average $F$-measure gives a good indication of the performance of a multi-label classifier.

These multi-label metrics are usually non-convex and discontinuous [@Zhang2014]. Therefore multi-label classifiers resort to considering surrogate metrics which are easier to optimise. 

> probably should add an example or maybe later

Other than predictive performance, are there other aspects on which multi-label classifiers can be evaluated, such as efficiency and consistency. Multi-label algorithms should be efficient in the sense that it takes the least amount of computational power for a given level of predictive performance [@Madjarov2012]. These classifiers can take a considerable amount of time to train when complicated ensembles are being implemented on datasets with huge labelsets. In cases where live updating and predictions are needed, this may be a problem [reference]. The other desirable attribute of multi-label classifiers are that they are consistent. This means that the expected loss of the classifier converges to the Bayes loss when the number of observations in the training set tends to infinity. Actually only a very few number of multi-label classifiers satisfy this property [Zhou2011], [@Koyejo2015].

## Label Dependence

With this chapter I want to investigate the need for approaches in multi-label classification which model the dependence structure between labels. For this we need a sound theoretical definition and analysis of label dependence and then we might want to investigate it empirically with synthetic datasets (or real world). The main papers inspiring this chapter are [@Dembcz2012] and [@Reade], and some content will be taken from [@Readd], [@Madjarov2012] (for empirical evidence maybe), [@Readb], [@Dembczyski2010], [@Dembczynski2012]. My main hypothesis is that modelling the input-output pairs individually should have just as good, if not better performance compared to approaches trying to model label dependence, since all the available information of the labels should be contained in $X$ and by the assumption that label $y_{i}$ can be determined with the help of the knowledge of label $y_{j}$, it should also be possible to find $y_{i}$ from $X$ since $y_{j}$ is found from $X$. This argument probably only holds for approaches trying to "correct" binary relevance (BR) with regards to its lack of modelling label dependence, such as classifier chains (CC), stacking like MBR/2BR/BR+, etc. Reformulate hypothesis later.

It is essentially a given in multi-label classification literature that in order to obtain competitive results, a learner should be able to model the dependence structure between labels in some way. Whenever a new MLC algorithm is proposed, it will be compared to independent label learning (BR) and if it has superior empirical performance, it is usually ascribed to its ability of modelling label dependence in some ad-hoc way (examples?). The authors of [@Dembczyski2010], [@Dembczynski] and [@Dembczyski] were the first to point out this lack of understanding of the term *label dependence*  in the literature (later on a comprehensive and extended discussion of the topics covered in the aforementioned papers was given in [@Dembcz2012]). They argued that *label dependence* is only understood and used by most in the literature in a purely intuitive manner, and that in order to build a better understanding of multi-label classifiers, theoretical backing is essential.

Modelling each label independently, *i.e.* using the binary relevance (BR) approach, is one of the simplest and most intuitive approaches to tackling the multi-label problem. But it has been criticized and overlooked by the majority because it does not take into account the possible dependence between labels. However, BR has many advantages. [@Dembcz2012] shows that BR is the risk minimizer of the Hamming Loss and [@Readd] pointed out that it is very rare for 'improved' methods to achieve significantly better results than BR in terms of this measure (also visible in [@Madjarov2012] (make sure)). In addition, BR is highly resistant to overfitting label combinations, since it does not expect samples to be associated with previously-observed combinations of labels [Read2011a]. It can naturally handle data streaming or other dynamic scenarios where the addition and removal of labels are quite common. BR's biggest strength is its low computational complexity compared to other multi-label classification methods. It scales linearly with increasing number of labels and it is easily parallelizable - desirable properties, especially working with large label sets.

Recently, [@Reade] has gone so far as to claim that BR can perform just as well as methods supposedly modelling label dependence, and if it does not, it is usually because of the inadequacy of the base learners used. In other words, if the base learner can extract the right features, BR will be as good as any other multi-label classifier, without the need to model label dependence. Some theoretical justifications were given but the empirical evidence was not convincing. This is what motivated the writing of this chapter - to answer the question, "is it essential for a multi-label classifier to take label correlations into account in order to be optimal?". To investigate this one needs a thorough, theoretical understanding of *label dependence*, how to possibly exploit it and how to evaluate it. This is what this chapter aims to do. Most of the work is based on the papers [@Dembcz2012] and [@Reade]. We will also attempt to back up the theory with empirical results.

### Two types of label dependence

As mentioned, most mutli-label learning papers display merely an intuitive understanding of *label dependence*, in the sense that in predicting a specific label, the information on the rest of the labels may be helpful. For example in an image recognition problem, if a picture is labelled with *beach* and *ocean*, *sand* will most likely be a relevant label. Clearly, this understanding is insufficient to gain advances in the multi-label learning literature (later on it will also be pointed out why this may indeed not make intuitive sense). In this section, a formal statistical definition of the two types of label dependence will be given. First, we briefly revisit the task of multi-label classification (MLC), in mathematical(?) terms.

#### Marginal vs. conditional dependence

First note that we denote the conditional distribution of $\boldsymbol{Y}=\boldsymbol{y}$ given $\boldsymbol{X}=\boldsymbol{x}$ as
$$
P(\boldsymbol{Y}=\boldsymbol{y}|\boldsymbol{X}=\boldsymbol{x})=P(\boldsymbol{y}|\boldsymbol{x})
$$
and the corresponding conditional marginal distribution of $Y_{k}$ (conditioned on $\boldsymbol{x}$) as
$$
P(Y_{k}=b|\boldsymbol{x})=\sum_{y_{i}=b}P(\boldsymbol{y}|\boldsymbol{x}).
$$
(can probably also write as $P(Y_{k}|\boldsymbol{x})$ since $b$ is either 0 or 1?)

[@Dembcz2012] defines two types of dependence among lables, namely, conditional dependence and marginal dependence. Their definitions follow:

\newtheorem{thm}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{exmp}{Example}

\begin{defn}
A random vector of labels $\boldsymbol{Y}=(Y_{1},Y_{2},\dots,Y_{K})$ is called marginally independent if 

\begin{equation}
\label{eq-mdep}
P(\boldsymbol{Y})=\prod_{k=1}^{K}P(Y_{k}).
\end{equation}\\
\end{defn}

Marginal dependence is also known as unconditional dependence and can be thought of as a measure of the frequency of co-occurrence among labels. Conditional dependence captures the dependence of the labels given a specific observation $\boldsymbol{x}$.  
  
\begin{defn}
A random vector of labels is called conditionally independent, given $\boldsymbol{x}$ if 

\begin{equation}
\label{eq-cdep}
P(\boldsymbol{Y}|\boldsymbol{x})=\prod_{k=1}^{K}P(Y_{k}|\boldsymbol{x}).
\end{equation}\\
\end{defn}

The conditional joint distribution of a random vector $\boldsymbol{Y}=(Y_{1},Y_{2},\dots,Y_{K})$ can be expressed by the product rule of probability ($P(AB)=P(A|B)P(B)$):

\begin{equation}
\label{eq-jointdist}
P(\boldsymbol{Y}|\boldsymbol{x})=P(Y_{1}|\boldsymbol{x})\prod_{k=2}^{K}P(Y_{k}|Y_{1},\dots,Y_{k-1},\boldsymbol{x}).
\end{equation}

A similar expression can be given for $P(\boldsymbol{Y})$. If $Y_{1},Y_{2},\dots,Y_{K}$ are conditionally independent, then \autoref{eq-jointdist} will simplify to \autoref{eq-cdep}.

Marginal and conditional dependence are closely related - it can be written as:

\begin{equation}
\label{eq-cdep-mdep}
P(\boldsymbol{Y})=\int_{\mathcal{X}}P(\boldsymbol{Y}|\boldsymbol{x})d\mu(\boldsymbol{x}),
\end{equation}

where $\mu$ is the probability measure on the input space $\mathcal{X}$ induced by the joint probability distribution $P$ on $\mathcal{X}\times\mathcal{Y}$. Marginal dependence can roughly be viewed as an 'expected dependence' over all instances. Nevertheless, marginal dependence does not imply conditional independence, or *vice versa*. Two examples from [@Dembcz2012] are given to illustrate this.  
  
\begin{exmp}
Suppose two labels, $Y_{1}$ and $Y_{2}$, are independently generated from $P(Y_{k}|\boldsymbol{x})=\left(1+\exp(-\phi f(\boldsymbol{x})\right)^{-1}$, where $\phi$ controls the Bayes error rate. Thus, by definition, the two labels are conditionally independent with conditional joint distribution, $P(\boldsymbol{Y}|\boldsymbol{x})=P(Y_{1}|\boldsymbol{x})\times P(Y_{2}|\boldsymbol{x})$. However, as $\phi\to\infty$, the Bayes error tends to zero and the marginal dependence increases to an almost deterministic case of $y_{1}=y_{2}$. Showing, conditional independence does not imply marginal independence.\\
\end{exmp}  
\begin{exmp}
Suppose two labels, $Y_{1}$ and $Y_{2}$, are to be predicted by using a single binary feature, $x_{1}$. Let the joint distribution $P(X_{1},Y_{1},Y_{2})$ be given by the following table:

```{r, echo=FALSE, results = "asis", fig.align = 'center'}
x <- rep(c(0, 1), each = 4)
y1 <- rep(rep(c(0, 1), each = 2), 2)
y2 <- rep(c(0, 1), 4)
P <- c(0.25, 0, 0, 0.25, 0, 0.25, 0.25, 0)

library(xtable)
options(xtable.floating = TRUE)
options(xtable.comment = FALSE)
xtab <- xtable(data.frame(x,y1,y2,P), digits = c(0, 0, 0, 0, 2), align = "ccccc")
colnames(xtab) <- c("$x_{1}$", "$y_{1}$", "$y_{2}$", "$P$")
print(xtab, include.rownames = FALSE, sanitize.text.function = function(x) {x})

```

Thus, the labels are not conditionally independent, 
$$
P(Y_{1}=0,Y_{2}=0|x_{1}=1)=0\neq P(Y_{1}=0|x_{1}=1)\times P(Y_{2}=0|x_{1}=1)=0.25\times 0.25,
$$
but it can be shown that they are indeed marginally independent. For example,
$$
P(Y_{1}=0,Y_{2}=0)=0.25=P(Y_{1}=0)\times P(Y_{2}=0)=0.5\times 0.5. 
$$
This holds for all the combination of labels, showing that marginal independence does not imply conditional independence.
\end{exmp}

This distinction between marginal and conditional dependence is crucial in the attempt to model label dependence in multi-label classification. We describe a multi-output model with the following notation, similar to [@Hastie2009]:

\begin{equation}
\label{eq-multiout}
Y_{k}=h_{k}(\boldsymbol{X})+\epsilon_{k}(\boldsymbol{X}),
\end{equation}

for all $k = 1,2,\dots,K$. $h_{k}:\boldsymbol{X}\to\{0,1\}$ will be referred to as the structural part and $\epsilon_{k}(\boldsymbol{x})$ as the stochastic part of the model. Note that a common assumption in multi-variate regression (real-outputs) is that

\begin{equation}
\label{eq-experr}
E[\epsilon_{k} (\boldsymbol{x})]=0.
\end{equation}

for all $\boldsymbol{x}\in\boldsymbol{X}$ and $k=1,2,\dots,K$. This is not a reasonable assumption in mutli-label classification [@Dembcz2012] - the distribution of the noise terms can depend on $\boldsymbol{x}$ and two or more noise terms can depend on each other. Classifier $h_{k}$ might also be very similar to $h_{l}$, $l\neq k;l=1,2,\dots,K$. Thus there are two possible sources of label dependence: the structural part and the stochastic part of the model.

It seems that marginal dependence between labels is caused by the similarity between the structural parts. This assumption is made since it is reasonable to assume that the structural part will dominate the stochastic part. Suppose there exists a function $f(.)$ such that $h_{k}\approx f\circ h_{l}$, *i.e.*

\begin{equation}
\label{eq-fdep}
h_{k}(\boldsymbol{x})=f(h_{l}(\boldsymbol{x})) + g(\boldsymbol{x}),
\end{equation}

with $g(.)$ being negligible in the sense that $g(\boldsymbol{x})=0$ with high probability. Then this $f(.)$-*dependence* between the classifiers is likely to dominate the averaging process in \autoref{eq-cdep-mdep}, compared to $g(.)$ and the stochastic parts. This is what happens in Example 1 when $\phi \to \infty$. Thus we see that even if the dependence between $h_{k}$ and $h_{l}$ is only probable, it can still induce a dependence between the labels $Y_{k}$ and $Y_{l}$ (verstaan nie presies wat hier bedoel word nie). Another example illustrating idea is given from [@Dembcz2012].  

\begin{exmp}
Consider a problem with a 2-dimensional input $\boldsymbol{x}=(x_{1},x_{2})$, where $x_{i}$ is uniformly distributed in $[-1,1]$ for $i=1,2$, and two labels, $Y_{1},Y_{2}$, determined as follows. $Y_{1}$ is set to 1 for all positve values of $x_{1}$, i.e. $Y_{1}=I(x_{1}>0)$. The second label is generated similarly but with the decision boundary of $Y_{1}$ ($x_{1}=0$) rotated by an angle of $\alpha\in [0, \pi]$ (give illustration). In addition, let the two error terms of the model be independent and both flip the label with a probability of $0.1$. If $\alpha$ is close to zero, the labels will almost be identical and a high correlation will be observed between them. But if $\alpha = \pi$, the decision boundaries of the labels are orthogonal and a low correlation will be observed.\\
\end{exmp}

With regards to \autoref{eq-fdep}, in Example 3, $f(.)$ is the identity function and $g(.)$ given by the $\pm 1$ in the regions between the decision boundaries. From this point of view, marginal dependence can be seen as a kind of soft constraint that a learning algorithm can exploit for the purpose of regularization [@Dembcz2012]. (verstaan nie wat dit beteken nie)

For the conditional dependence, it seems that the stochastic part of the model is the cause. In Example 3, $Y_{1}$ and $Y_{2}$ is conditionally independent because the error terms are assumed to be independent. However, if there is a close relationship between $\epsilon_{1}$ and $\epsilon_{2}$, this conditional independence will be lost. [@Dembcz2012] proves the proposition that a vector of labels is conditionally dependent given $\boldsymbol{x}$ if and only if the error terms in \autoref{eq-multiout} are conditionally dependent given $\boldsymbol{x}$, *i.e.*
$$
E\left[\epsilon_{1}(\boldsymbol{x})\times \dots \times \epsilon_{K}(\boldsymbol{x}) \right]\neq E\left[\epsilon_{1}(\boldsymbol{x})\right]\times\dots\times E\left[\epsilon_{K}(\boldsymbol{x})\right].
$$

(Include proof?) It should also be noted that conditional independence can also cause marginal dependence because of \autoref{eq-cdep-mdep}. Thus the similarity between models is not the only source of of marginal dependence. 

What we have learned thus far is that there is a difference between marginal and conditional label dependence. The presence of marginal dependence does not imply conditional label dependence and *vice versa*. If label correlations are observed it can only be assumed that marginal dependence between the labels exist. It does not necessarily imply that there are any dependencies among the error terms (although it could be the cause). On the other hand, if conditional dependence is observed, one can safely assume that there are dependencies among the error terms. Next, we see how to exploit both types of label dependence to improve predictive accuracy.  

### Link between label dependence and loss minimization

One can view the MLC task from different persepectives in terms of loss minimizations. [@Dembcz2012] describes three such views, determined by the type of loss function to be minimized, the type of dependence taken into account and the distinction between marginal and joint distribution estimation. The three views and the main questions to consider for each of them are:

1. The individual label view: How can we improve the predictive accuracy of a single label by using information about other labels?
2. The joint label view: What type of non-decomposable MLC loss functions is suitable for evaluating a multi-label prediction as a whole and how to minimize such loss functions?
3. The joint distribution view: Under what conditions is it reasonable to estimate the joint conditional probability distribution over all label combinations?

#### The individual label view

With this view, the goal is to minimize a loss function that is label-wise decomposable and we want to determine whether or not it will help taking label relationships into account. The most common and intuitive label-wise decomposable loss function is the Hamming loss, which is defined as the fraction of labels whose relevance is incorrectly predicted:

\begin{equation}
\label{eq-hloss}
L_{H}\left(\boldsymbol{y}, \hat{\boldsymbol{y}}\right)=\frac{1}{K}\sum_{k=1}^{K}I\left(y_{k}\neq \hat{y}_{k}\right).
\end{equation}

\autoref{eq-hloss} is only the Hamming loss for one observation. To compute the Hamming loss over an entire dataset, \autoref{eq-hloss} is averaged over all the observations. 

It is easy to see that the Hamming loss is minimized when

$$
\hat{\boldsymbol{y}}=(\hat{y}_{1},\dots,\hat{y}_{K}),
$$

where 
$$
\hat{y}_{k}=\arg\max_{y_{k}\in\{0,1\}}p(y_{k}|\boldsymbol{x}),
$$
for $k=1,2,\dots,K$. This shows that it is enough to take only the conditional marginal distribution $P(Y_{k}|\boldsymbol{x})$ into account to solve the problem, at least on a population level. Thus the Hamming loss is minimized by BR. [@Dembcz2012] also gives a similar result for label-wise decomposable loss functions in general (thus also relevant for F-measure, AUC, *etc.*). This result implies that the multiple single label predictions problem can be solved on the basis of $P(Y_{k}|\boldsymbol{x})$ alone. Hence, with a proper choice of base classifiers and parameters for estimating the conditional marginal probabilities, there is in principle no need for modelling conditional dependence between the labels. However, in cases where the base classifiers are inadequate, dependence between the errors will exist and BR will give a suboptimal solution (make sure this statement is used correctly). Methods exist to improve BR in these situations and will be discussed shortly.

#### The joint label view

Here we are interested in non-decomposable (label-wise) MLC loss functions such as rank loss and the subset 0/1 loss. We discuss when they are appropriate and how to minimize them. First, consider the rank loss. Suppose the true labels constitute a ranking in which all relevant labels ideally precede all irrelevant ones and $\boldsymbol{h}(\boldsymbol{x})=(h_{1}(\boldsymbol{x}),\dots,h_{K}(\boldsymbol{x}))$ is seen as a ranking function representing a degree of label relevance sorted in a decreasing order. The rank loss simply counts the number of label pairs that disagree in these two rankings:

\begin{equation}
\label{eq-rloss}
L_{r}\left(\boldsymbol{y},\boldsymbol{h}(\boldsymbol{x})\right)=\sum_{(k,l);y_{k}>y_{l}}\left(I\left(h_{k}(\boldsymbol{x})<h_{l}(\boldsymbol{x})\right)+\frac{1}{2}I\left(h_{k}(\boldsymbol{x})=h_{j}(\boldsymbol{x})\right)\right).
\end{equation}

This function is not convex nor differentiable, thus an alternative would be to minimize a convex surrogate like the hinge or exponentional function. However, [@Dembcz2012] proves that it is enough to minimize \autoref{eq-rloss} by sorting the labels by their probability of relevance:

\begin{thm}
A ranking function that sorts the labels according to their probability of relevance, i.e. using the scoring function $\boldsymbol{h}(.)$ with $h_{k}(\boldsymbol{x})=P(Y_{k}=1|\boldsymbol{x})$, minimizes the expected rank loss.\\
\end{thm}

(include proof?) This implies again (just like in the case for the label-wise decomposable loss functions) that, in principle, it is not necessary to know the joint label distribution $P(\boldsymbol{Y}|\boldsymbol{x})$ when training a multi-label classifier, *i.e.* risk-minimizing predictions can be made without any knowledge about the conditional dependency between labels. Thus, to minimize the rank loss, one can simply use any approach minimizing the single label losses. Note this results does not hold for the normalized version of rank loss.

Next, we look at the extremely stringent multi-label loss function, the subset 0/1 loss:

\begin{equation}
\label{eq-s01}
L_{S}\left(\boldsymbol{y},\hat{\boldsymbol{y}}\right)=I\left(\boldsymbol{y}\neq \hat{\boldsymbol{y}}\right).
\end{equation}

Although most would agree that this is not a fair measure for MLC performance, since it does not disinguish between almost correct and completely wrong, it is still interesting to study with regards to exploiting label dependence. The risk-minimizing prediction for \autoref{eq-s01} is given by the mode of the distribution: 

\begin{equation}
\label{eq-smin}
h_{s}^{*}(\boldsymbol{x})=\arg\max_{\boldsymbol{y}}P(\boldsymbol{Y}|\boldsymbol{x}).
\end{equation}

This implies that the entire distribution of $\boldsymbol{Y}$ given $\boldsymbol{X}$ is needed to minimize the subset 0/1 loss. Thus a risk minimizing prediction requires the modelling of the joint distribution and hence the modelling of the conditional dependence between labels. Later on we will show an important results that under independent outputs, minimizing the Hamming loss and the subset 0/1 loss is equivalent, implying that BR will indeed also minimize the subset 0/1 loss (consider to show it here).

The cases for F-measure loss and the Jaccard distance is a bit more complicated and will not be discussed here. (give citation of where this can be found)

#### The joint distribution view

> not sure if I want to mention the joint distribution view. Maybe only distinguish between single label and joint label prediction approach.

We just saw that minimzing the subset 0/1 loss requires the estimation of the entire conditional joint distribution, $P(\boldsymbol{Y}|\boldsymbol{X})$. Generally, if the joint distribution is known, a risk-minimizing prediction can be derived for any loss function in an explicit way:

$$
h^{*}(\boldsymbol{x})=\arg\min_{\boldsymbol{y}}E_{\boldsymbol{Y}|\boldsymbol{x}}\left[L(\boldsymbol{Y},\boldsymbol{y})\right].
$$

In some applications modelling the joint distribution may result in using simpler classifiers, potentially leading to a lower cost and a better performance compared to directly estimating marginal probabilities by means of more complex classifiers. Nevertheless, it remains a difficult task. One has to estimate 2^{K} values to estimate for a given $\boldsymbol{x}$.

### Previous attempts to 'exploit' label dependence

### Improved attempts to 'exploit' label dependence


**Theoretical insights into MLC**

+ when new MLC algorithm is introduced, it should be specified which loss functions it intends to minimize. Otherwise it may give misleading results (like that it is optimal for many loss functions).
+ a classifier supposed to be good in solving one problem may perform poorly on a different problem and vice versa
+ restricts attention to hamming loss and subset 0/1 loss.
+ hamming is representative of single label scenario and subset 0/1 for the mutli-label loss.
+ assumes unconstrained hypothesis space.
+ proposition (with proof in paper): The hamming loss and subset 0/1 loss have the same risk-minimizer, *i.e.* $\boldsymbol{h}_{H}^{*}(\boldsymbol{x})=\boldsymbol{h}_{s}^{*}(\boldsymbol{x})$, if one of the following conditions holds: (1) Labels $Y_{1},\dots,Y_{K}$ are conditionally independent, *i.e.* $P(\boldsymbol{Y}|\boldsymbol{x})=\prod_{k=1}^{K}P(Y_{k}|\boldsymbol{x})$. (2) The probability of the mode of the joint probability is greater than or equal to 0.5, *i.e.* $P(\boldsymbol{h}_{S}^{*}(\boldsymbol{x})|\boldsymbol{x})\geq 0.5$.
+ corollary (with proof in paper): In the separable case (*i.e.* the joint conditional distribution is deterministic, $P(\boldsymbol{Y}|\boldsymbol{x})=I(\boldsymbol{Y}=\boldsymbol{y})$), the risk minimizers of the hamming loss and subset 0/1 loss coincide.
+ Then 3 propositions on upper bounds of these losses. Ponder its relevance.

**MLC algorithms for exploiting label dependence**

+ proposed algorithms improve predictive performance by supposedly modelling label dependence.
+ type of dependence and loss to be optimized is omitted
+ leads to poor designs and misleading results.
+ focus on PT methods
+ discussion on BR:
+ simplest. does not take marginal or conditional dependence into account.
+ in general not able to yield risk-mininizing predictions for multi-label losses but is well suited for loss functions whose risk-minimizer can solely be expressed in terms of marginal (conditional) distributions.
+ may be sufficient, but exploiting marginal dependencies may still be beneficial especially for small-sized problems.
+ moves discussion to single label predictions
+ several methods that exploit similarities between structural parts of the label models.
+ general scheme: 

\begin{equation}
\label{eq-sl}
\boldsymbol{y}=\boldsymbol{b}(\boldsymbol{h}(\boldsymbol{x}), \boldsymbol{x}),
\end{equation}

where $\boldsymbol{h}(\boldsymbol{x})$ is the binary relevance learner and $\boldsymbol{b}(.)$ is an additional classifier that shrinks or regularizes the solution of BR. Or

\begin{equation}
\label{eq-sl-inv}
\boldsymbol{b}^{-1}(\boldsymbol{y},\boldsymbol{x})=\boldsymbol{h}(\boldsymbol{x}),
\end{equation}

where the output space is first transformed and then the BR classifiers are trained and then transformed back to original.
+ Stacking follows first scheme. Form of regularization or feature expansion. Not clear which inputs should all be use for second level.
+ multivariate regression
+ kernel dependency estimation
+ compressive sensing
+ next section on methods that seek to estimate the joint distribution $P(\boldsymbol{Y}|\boldsymbol{x})$.
+ LP. Largest drawback the number of label combinations
+ the literature usually claims LP is generally the rigth approach. FALSE. LP takes conditional dependence into account but usually fails for losses like Hamming.
+ can improve with RAKEL, but it is still not well understood from a theoretical point of view.
+ PCC. computationally more manageable. ECC to reduce importance of label chain order.

**Experimental evidence**

+ real and synthetic data
+ BR, SBR, CC, LP
+ hamming loss and subset 0/1
+ MULAN
+ logistic regression for base classifier.
+ marginal independence: stacking does improve on BR, CC similar to SBR, LP also bad. Error increases with number of labels. hamming and subset 0/1 coincide.
+ conditional independence: again loss functions coincide. SBR improves over BR, even higher when structural parts are more similar.Supports theoretical claim that the higher the structural similarties the more prominent effect of stacking. Study rest of results.
+ conditional dependence:
+ xor problem

**Conclusions**

+ study

Nou opsomming van [@Reade] - sodra klaar, probeer in hoofstuk inkorporeer.

**Introduction**

+ $n$-th feature vector $\boldsymbol{x}^{(n)}=[x_{1}^{(n)},\dots,x_{p}^{(n)}]$, where $x_{j}\in \mathcal{R}$, $j=1,\dots,p$.
+ in the traditional binary classification task we are intersted in having a model $h$ to provide a prediction for test instances $\tilde{\boldsymbol{x}}$, *i.e.* $\hat{y}=h(\tilde{\boldsymbol{x}})$. In MLC there are $K$ binary output class variables (labels) and thus $\hat{\boldsymbol{y}}=[\hat{y}_{1},\dots,\hat{y}_{K}]=h(\boldsymbol{x})$.
+ probabilistic speaking $h$ seeks the expecctation $E[\boldsymbol{y}|\boldsymbol{x}]$ of unknown $p(\boldsymbol{y}|\boldsymbol{x})$. This task is typically posed as a MAP estimate of the joint posterior mode
$$
\hat{\boldsymbol{y}}=[\hat{y}_{1},\dots,\hat{y}_{K}]=h(\tilde{\boldsymbol{x}})=\arg\max_{\boldsymbol{y}\in \{0,1\}^{p}}p(\boldsymbol{y}|\tilde{\boldsymbol{x}})
$$
This corresponds to minimizing the susbet 0/1 loss.

+ $h_{BR}(\tilde{\boldsymbol{x}}):=[h_{1}(\tilde{\boldsymbol{x}}),\dots,h_{K}(\tilde{\boldsymbol{x}})]$
+ entirety of ML literature point out that BR obtain suboptimal performance because it assumes labels are independent.
+ several approaches attempt to correct/regularize BR, SBR.
+ others attempt to learn the labels together, LP. $\hat{\boldsymbol{y}}=h_{LP}(\tilde{\boldsymbol{x}})$
+ another example is CC done using a greedy search:
$$
h_{CC}(\boldsymbol{\tilde{x}}):=[h_{1}(\tilde{\boldsymbol{x}}),h_{2}(\tilde{\boldsymbol{x}},h_{1}(\tilde{\boldsymbol{x}})),\dots,h_{K}(\tilde{\boldsymbol{x}},\dots,h_{K-1}(\tilde{\boldsymbol{x}}))]
$$
+ PCC formulates CC as the joint distribution using the chain rule,
$$
h_{CC}(\boldsymbol{x}):=\arg\max_{\boldsymbol{y}}p(y_{1}|\boldsymbol{x})\prod_{k=2}^{K}p(y_{k}|\boldsymbol{x},y_{1},\dots,y_{K-1})
$$
and show that it is indeed possible to make a Bayes-optimal search with guarantees to the optimal solution for 0/1 loss. Several search techniques exist to make the seach optimal, but greedy is still popular.
+ order and structure of chains in cc is the main focus point.
+ although in theory the chain rule holds regardless of the order of variables, each $p(y_{k}|\boldsymbol{x},y_{1},\dots,y_{K-1})$ is only an approximation of the true probability because it is modelled from finite data under a constrainded class of model, and consequently a different indexing of labels can lead to different results in practice.
+ many approaches try to find the best order and show better empirical results, but the reason why is not quite clear
+ LP can be viewed as modelling the joint probability directly, 
$$
h_{LP}(\boldsymbol{x}):=\arg\max_{\boldsymbol{y}}p(\boldsymbol{y},\boldsymbol{x})
$$
+ two main points from previous papers: (1) the best label order is impossible to obtain from observational data only. (2) the high performance of classifier chains is due to leveraging earlier labels in the chain as additional feature attributes.

**The role of label dependence in multi-label classification**

+ marginal dependence: frequency of co-occurence among labels
+ conditional dependence: after conditioning on the input
+ modelling complete dependence is intractable
+ rather attempt pairwise marginal dependence or use of ensemble.
+ many new methods do not outperform each other over a reasonable amount of datasets.
+ improvements of prediction on standard multi-label datasets reached a plateau (maybe investigate).
+ question the logic, if the ground truth label dependence could be known and modelled, multi-label predictive performance would be optimal and therefore as more technique and computational  effort is invested into modelling label dependence, the lead of the new methods over BR and other predecessors will widen.
+ BR might be underrated
+ modelling label dependence is a compensation of lack of training data and one could only assume that given infinite data two separate binary models on labels $y_{k}$ and $y_{l}$ could achieve as good performance as one that models them together.
+ the 'intuitive' understanding actually seems quite flawed: if we take two labels and wish to tag images with them, the assumption that label dependence is key to optimal multi-label accuracy is analogous to assuming that an expert trained for visually recognising one label will make optimum classifications only if having viewed the classiication of an expert trained on the other label.
+ in reality, modelling label dependence only helps when a base classifier behind one or more labels is inadequate.
+ depends on the base classifier
+ there is no guarantee that an ideal structure based on label dependence can be found at all given any amount of training data.
+ see XOR problem
+ take the view that BR can perform as well as any other method when there is no dependence among the outputs given the inputs.
+ not to say that BR should perform as well as other methods if there is no dependence *detected*. Due to noisy data or insufficient model dependence may be missed or even introduced.
+ if a ML method outperforms BR under the same base classifier then we can say that it using label dependence to compensate for the inadequacy in its base classifiers.
+ attempt to remove the dependence among the labels
+ dependence generated by inadequate base classifiers

**Binary relevance as a state-of-the-art classifier**

+ CC and LP are representative of PT problems. Succesful on many fronts and can be builded on. Still has some drawbacks. Discusses them.
+ BR less parameters to tune.
+ multi-label classifiers can be comprised of individual binary models that perform equally as well as models explicitly linked together based on label dependence or even a single model that learns labels together (intrinsic label dependence modelling).
+ claim this is the case for example and label based metrics. (not what the previous paper found)
+ proposition with proof: given $X=x$, there exists a classifier $h_{2}'(x)\approx \arg\max_{y_{2}\in \{0,1\}}p(Y_{2}|X)$ that achieves at least as small error as classifier $h_{2}(x)\approx \arg\max_{y_{2}\in \{0,1\}}p(Y_{2}|Y_{1},X)$, under loss $L(y_{2},\hat{y}_{2})=I(y_{2}\neq \hat{y}_{2})=I(y_{2}\neq h_{2}(x))$. Instances of $X,Y_{1},Y_{2}$ are given in the training data but only $\tilde{x}$ is given at test time. (see proof in paper)
+ This means that if we are interested in a model for any particular label, best accuracy can be obtained in ignorance of other labels.
+ proposition and proof: under observations $X=x$, there exists two individually constructed classifiers $h_{1}'\approx \arg\max_{y_{1}}p(Y_{1}|X)$ and $h_{2}'\approx \arg\max_{y_{2}}p(Y_{2}|X)$ such that under 0/1 loss, $[h_{1}(x),h_{2}(x)]\equiv \hat{\boldsymbol{y}}\equiv \boldsymbol{h}(x)$ are equivalent, where $\boldsymbol{h}\approx \arg\max_{[y_{1},y_{2}]}p(Y_{1},Y_{2}|X)$ models labels together. Instances of $X,Y_{1},Y_{2}$ are given in the training data but only $x$ (tilde) is given at test time. (see proof in paper)
+ following examples, $X$ represents some document and $Y_{1},Y_{2}$ represent the relevance of two subject categories for it. Latent variable $Z$ represents the unobservable current events which may affect both the observation $X$ and the decisions for labelling it. (illustration of all of the scenarios)
+ ignore case where input and all labels are independent.
+ case of conditional independence - a text document is given independently to two human labelers who each independently identify if the document is relevant to their expert domain. 
$$
\begin{aligned}
p(\boldsymbol{y},x)&=p(y_{1},y_{2})\\
&=p(y_{1}|x)p(y_{2}|y_{1},x)\\
&=p(y_{1}|x)p(y_{2}|x)
\end{aligned}
$$
  which obviously can be solved with BR, where $h_{k}(\tilde{x}):=\arg\max_{y_{k}}p(y_{k}|\tilde{x})$.
+ a text document is labelled by the first labeller and afterwards by the second expert - potentially biasing the decision to label relevance or not with this second label. If we do not impose any restriction on any $h_{k}(x)$, it is straightforward to make some latent $z\equiv h_{1}(x)$ such that $h_{2}(x, z)\equiv h_{2}(x, h_{1}(x))$. We speak of equivalence in the sense that given $Z$ we can recover $Y_{2}$ to the same degree of accuracy (probably compared to case without $Z$). In this analogy the second labeller must learn also the first labeller's knowledge and thus makes the first labeller redundant. If we drop $Y_{1}$ we return to the original structure.
+ two experts label a document $X$ but both are biased by each other and - possibly to alternate degrees - by an external source of information $Z$. Can also introduce latent variables $Z_{1},Z_{2}$ to break the dependence between the labels.
+ note the dependence between any variable can be broken by introducing hidden variables not just the label variables. Hence we can further break dependence between $X$ and $Y_{1}$ in the same way - if we desire.
+ universal approximation: with a finite number of neurons, even with even with a linear output layer, a network can approximate any continuous function. Implies for ML - given a large enough but finite feature representation in the form of a middle layer, any of the labels can be learned independently of the others, *i.e.* a linear BR layer can suffice for optimal classification performance.
+ to summarise: if we find dependence between labels it can be seen as a result of marginalizing out hidden variables that generated them. Also, we can add hidden variables to remove the dependence between labels.
+ this does not mean we have a method to learn this structure. Which is learning latent variables powerful enough. 
+ EM and MCMC sampling under energy models to learn latent variables by minimizing the energy and thus maxmimizing the joint probability with observed variables. (iterative procedures). 
+ unsupervised part more difficult than supervised
+ **existing methods to obtain conditional independence among labels.**
+ task: making outputs independent of each other by using a different input space to the original such that a simpler classifier can be employed to predict outputs.
+ deep learning to learn a powerful higher-level feature representations of the data. (uses multiple hidden layers)
+ in MLC the labels can be seen as high-level feature representations.
+ **the equivalence of loss metrics under independent outputs**
+ if outputs are independent of each other given the input, then minimizing Hamming loss and 0/1 loss is equivalent.
+ the risk of Hamming loss is minimized by BR
$$
\hat{y}_{k}=\arg\max_{y_{k}\in\{0,1\}}p(y_{k}|\boldsymbol{x})
$$
  for each label. The 0/1 loss on the other hand, is minimized by taking the mode of the distribution,
$$
\hat{\boldsymbol{y}}=\arg\max_{\boldsymbol{y}\in \{0,1\}^{K}}p(\boldsymbol{y}|\boldsymbol{x})
$$
  equivalently written as
$$
\hat{\boldsymbol{y}}=\arg\max_{\boldsymbol{y}\in \{0,1\}^{K}}p(y_{1}|\boldsymbol{x})\prod_{k=2}^{K}p(y_{k}|\boldsymbol{x},y_{1},\dots ,y_{K-1}).
$$
+ Noting that when all outputs are independent of each other given the input ($p(y_{k}|\boldsymbol{x},y_{l})\equiv p(y_{k}|\boldsymbol{x})$), then for all $k,l$ it becomes
$$
\begin{aligned}
\hat{\boldsymbol{y}}&=\arg\max_{\boldsymbol{y}\in\{0,1\}^{K}}\prod_{k=1}^{K} p(y_{k}|\boldsymbol{x})\\
&=\left[\arg\max_{y_{1}\in \{0,1\}}p(y_{1}|\boldsymbol{x}), \dots ,\arg\max_{y_{K}\in\{0,1\}}p(y_{k}|\boldsymbol{x})\right].
\end{aligned}
$$
+ here input refers to the input into the model and not the original features.
+ we can replace the input with hidden variables derived from the original feature space in order to make them independent. If this is successful, the above holds, and using BR will achieve the same result as CC on either measure.
+ suppose only the third of three outputs is successfully made independent, then prediction of independent models is optimizing 
$$
\hat{\boldsymbol{y}}=\left[\arg\max_{y_{1},y_{2}\in \{0,1\}^{2}}p(y_{1},y_{2}|\boldsymbol{x}),\arg\max_{y_{3}\in\{0,1\}}p(y_{3}|\boldsymbol{x})\right].
$$
+ if this is the case it could be handled elegantly by RAkELd - disjoint labelset segmentations RAkEL. But detecting these mixed dependence sets is difficult.
+ RAkEL and ECC benefit from the ensemble effect of reducing variance of estimates but it is not clear what loss measure is being optimized.

**Classifier chains augmented with synthetic labels (CCASL)**

+ difficult to search for good order in CC
+ if 'difficult' label is at start of chain, all other labels may suffer.
+ present a method that adds synthetic labels to the beginning of the chain and builds up a non-linear representation, which can be leveraged by other classifiers further down the chain. CCASL
+ create $H$ synthetic labels.
+ many options - they used threshold linear unit (TLU) to make binary, can also try others like ReLU with continuous output. or sigmoid and radial basis.
+ the synthetic labels can be interpreted as random cascaed basis functions, except that at prediction time the values are predicted and thus we refer to them as synthetic labels.
+ synthetic label $z_{k}=I(a_{k}>t_{k})$ with activation values 
$$
a_{k}=\left([B* W]^{T}_{k,1:(p+(k-1))}\cdot\boldsymbol{x}_{k}'\right)
$$
  where $W$ is a random weight matrix (sampled from multivariate normal) with identically sized masking matrix $B$ where $B_{i,j}\sim Bernoulli(0.9)$, input $\boldsymbol{x}_{k}'=[x_{1},\dots , x_{p}, z_{1},\dots ,z_{k-1}]$ (not the same $k$ as label index), and threshold $t_{k}\sim \mathcal{N}(\mu_{k},\sigma_{k}\cdot0.1)$
+ want to use synthetic labels at beginning of chain to improve prediction of the real labels.
+ $\boldsymbol{y}'=[z_{1},\dots ,z_{H},y_{1},\dots , y_{K}]$ and from the predictions $\hat{\boldsymbol{y}}'$ we extract the real labels $\hat{\boldsymbol{y}}=[\hat{y}_{H+1}', \dots , \hat{y}_{H+K}]=[\hat{y}_{1},\dots, \hat{y}_{K}]$.
+ $\hat{y}_{j}=\arg\max_{y_{j}\in \{0,1\}}p(y_{j}|x_{1},\dots ,x_{p},z_{1},\dots, z_{H},y_{1},\dots,y_{j-1})$
+ use LR as base classifier
+ label order less of an issue.
+ does well on complex non linear synthetic data - overfits on simple linear synthetic data.
+ lots of tunable parameters
+ few hidden labels are necessary for CCASL, empirical suggests $H=K$.
+ **CCASL + BR**
+ guards against overfitting, removes connections among the output
+ advantages of BR, stacking and CC
+ no back prop necessary.
+ **CCASL+AML**
+ CCASL strucutre is powerful for modeling non-linearities. CCASL+BR regularizes but otherwise does not offer a more powerful classifier.
+ whereas we created synthetic labels from feature space, we can do the same from the label space.
+ layer of binary nodes which are feature functions created from the label space for each subset
+ see rest in paper.
+ section on other network based literature
+ back prop bad
+ simply using a powerful non-linear base classifier may remove the need for transformations of the feature space altogether.

**Experiments**

+ done in python and sklearn
+ synthetic dataset and music, scene, yeast, medical, enron, reuters (max K = 103)
+ 10 iterations for each datset 60/40 split
+ report parameters
+ all out-perform BR and CC
+ BR_{RF} does best under hamming loss! RF are adequately powerful to model each layer
+ CCASL are quite expensive
+ the main advantage brought by modelling label dependence via connections among outputs is that of creating a stronger learner.
+ did not investigate ensembles



It has been shown repeatedly in the literature that in order to achieve acceptable empirical results, the multi-label algorithm used must in some way or another exploit the dependence/correlation amongst the labels. Unfortunately very little theoretical evidence exists for this suggestion. To delve deeper into this topic an understanding of the evaluation metrics of multi-label classifiers is a fundamental step.

> not sure if this should go here and to what extent. This is only an introduction and the rest will be continued after algorithms are introduced.

It has been mentioned here and many times in literature that the exploitation of label structures is essential to an effective multi-label algorithm. The problem is that the correlation/dependence/relationship between labels is not yet well defined in the literature [@Zhang2014]. In [@Dembcz2012] the authors comment that researchers often use the term label dependence in an intuitive sense and not as a formally defined concept. Naturally this makes it a hard problem to solve, if it is not well defined. Some valiatnt attempts were made in [@Dembczynski], [@Dembcz2012] (and others). The following is an overview of them.

In [@Zhang2014] the existing strategies for multi-label classification are divided into categories based on the order of label correlations being considered by the algorithms. So-called first-order approaches are those that do not take label correlations into account. Second-order approaches consider the pairwise relationships between labels and high-order approaches allows for all interactions between labels and/or combinations of labels. First-order strategies simply ignore label correlations, but they are usally simpler. The latter two strategies are far more complex but also limited in some cases. Second-order strategies will not generalise well when higher-order dependencies exist amongst the labels and the the high-order strategies may 'overfit' if only subgroups of the labels are correlated [@Zhang2014].

From the Bayesian point of view, the problem of multi-label learning can be reduced to modeling the conditional joint distribution of $P(\boldsymbol{y}|\boldsymbol{x})$. This can be done in various ways. First-order approaches solve the problem by decomposing it into a number of independent task through modelling $P(y_{k}|\boldsymbol{x})$, $k=1,\dots,K$. Second-order approaches solve the problem by considering interactions between a pair of labels through modelling $P((y_{k},y_{k'})|\boldsymbol{x})$, $k\neq k'$. High-order approaches solve the problem by adressing correlations between a subset of labels through modelling $P((y_{k_{1}},y_{k_{2}},\dots,y_{k_{K'}})|\boldsymbol{x})$, $K'\leq K$. Our goal is to find a simple and efficient way to improve the performance of multi-label learning by exploiting the label dependencies [@Zhang2014]. Propose LEAD approach.

+ [Tsoumakasf] use the $\phi$ coefficient to estimate label correlations.

+ [@Sorower]
+ mention the holy grail comment
+ comment on what 'exploitation' means. Since many authors claim that exploiting label dependence structures is the only way to effectively handle multiple labels, I would assume this means that we can make use of label correlations to spare time and increase accuracy.
+ we need to think about how observations are labelled, when will it be useful to take label dependence into account and how.
+ Such a solution, however, ne- glects the fact that information of one label may be help- ful for the learning of another related label; especially when some labels have insufficient training examples, the label correlations may provide helpful extra information [@Huang]

### Symmetry

+ [@Huang] claims that most of the time the label dependencies are asymmetric and suggest the MAHR algorithm. Also most of the existing methods exploit label correlations globally, which is not necessarily a good assumption if these correlations only exist for some instances [@Huang]. They suggest a ML-LOC algorithm (which seems to do very well).

### Locality

+ is local the same as conditional? and global unconditional?

+ [@Zhu]

Existing approaches to exploiting label correlations either assume the the label correlations are global and shared by all instances, or that the label correlations are local and shared only by a subset of the data. It may be that some label correlations are globally applicable and some share only in a local group of observations.

+ give example
+ mention GLOCAL [@Zhu]

+ [@Huang]

Existing approaches typically exploit label correlations globally by assuming that the label correlations are shared by all observations. In the real-world, however, different observations may share different label correlations and few correlations are globally applicable.

+ propose ML-LOC approach
+ mentions that by assuming global correlations may be hurtful to the performance [@Huang] in empirical discussion

+ maybe meta analysis on how others claimed to improved label correlation modelling

## Problem Transformation Approaches

Problem transformation methods consist of first transforming the multi-label problem into one or more single-label problem(s) and then fitting any standard supervised learning algorithm(s) to the single-label data. For that reason, problem transformation methods are called algorithm independent, i.e. once the data is transformed, any single-label classifier can be used [@Tsoumakasc].

The two main problem transformation algorithms are the binray relevance and label powerset transformations. Both methods suffer from several limitations but they form the basis of arguably any problem transformation method. The state-of-the-art problem transformations algorithms are most of the times extensions of either the standard binary relevance or label powerset algorithms [@Alazaidah2016]. Therefore the understanding of these two basic methods are crucial in dealing with the more complex, modern problem transformation methods.

### Binary Relevance

+ basic idea
+ notation
+ cross-training
+ T-criterion for avoiding empty prediction
+ psuedo-code
+ remarks: first-order; parallel; straightforward; building block of state-of-the-art; ignores potential label correlations; may suffer from class-imbalance; computational complexity

The most common transformation method is binary relevance (BR). BR transforms the mutli-label into $K$ single-label problems by modelling the presence of the labels separately. Typically $K$ single-label binary data sets, $D_{k}=(X,\boldsymbol{Y}_{k})$ for $k=1,...,K$, would be constructed from the multi-label data set, $D=(X,Y)$. To each $D_{k}$ any single-label classifier can be applied. In the end, predictions $\hat{\boldsymbol{Y}}_{1},...,\hat{\boldsymbol{Y}}_{K}$ are obtained separately which can then be combined to allocate all the predicted relevant variables to each instance. Note, that it may occur that all of the single-label learners produces zeroes, which would imply that the instance belongs to an empty set. To avoid this [@Zhang2014] suggests following the T-criterion rule. The rule states, briefly, that in such a case the labels associated with the greatest output should be assigned to the instance. Clearly, this will only work if the base learners used gives continuous outputs and it will only make sense if all the base learners are of the same type. I suppose these rules are ad-hoc and I can think of alternatives.

> With this approach the standard single label feature selection procedures can be applied. The relevant subset of features can be identified for each label. This is convenient since it is not unlikely that the optimal subset of features will differ from label to label.

The biggest drawback for this approach is that it models each label separately and ignores the possible correlations between labels. Thus BR assumes that there are no correlations between the labels. However, these correlations can be very helpful in predicting the labels present. This is a first-order strategy. Also it can be time consuming since data sets with hundreds of labels is not rare. This would mean more than a hundred models should be fit and tuned separately. But this complexity scales linearly with increasing $K$, which is actually not so bad when comparing to other multi-label algorithms. Grouping the labels in a hierachical tree fashion may become useful when $K$ is very large [Cherman2011] (see also Incorporating label dependency into the binary relevance framework for multi-label classification by the same authors).

Another argument against BR from [@Readb]: The argument is that, due to this information loss, BR's predicted label sets are likely to contain either too many or too few labels, or labels that would never co-occur in practice.

Advantage of BR by [@Readb]:
Its assumption of label independence makes it suited to contexts where new examples may not necessarily be relevant to any known labels or where label relationships may change over the test data; even the label set $L$ may be altered dynamically - making BR ideal for active learning and data stream scenarios.

Nevertheless, BR remains a competitive ML algorithm in terms of efficiency and efficacy, especially when minimising a macro-average loss function is the goal [@Luaces]. The most important advantage of BR is that it is able to optimise several loss functions [@Luaces] also see small proof. They also show empirically that BR tends to outperform ECC when there are many labels, high label dependency and high cardinality, i.e. when the multi-label data becomes more complicated.

Compared to label powerset (LP) which will be discussed later, BR is able to predict arbitrary combinations of labels [@Tsoumakasb] not restricted only to those in the training set.

[Cherman2011] also proposes a variation of BR called BR+. Its aim is to keep the simplicity of BR but also to consider the possible label correlations. It does so by also creating $K$ binary data sets but this time each of these data sets treat all the label columns not to be predicted by the current single-label classifier as features to the classifier. Thus each sinlge-label classifier will have $p + K - 1$ inputs. So now when predicting label $l$, all of the original features in $X$ and the remaining variables $\boldsymbol{Y}_{k}$, $k\neq l$, are used as inputs for classifier $l$. (second order strategy?)

The problem arises when predicting unseen instances for which the labels are unknown. Thus the input needed for each binary classifier is not available. One workaround is to obtain an initial prediction of the labels using an ordinary BR approach and then using these predictions as inputs to the BR+ algorithm. The BR+ algortihm will most likely produce different predictions to the initial predicitons or BR which can then also be used in a next round of BR+. These steps can be continued until convergence but this seems like the classifier chains approach. (to be investigated).

[@Tsoumakasb] mentions the 2BR strategy that seems very similar/identical to BR+. They describe the 2BR method as follows: first train a binary classifier on each of the $K$ binary data sets and then use their predictions (and or probabilities) as so called meta-features for a second round of BR. They mention that it might be better to train the base and meta learners on separate parts of the training data to avoid biased predictions. They suggest using a cross-validation approach for both learners to also avoid size constraints of the training data. They describe this approach as a stacked generalisation, also mentioned in [@Tsoumakasa], [@Godbole], [@Pachet2009] calls it classifier fusion.

The adding of all the base learner predicitions as meta-feature to the meta-learners is not necessarily desirable. Some label pairs might have no correlation and adding predictions for those labels as inputs to the meta-learner will add noise to the model and waste computation time. [@Tsoumakasb] suggests a solution called corerlation-based pruning. They calculate the pairwise correlations between labels, $\phi$, and only add base learner prediction of label $i$ as a meta-feature to meta-learner $j$ if $\phi_{ij}$ is greater than some threshold. In this way only label-pairs that are highly correlated will be used in the final prediction of each other.

+ BR performs well for Hamming loss, but fails for subset 0/1 loss.
+  It is not clear, in general, whether the meta-classifier b should be trained on the BR predictions h(x) alone or use the original features x as additional inputs. Another question concerns the type of information provided by the BR predictions. One can use binary predictions, but also values of scoring functions or probabilities, if such outputs are delivered by the classifier @[@Dembcz2012].

### Label Powerset

### Classifier Chains

+ basic idea
+ notation
+ importance of ordering
+ ECC brief explanation
+ psuedo-code
+ remarks: high-order; considers label correlations in a random manner; not parallel; computational complexity

Another extension of BR, similar to 2BR and BR+, is the classifier chains (CC) approach introduced by [@Readb]. It also consists of transforming the mutli-label data set $D$ to $K$ single-label data sets but the transformations are done sequentially in the sense that the label previously treated as a response will be added as a feature for predicting the next label. This will give data sets similar to $D_{1}=(X,\boldsymbol{Y}_{1}),D_{2}=(X,\boldsymbol{Y}_{1},\boldsymbol{Y}_{2}),...D_{K}=(X,\boldsymbol{Y}_{1},\boldsymbol{Y}_{2},...,\boldsymbol{Y}_{K})$, where the last column of each is the response that needs to be predicted. To each of these single-label data sets a classifier can be trained and then their predictions are combined in the same fashion as BR. CC keeps the simplicity of BR but has that additional capacity to model label dependencies by passing label information between classifiers. This should raise the question of what order of labels should the chain consist of and should it stop after one cycle?

In a response to this, the ensembles of classifier chains (ECC) was suggested by [@Readb]. Here the term ensemble refers to an ensemble of multi-label classifiers instead of an ensemble of binary classifiers already mentioned before. ECC trains $m$ classifier chains, each with a random chain ordering and a random subset of instances. These parameters of ECC contributes to the uniqueness of each classifier chain which helps with variance reduction when their predictions are combined. These predictions are summed by label so that each label receives a number of votes. A threshold is used to select the most popular labels which form the final predicted multi-label set [@Readb] (copied from). More details still to cover in article.

CC and ECC has an advantage over the ensemble methods of BR, that it is not necessary for an initial step of training to obtain predictions of labels that can later be used as features, it does this simultaneously.

Paper still need to look at for CC [@Sucar2013].

## Algorithm Adaption Approaches

These are methods tackling the multi-label learning task by adapting, extending and/or customising an existing supervised learning algorithm [@Madjarov2012]. 

The main weakness of algorithm adaption methods is that they are mosty tailored to suit a specific model, whereas problem transformation methods are more general and allows for the use of many well-known and effective single-label models [Systems] (algorithm independent).

## Ensemble Approaches

+ Ensembles are well known for their effect of increasing overall accuracy and overcoming over-fitting, as well as allowing parallelism. The main idea behind ensembles is to exploit the fact that different classifiers may do well in different aspects of the learning task so combining them could improve overall performance. Ensembles have been extensively used in literature [13] with stacking [14], bagging [15] and boosting [16] being the main methods employed. In the context of multi-label problems, [17] proposes a fusion method where the probabilistic outputs of heterogeneous classifiers are averaged and the labels above a threshold are chosen. Copied from [Papanikolaou] (can maybe use to explain why these methods perform better and not because of label dependence)
+ evidence of stacking working [Tsoumakase]. Read conclusions chapter. Ensembling effective. Linear models good for text classification. Thresholding important.

### Ensemble of Classifier Chains

### Random $k$-Labelsets

As mentioned before, the LP method has the advantage of taking label correlations into account but typically suffers from a huge class imbalance problem. [@Tsoumakasc] suggested the Random $k$-labelsets (RAKEL) algorithm to overcome the drawbacks of LP while still being able to model label dependencies. RAKEL is simply an ensemble of LP classifiers, but the LP classifiers are trained on different subsets of the labelset. The author defined a $k$-labelset as a set $Y \subseteq L$ with $k=|Y|$, where $L$ is the complete labelset and $|Y|$ the size of the set, $Y$. Let $L^{k}$ denote the set of all distinct $k$-labelsets on $L$. The size of $L^{k}$ can thus be given by $|L^{k}|= {{|L|}\choose{k}}$. 

First, the RAKEL algorithm iteratively constructs $m$ LP classifiers. At each iteration, $j=1,2,\dots,m$, it randomly selects a $k$-labelset, $Y_{j}$, from $L^{k}$ without replacement, and then learns the classifier $h_{j}:X\to P(Y_{j})$ (review notation). For classifying an instance, $x$, each model, $h_{j}$, provides binary decisions, $h_{j}(x, \lambda_{l})$ for each label $\lambda_{l}$ in $k$-labelset $Y_{j}$. The average of these binary decisions are then computed and a final prediction for a label is given if its corresponding average is bigger than some threshold $t$. Note, the average for label $\lambda_{l}$ is not calculated by the sum of $h_{j}(x, \lambda_{l})$ divided by $m$, but by instead dividing by the number of times $\lambda_{l}$ was in $Y_{j}$ for $j=1,\dots, m$. 

The values $m$, $k$ and $t$, are all parameters to be specified by the user. Clearly, $k$ can only lie between $1$ and $|L|$, where if $k=1$, the algorithm is equivalent to the BR approach, and if $k=|Y|$, the algorithm is equivalent to the LP approach. In the original paper, the author showed empirically that by using small labelsets and an adequate number of iterations, RAKEL will manage to model label correlations effectively. An intuitive value for $t$ would be 0.5, however, in the same paper, it is shown that RAKEL performs well over a wide range of values for $t$.

A concern might be the number of classes, $2^{k}$ that each LP classifiers must deal with. In practice, each LP classifier deals with a much smaller subset of label combinations, since it can only model combinations that exist in the training set. Also, RAKEL is preferred to LP when there are a large number of labels. In this case, RAKEL would only need to model a subset of $2^{k}$ possible label combinations compared to LP that needs to model a much larger subset of $2^{|Y|}$ possible label combinations.

In [@Tsoumakasc] it is shown that RAKEL outperforms LP and BR on 3 benchmark datasets with numerous configurations. The author concluded that the randomness of the RAKEL algorithm might not be the best ensemble selection approach since it may lead to the inclusion of models that affect the ensemble's performance in a negative way. Continue with papers that improve on this idea.



+ something on (re)sampling

## Spatial Regularization Networks

+ https://arxiv.org/pdf/1702.05891.pdf

## From Single to Multi Output Paper ()

## RNN-CNN paper ()

## Direct binary Embedding

+ https://arxiv.org/pdf/1703.04960.pdf

## Another ML architecture

+ https://arxiv.org/pdf/1609.07982.pdf
+ replace last softmax with sigmoid
+ replace last FC with maxout
+ replace last pooling with spatial pyramid pooling
+ other option use FCN with global max pool at end before sigmoid
+ dropout only on first layers after representation (fixed VGG)
+ note they also used dropout at testing and then combined for mean prediction

## Is object localization for free? – Weakly-supervised learning with convolutional neural networks

+ http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Oquab_Is_Object_Localization_2015_CVPR_paper.pdf
+ section on label correlations for MLC

## Near Perfect Protein Multi-Label Classification with Deep Neural Networks

+ https://arxiv.org/pdf/1703.10663.pdf
+ spp layer after convolutions - not sure if it has to do with MLC

## BP-MLL

+ http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.130.7318&rep=rep1&type=pdf
+ description on NN and some references

## ML NN for text

+ https://arxiv.org/pdf/1312.5419.pdf

## ML MT

+ http://www.cripac.ia.ac.cn/irds/People/lwang/M-MCG/Publications/2013/YH2013ICIP.pdf
+ but not much detail

## ML Attention:

+ https://arxiv.org/pdf/1412.7755.pdf

## Sparsemax ML loss

+ https://arxiv.org/pdf/1602.02068.pdf

+ Other object detection
+ winner of yt8m challenge: https://arxiv.org/pdf/1706.06905.pdf [@Lee2017a]