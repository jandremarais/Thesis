# Multi-Label Deep Neural Networks
\label{chp:dnn_mlc}

## Introduction 

We have seen that Deep Convoltional Neural Networks have achieved great success on single-label image classificaiton problems. This is because of their strong capability in learning discriminative features when trained on large datasets, which are also transferable to other image classification problems. Naturally, one might wonder whether these learnt features are useful in the MLC domain. 

This may shed some light on the multi-label image classification problem. 

Main questions:

+ is the single-label features transferable to multi-label images?
+ can we learn from label correlations?

A common approach that extends CNNs to MLC is to transform it into multiple single-label classification problems, which can be trained with the ranking loss pr the cross-entropy loss. However, when treating labels independently, these methods fail to model the dependency between multiple labels. [@Xue2011] has shown that multi-label classification problems ehibit strong label co-occurrence dependencies.

The most common and simple method is to take popular existing networks, *e.g.* VGG and ResNet, and replace the activation function of their final classification layer to a sigmoid activation and train the network by minimising the sum of the binary cross-entropies per label. As we have seen in Chapter \ref{chp:dnn}, the sigmoid activation ensures that the output lies between 0 and 1, but unlike the softmax activation, the sum of these outputs do not add to 1. Therefore the sigmoid layer is more suitable for cases with potentially multiple labels. 

The simple approach described above is mostly related to the BR approach, since direct relationships between labels are not taken into account. However, the weights of the network are learnt by taking all of the labels jointly into account. A pure BR approach would be to train a separate network for each label. If there are enough images per label, this approach should in theory be sufficient to classify the images. For instance, it should be possible to detect a *car* in an image indepently of the presence of a *road*. However, in practice, image classifiers are imperfect and could be improved by having information on the other labels.

## Differences Between Single Label and Multiple Labels Images

However, these feature representations might not be optimal for images with multiple labels. For example, one of the labels might only relate to a very small region of the image. This label would most likely be underepresented by the features learned on single label datasets, since for single label images, the label is typically related to a large part, and usually in the centre, of the image.

In single label image datasets, it is quite fair to assume that the foreground objects are roughly aligned. This assumption more risky for images with multiple labels. A typical multi-label image has objects of different categories scattered around the image at different scales and orientations. Maybe show example images?

Interaction between objects.

Label space has been expanded and more training data is needed. But also more costly to annotate multi-label images.

## Short History

+ focus mostly on image classification, but can also mention other DNN which is possible to transfer to image classification.

Backpropogation for Multi-Label Learning (BP-MLL) [@Zhang2006] is claimed by the authors to be the first multi-label neural network. It was applied in text classification and function genomics, but could be applied to image classification aswell. As its name implies, BP-MLL is derived from the backpropogation algorithm through replacing its error function with a new function defined to capture the characteristics of multi-label learning, that is, the labels belonging to an instance should be ranked higher than those not belonging to that instance. It views each output node as binary classification task, and relies on the architecture and loss function to exploit the dependency across labels. It was later expanded by [@Nam2013] with state-of-the-art learning techniques such as dropout. Also se [@Hu2015] for structured inference NN which uses concept layers modeled with label graphs.

Extensions of conventional DNNs to suit multi-label image problems are either architectural adaptions or the optimisation of different loss functions more suited for MLC. We will first look at the latter.

## Non-Deep Learning Approaches

Traditional bag-of-words model can be decomposed into multiple modules:

+ feature representation: generate global representations for images
  + feature extraction: Extracted from dense grids and sparse interest points - SIFT, Histogram of Oriented Gradients and Local Binary Patterns
  + feature coding: Qunatise extracted features - Vector Qunatisation, Sparse Coding and Gaussian Mixture Models.
  + feature pooling: Feature aggregation methods - Spatial Pyramid Matching
+ classification: SVM or Random Forest
+ context modelling: the use of context information such as spatial location of object and background scene from global view can considerably improve the performance

These learnt features are not always optimal.

Where does VLAD and Fisher vectors fit in?

## Optimising Multi-Label Loss functions

The first proposed approach to extend neural networks to MLC (in general), named Backpropogation for Multi-Label Learning (BP-MLL), was a modification of the loss function to be optimised. Consider the global error of the network on the training set as

$$
E=\sum_{i=1}^{n}E_{i},
$$
where $E_{i}$ is the error of the network on $\boldsymbol{x}_{i}$, which could be defined as:

$$
E_{i}=\sum_{j=1}^{Q}(c_{j}^{i}-d_{j}^{i})^{2},
$$
where $c_{j}^{i}=c_{j}(\boldsymbol{x}_{i})$ is the actual output of the network on $\boldsymbol{x}_{i}$ on the $j$-th class, $d_{j}^{i}$ is the desired output of $\boldsymbol{x}_{i}$ on the $j$-th class which takes the value of either +1 ($j\in Y_{i}$) or -1 ($j\notin Y_{i}$). If the weights of the network is learned through backpropogating the errors from this loss function, some important characteristics of multi-label learning are not considered. Here $E_{i}$ only concentrates on indivdual label discrimination and it does not consider the correlations between the different labels. Therefore the authors of [@Zhang2006] suggested rewriting the global error function as follows:

$$
E=\sum_{i=1}^{n}E_{i}=\sum_{i=1}^{n}\frac{1}{|Y_{i}||\bar{Y}_{i}|}\sum_{(k,l)\in Y_{i}\times\bar{Y}_{i}}\exp(-(c_{k}^{i}-c_{l}^{i})).
$$
Here $\bar{Y}_{i}$ is the complementary set of $Y_{i}$ in $\mathcal{Y}$ and $|\cdot|$ measures the cardinality/size of a set. $c_{k}^{i}-c_{l}^{i}$ measures the difference between the outputs of the network on one label belonging to $\boldsymbol{x}_{i}$ ($k\in Y_{i}$) and one label not belonging to it ($l\in \bar{Y}_{i}$). Therefore, the bigger the difference, the better performance. The negation of this difference is fed to the exponential function in order to severly penalise the $i$-th error term if the output on the label belonging to $\boldsymbol{x}_{i}$, $c_{k}^{i}$, is much smaller than the output on the label not belonging to $\boldsymbol{x}_{i}$, $c_{l}^{i}$. The summation is over all pairs of labels where the one belongs to $\boldsymbol{x}_{i}$ and the other does not. This is then normalised by the denominator, $|Y_{i}||\bar{Y}_{i}|$, which is the total number of such possible pairs.

The minimisation of this global error will lead the network to ouput larger values for labels belonging to the training instance and smaller values for those not belonging to it.  It is shown in their paper that this error function is closely related to the rankining loss criterion. The specifics of the minimisation of this error function with gradient descent and backpropogation is not in the scope of this thesis and can be found in the original paper [@Xhang2006]. In the paper, BP-MLL, show superiority to the well-established multi-label learning algorithms of that time. Although the algorithms were tested on the applications of text classification and functional genomics, it could also be applied to image classification, since the loss function is not affected by the application domain.

One of the first approaches to extending CNNs to a multi-label image classification problems was by minimising loss functions more suited for MLC, more specifically, the multi-label ranking loss [@Gong2013]. Found that weighted approximate ranking loss worked best for CNNs. Showed more than 10% increase to conventional BoW methods on NUS-WIDE dataset. Still need to read the rest of the article but apparently an effective model requires lots of training samples (they did not use transfer leanring).

They show that a significant performance gain could be obtained by combining convolutional architectures with approximate top-$k$ ranking objectives.

Focus on loss functions tailored for multi-label prediction tasks. Compared: (see paper for description of each)

+ multi-label softmax regression loss
+ simple modification of a pairwise-ranking loss
+ multilabel variant of the WARP loss

They only compared on NUS-WIDE with no standard errors and only on per class precision and recall and overall precision and recall. Might be valuable to compare on more datasets and with more metrics. Also, these were tested with older CNN architectures, so it may be unfair to compare novel approaches utilising state-of-the-art CNN architectures to these approaches.

+ https://arxiv.org/pdf/1609.07982.pdf
+ replace last FC with maxout
+ replace last pooling with spatial pyramid pooling
+ other option use FCN with global max pool at end before sigmoid
+ dropout only on first layers after representation (fixed VGG)
+ note they also used dropout at testing and then combined for mean prediction
+ see [https://cs.nju.edu.cn/_upload/tpl/01/0b/267/template267/zhouzh.files/publication/tkde06a.pdf] for ML-loss function

### Sparsemax ML loss

+ see https://arxiv.org/pdf/1602.02068.pdf

### F-measure maximisation

+ https://arxiv.org/pdf/1604.07759.pdf

### Other

+ see https://arxiv.org/pdf/1701.05616.pdf
+ https://arxiv.org/pdf/1705.02315.pdf
+ see https://arxiv.org/pdf/1706.07960.pdf for pseudo huber loss

## Proposal Based Approaches

[@Razavian2014] and [@Sermanet2013] was the first to propose a CNN feature extraction approach. It consisted of feeding all the images of a multi-label image dataset to a CNN trained on ImageNet to get CNN activations as the off-the-shelf features for classification, where they used a SVM. Since ImageNet is a single label image classification problem, these features were not optimal for a multi-label problem (because of alignment and occlusion issues). An improvement to this idea is to annotate the images with bounding boxes indicating the presence of objects, such as in [@Oquab2014] and [@Girshick2013], but these bounding box annotations are very costly.

The HCP method described below requires no bounding box information for tranining and is robust to the possible noisy and/or redundant hypotheses. Much fewer hypotheses are alos required, giving a significant speed up in training.

If multiple labels are associated with a single image, it is fair to assume that the different labels are related to different visual regions of the image. Proposal based CNN methods attempt to cope with this problem. (Start with single to multi CNN paper). Proposal based methods are also very popular or object detection problems.

+ Hypotheses-CNN-Pooling (HCP)
+ takes an arbitrary number of object segment hypotheses as the inputs (use state-of-the-art objectiveness detection techniques, like BING)
+ and then a shared CNN is connected with each hypothesis.
+ The CNN output from each hypothesis is aggregated by max pooling
+ Pro: no ground truth is required which makes labelling cheaper
+ Pro: robust to noisy and/or redundant hypothesis (thanks to max pooling)
+ Pro: can take an arbritrary number of hypothesis as input
+ Pro: CNN can be pretrained on single image datasets.


However, these methods ignore semantic relations between labels. Next we will look at ways to capture these semantic relations in image classification.

## RNN-CNN

+ [@Wang2016]
+ note end to end frameworks are proven to be very effective.

Traditional approaches to multi-label image classification learn independent classifiers for each category and employ ranking or thresholding on the classification results. These techniques work well, but fail to explicitly exploit the label dependencies in an image.

So far it seems that this is the first paper on CNN for MLC that attempts to exploit label correlations. Previous attempts to model label dependency are mostly based on graphical models. This approach is prohibitive with large labelsets. This paper explicitly model label dependencies with recurrent neural networks (RNNs) to capture higher-order label relationships while keeping the computational complexity tractable.

To avoid problems like overfitting, previous methods normally restrict CNN classifiers to share the same image features for each class. When using the same image features to predict multiple labels, objects that are small in the images are easily ignored or hard to recognise independently. The RNNs framework is designed to adapt the image features based on the previous prediction results, by encoding the attention models implicitly in the CNN-RNN structure. The idea behind it is to implicitly adapt the attentional area in images so the CNNs can focus its attention on different regions of the images when predicting different labels. Small objects are hard to recognise by itself, but can be easily inferred given enough contexts.

> the following part may need to go under label embedding sections

In addition, many image labels have overlapping meanings. Exploiting the semantic redundancies reduce the computational cost and also imporves the generalisation ability because the labels with duplicate semantics can get more training data. The label semantic redundancy can be exploited by joint image/label embedding, which can be learned via canonical correlation analysis, metric learning or learning to rank methods. The joint image/lable embedding maps each label or image to an embedding vector in a joint low-dimensional Euclidean space such that the embeddings of semantically similar labels are close to each other, and the embedding of each image should be close to that of its associated labels in the same space. This is effective for exploiting label semantic redundancy because it essentially share classification parameters for semantically similar labels. But the label co-occurrence decpendency is largely ignored.

RNN-CNN is a unified framework for multi-label image classification which effectively learns both the semantic redundancy and the co-occurence dependency in an end-to-end way. The framework is as follows. The multi-label RNN model learns a joint low-dimensional image-label-embedding to model the semantic relevance between iamges and labels. The image embedding vectors are generated by a deep CNN while each label has its own label embedding vector. The high-order label co-occurence dpendency in this low-dimensional space is modeled with the long short term memory recurrent neurons, which maintains the information of label context in their internal memory states. The RNN framework computes the probability of a multi-label prediction sequentially as an ordered prediction path, where the a priori probability of a label each time step can be computed based on the image embedding and the output of the recurrent neurons. During prediction, the multi-label prediction with the highest probability can be approximately found with the beam search algorithm. This whole framework can be trained in an end-to-end fashion.

Other methods modelling label dependencies also only mostly model pairwise combinations.

Can visualise attentional regions.

Test on MS COCO, NUS-WIDE and PASCAL VOC 2007. No standard errors and report per class precision, recall, F1 and overall. Also MAP@10. Shows good performance but strangely does not use the best HCP on VOC2007 dataset to compare with. CNN-RNN and HCP2000 is actually quite equal. Should be tested on more datasets and maybe a combination should be considered. For example region proposals from HCP and label dependencies with RNN.

Have not read the full CNN-RNN paper. An extra detail is that the RNN require sequential input. Therefore the unordered labelset should be ordered. The original paper uses the frequent first order. [@Jin2016] uses the rarest first order, which apparently helps with the classification of the less frequently occurring classes. Read the rest of the paper. This order problem is mostly probaly solved by [@Chen2017]. 

There are some extensions to this RNN-CNN idea. The first we will look at is given by [@Liu2016]. Note, the CNN-RNN pattern is also commonly used in image captioning. The original CNN-RNN just discussed, utilises the weakly semantic CNN hidden layer or its transform as the image embedding that provides the interface between the CNN and RNN. This overstretches the RNN to complete two tasks: predicting the visual concepts of the image and modelling their correlations. This makes end-to-end training of the network slow and ineffective due to the difficulty of backpropogating gradients through the RNN to train the CNN. [@Liu2016] proposes a simple modification to improve the training of the RNN-CNN network. They propose a semantically regularised embedding layer as the interface between the CNN and RNN.

The original RNN-CNN uses the final feature layer of the CNN as an interface to the RNN. This has a number of adverse effects on learning an end-to-end recurrent image annotation model. First, since the CNN output feature is not explicitly sematincally meaningful, both the label prediction and label correlation modelling tasks are expected of the RNN model. This is difficult for the RNN especially when the number of labels is vast and their correlation rich. Also, altogether the CNN-RNN is a very deep network with only supervision at the final RNN layer. This makes convergence in training very slow. 

[@Liu2016] proposes to replace the image embedding layer and introduces semantic regularisation to the CNN-RNN model in order to produce significantly more accurate results and faster training times. Basically they follow a multi-task learning framework where the CNN-RNN network is also supervised after the CNN model by an auxiliary loss function.

Another benefit is that these models output the labels directly instead of the probability scores. This avoids the challenging thresholding calibration.

Now we should look at [@Chen2017] which is an improvement of this that does not depend on the ordering of the LSTM.

A fundamental and challenging issue for MLC is to identify and recover the co-occurrence of multiple labels, so that satisfactory prediction accuracy can be expected. Despite its effectiveness (in single-label image classification), how to extend CNNs for solving multi-label classification problems is still a research direction to explore.

Because of the use of a LSTM, a predefined label order is required during training. The labels are usually ordered by label frequency which is not necessarily a good proxy for label correlations. There is another concern with the above CNN-RNN which is that labels of objects which are in smaller scales in images would often be more difficult to be recovered. Can use the attention map as a solution. Still does not solve the label order problem. The other problem is that there is an inconsistency between the training and testing procedures of the CNN-RNN. In training, the labels are selected from the image ground-truth list, whereas in testing, the labels are selected from the full labelset. In other words, if a label is incorrectly predicted during a time step during prediction, such an error would propogate during the recurrent process.

[@Chen2017] present a novel deep learning framework of visually attended RNN, which consists of visual attention and confidence-ranked LSTM. This network is able to identify regions of interest associated with each label (even the smaller objects get attended). The order of the labels can automatically learned without any prior knowledge or assumption (with the confidence scores of the attenion maps?). This also alleviates the inconsistent procedures between training and testing.

Actually, does not look like a huge empirical improvement over the original CNN-RNN (compared on the common 3 ML image datasets). Again they compare with methods that utilise older CNN architectures.

However, these methods do not capture spatial relations between labels. This is a challenging problem because most of the times these spatial locations are not known beforehand, *i.e* the images are not annotated with these spatial locations. Spatial Regularisation Networks [@Zhua] attempt to capture both semantic and spatial relations between labels without any prior knowledge on the spatial locations of each label. This is discussed in the next section.

## Spatial Regularization Networks

+ This paper provides a unified deep neural network for exploiting both semantic and spatial relationships between labels with only image-level supervisions
+ SRN generates attention maps for all labels and captures the underlying relations between them via learnable convolutions.
+ Also aggregates regularised classification with original classifcation from RN101.
+ tests on 3 benchmark datasets that show sota performance and great generalisation capability.
+ can be trained end-to-end

In short, the SRN learns separate attention maps for each label, which associates related image regions to each label. By performing learnable convolutions on the attention maps of all labels, the SRN captures the underlying semantic and spatial relations between labels and act as a spatial regulariser for multi-label classification.

The attention mechanism adaptively focuses on related regions of the image when the deep networks are trained with spatially related labels (segmentation?). Intuitively this seems likely to work for ML image classification problems.

The main net has the same network structure as ResNet-101 [cite]. The SRN takes visual features from the main net as inputs and learns to regularise spatial relations between labels. Such relations are exploited based on the learned attention maps of each label. Label confidences from both the main net and SRN are aggregated to generate final classification scores.

![End-to-end architecture of the proposed SRN.](figures/SRN_architecture.jpg)

Not sure how in detail I should go here. 

Used $224 \times 224$ sized images. The $14\times 14$ feature map from layer named *res4b22_relu* of ResNet-101 is used as inputs to the SRN. The rest of the main net still continues to produce $K$ class scores, which is later combined with the output of the SRN. The SRN is composed of two sub-networks. The first sub-network learns label attention maps with image-level supervisions and the second sub-network captures spatial regularisations of labels based on the learned attention maps.

Multiple image regions are semantically related to different labels. The regions locations are generally not provided, but it is desirable that more attention is paid to the related regions. SRN attempts to predict such related regions using the attention mechanism. The attention maps are then used to learn spatial regularisations for the labels. The attention map for label $l$ related to an image should indicate the image regions related to $l$ by displaying higher attention values to that region. The attention estimator is modeled as 3 convolutional layers with 512 kernels of $1\times 1$, 512 kernels of $3\times 3$ and $K$ kernels of $1\times 1$, where $K=|\mathcal{L}|$. The ReLU activation function is applied after the first two convolutional layers, and the softmax after the third.

Since ground-truth annotations of attention maps are not available, the network is learned with only image-level label annotations. A weighted global average is computed for each label attention maps (similar to global average pooling in ResNet). This results in a 1024 sized vector on which a linear classifier is learned to obtain class scores. They are learned by minimising the cross-entropy loss between these predicted class scores and the ground-truth labels. (seems like the attention maps work from example given in paper).

Also compute a $1\times 1\times 1024$ convolutional layer on the feature inputs to obtain a $14\times 14\times K$ confidence map. $A$ and $S$ are multiplied element-wise and then spatially sum-pooled to obtain the label confidence scores (after Sigmoid activation).

Then combine weighted confidence scores with attention map, by element-wise multiplication and feed as input to another series of convolutional layers. These sizes should be chosen carefully in order not to have too many parameters. Authors suggest 3 convolutional layers with ReLU, followed by one fully-connected layer. See \autoref{fig:srn_fst} for size of convolutions. Empirically showed the weighted attention maps work.

![fst\label{fig:srn_fst}](figures/SRN_fst.png)

The final label scores are the weighted sum of the outputs of the main net and SRN, weighted by learnable paramter $\alpha$. However, $\alpha$ can be set to 0.5 without observable performance drop.

The training is done in three parts. Training is done first by fine-tuning only the main net (but the full main net), which was pretrained on ImageNet. Then learning the attention map and confidence map simultaneously and then the convolution of the combination of the weighted attention map and confidence map. (This description will be much easier with notation).

Used data augmentation. Random crops of $256\times 256$ image from the four corners and center, then rescaled to $224\times 224$. See paper ref for detail.

Used SGD with batch size 96, momentum 0.9 and weight decay of 0.0005. Initial learning rate is set to 0.001 and decreases by factor 0.1 when validation loss reaches a plateau, until 0.00001. Testing is done by resizing image to $224\times 224$.

Evaluates on 3 benchmark datasets, with multiple measures and shows promising results. Compared to pure ResNet and CNN-RNN and is the best on almost all measures.

Here is another paper on exploiting spatial relations: https://arxiv.org/pdf/1612.01082.pdf. However, I think they are using Recurrent Nets.

Attention maps interpretable.

+ see https://arxiv.org/pdf/1705.02315.pdf for more attention maps, interesting pooling and other loss optmisations.

## Is object localization for free? – Weakly-supervised learning with convolutional neural networks

+ http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Oquab_Is_Object_Localization_2015_CVPR_paper.pdf
+ section on label correlations for MLC

## Near Perfect Protein Multi-Label Classification with Deep Neural Networks

+ https://arxiv.org/pdf/1703.10663.pdf
+ spp layer after convolutions - not sure if it has to do with MLC

## ML MT

+ http://www.cripac.ia.ac.cn/irds/People/lwang/M-MCG/Publications/2013/YH2013ICIP.pdf
+ but not much detail

## ML Attention:

+ https://arxiv.org/pdf/1412.7755.pdf
+ more ons weakly supervised attention https://arxiv.org/pdf/1707.05821.pdf

## Context Gating

+ winner of yt8m challenge: https://arxiv.org/pdf/1706.06905.pdf [@Lee2017a]

## Chaining

+ https://arxiv.org/pdf/1706.05150.pdf 
+ https://arxiv.org/pdf/1707.03296.pdf 

## Mixture of Experts

+ see https://arxiv.org/pdf/1409.4698.pdf for MOE for MLC
+ https://arxiv.org/pdf/1707.01408.pdf
+ https://arxiv.org/pdf/1707.03296.pdf

## Label Concept Learning

+ https://arxiv.org/pdf/1707.01408.pdf

## Label Processing Layer

+ https://arxiv.org/pdf/1706.07960.pdf

## Nearest Labelset

+ see https://arxiv.org/pdf/1702.04684.pdf for nearest labelset approach

## Label Embedding Approaches

+ suggestion from CNN-RNN paper: [@Gong2012], [@Weston2011]

Multi-label classification can also be achieved by learning a joint image/label embedding. Multiview Canonical Correlation Analysis [] is a three-way canonical analysis that maps the image, label, and the semantics into the same latent space. WASABI [] and DEVISE [] learn the joint embedding using the learning to rank framework with WARP loss. Metric learning [] learns a discriminative metric to measure the image/label similarity. Matrix completion [] and bloom filter [] can also be employed as label encodings. These methods effectively exploit the label semantic redundancy, but they fall short on modeling the label co-occurrence dependency.

+ Learning Deep Latent Spaces for Multi-Label Classification: https://arxiv.org/pdf/1707.00418.pdf
+ Direct Binary embedding: https://arxiv.org/pdf/1703.04960.pdf
+ cost sensitive: https://arxiv.org/pdf/1603.09048.pdf

## Stratification

+ https://arxiv.org/pdf/1704.08756.pdf

## To read:

### Multi-Label Transfer Learning with Sparse Representation

+ https://pdfs.semanticscholar.org/92f5/bd6aa3544c36490e2dac798513055233b02c.pdf
