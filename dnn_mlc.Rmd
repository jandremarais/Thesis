# Multi-Label Deep Neural Networks
\label{chp:dnn_mlc}

## Introduction 

We have seen that Deep Convoltional Neural Networks have achieved great success on single-label image classificaiton problems. This is because of their strong capability in learning discriminative features when trained on large datasets, which are also transferable to other image classification problems. Naturally, one might wonder whether these learnt features are useful in the MLC domain. 

This may shed some light on the multi-label image classification problem. 

Main questions:

+ is the single-label features transferable to multi-label images?
+ can we learn from label correlations?

A common approach that extends CNNs to MLC is to transform it into multiple single-label classification problems, which can be trained with the ranking loss pr the cross-entropy loss. However, when treating labels independently, these methods fail to model the dependency between multiple labels. [@Xue2011] has shown that multi-label classification problems ehibit strong label co-occurrence dependencies.

The most common and simple method is to take popular existing networks, *e.g.* VGG and ResNet, and replace the activation function of their final classification layer to a sigmoid activation and train the network by minimising the sum of the binary cross-entropies per label, as in [@Grzeszick2016] and [@Szalkai2017]. As we have seen in Chapter \ref{chp:dnn}, the sigmoid activation ensures that the output lies between 0 and 1, but unlike the softmax activation, the sum of these outputs do not add to 1. Therefore the sigmoid layer is more suitable for cases with potentially multiple labels. 

The simple approach described above is mostly related to the BR approach, since direct relationships between labels are not taken into account. However, the weights of the network are learnt by taking all of the labels jointly into account. A pure BR approach would be to train a separate network for each label. If there are enough images per label, this approach should in theory be sufficient to classify the images. For instance, it should be possible to detect a *car* in an image indepently of the presence of a *road*. However, in practice, image classifiers are imperfect and could be improved by having information on the other labels.

## Differences Between Single Label and Multiple Labels Images

However, these feature representations might not be optimal for images with multiple labels. For example, one of the labels might only relate to a very small region of the image. This label would most likely be underepresented by the features learned on single label datasets, since for single label images, the label is typically related to a large part, and usually in the centre, of the image.

In single label image datasets, it is quite fair to assume that the foreground objects are roughly aligned. This assumption more risky for images with multiple labels. A typical multi-label image has objects of different categories scattered around the image at different scales and orientations. Maybe show example images?

Interaction between objects.

Label space has been expanded and more training data is needed. But also more costly to annotate multi-label images.

## Short History

+ focus mostly on image classification, but can also mention other DNN which is possible to transfer to image classification.

Backpropogation for Multi-Label Learning (BP-MLL) [@Zhang2006] is claimed by the authors to be the first multi-label neural network. It was applied in text classification and function genomics, but could be applied to image classification aswell. As its name implies, BP-MLL is derived from the backpropogation algorithm through replacing its error function with a new function defined to capture the characteristics of multi-label learning, that is, the labels belonging to an instance should be ranked higher than those not belonging to that instance. It views each output node as binary classification task, and relies on the architecture and loss function to exploit the dependency across labels. It was later expanded by [@Nam2013] with state-of-the-art learning techniques such as dropout. Also se [@Hu2015] for structured inference NN which uses concept layers modeled with label graphs.

Extensions of conventional DNNs to suit multi-label image problems are either architectural adaptions or the optimisation of different loss functions more suited for MLC. We will first look at the latter.

## Non-Deep Learning Approaches

Traditional bag-of-words model can be decomposed into multiple modules:

+ feature representation: generate global representations for images
  + feature extraction: Extracted from dense grids and sparse interest points - SIFT, Histogram of Oriented Gradients and Local Binary Patterns
  + feature coding: Qunatise extracted features - Vector Qunatisation, Sparse Coding and Gaussian Mixture Models.
  + feature pooling: Feature aggregation methods - Spatial Pyramid Matching
+ classification: SVM or Random Forest
+ context modelling: the use of context information such as spatial location of object and background scene from global view can considerably improve the performance

These learnt features are not always optimal.

Where does VLAD and Fisher vectors fit in?

## Optimising Multi-Label Loss functions

The first proposed approach to extend neural networks to MLC (in general), named Backpropogation for Multi-Label Learning (BP-MLL), was a modification of the loss function to be optimised. Consider the global error of the network on the training set as

$$
E=\sum_{i=1}^{n}E_{i},
$$
where $E_{i}$ is the error of the network on $\boldsymbol{x}_{i}$, which could be defined as:

$$
E_{i}=\sum_{j=1}^{Q}(c_{j}^{i}-d_{j}^{i})^{2},
$$
where $c_{j}^{i}=c_{j}(\boldsymbol{x}_{i})$ is the actual output of the network on $\boldsymbol{x}_{i}$ on the $j$-th class, $d_{j}^{i}$ is the desired output of $\boldsymbol{x}_{i}$ on the $j$-th class which takes the value of either +1 ($j\in Y_{i}$) or -1 ($j\notin Y_{i}$). If the weights of the network is learned through backpropogating the errors from this loss function, some important characteristics of multi-label learning are not considered. Here $E_{i}$ only concentrates on indivdual label discrimination and it does not consider the correlations between the different labels. Therefore the authors of [@Zhang2006] suggested rewriting the global error function as follows:

$$
E=\sum_{i=1}^{n}E_{i}=\sum_{i=1}^{n}\frac{1}{|Y_{i}||\bar{Y}_{i}|}\sum_{(k,l)\in Y_{i}\times\bar{Y}_{i}}\exp(-(c_{k}^{i}-c_{l}^{i})).
$$
Here $\bar{Y}_{i}$ is the complementary set of $Y_{i}$ in $\mathcal{Y}$ and $|\cdot|$ measures the cardinality/size of a set. $c_{k}^{i}-c_{l}^{i}$ measures the difference between the outputs of the network on one label belonging to $\boldsymbol{x}_{i}$ ($k\in Y_{i}$) and one label not belonging to it ($l\in \bar{Y}_{i}$). Therefore, the bigger the difference, the better performance. The negation of this difference is fed to the exponential function in order to severly penalise the $i$-th error term if the output on the label belonging to $\boldsymbol{x}_{i}$, $c_{k}^{i}$, is much smaller than the output on the label not belonging to $\boldsymbol{x}_{i}$, $c_{l}^{i}$. The summation is over all pairs of labels where the one belongs to $\boldsymbol{x}_{i}$ and the other does not. This is then normalised by the denominator, $|Y_{i}||\bar{Y}_{i}|$, which is the total number of such possible pairs.

The minimisation of this global error will lead the network to ouput larger values for labels belonging to the training instance and smaller values for those not belonging to it.  It is shown in their paper that this error function is closely related to the rankining loss criterion. The specifics of the minimisation of this error function with gradient descent and backpropogation is beyond the scope of this thesis and can be found in the original paper [@Xhang2006]. In the paper, BP-MLL show superiority to the well-established multi-label learning algorithms of that time. Although the algorithms were tested on the applications of text classification and functional genomics, it could also be applied to image classification, since the loss function is not affected by the application domain.

Is it acceptable to claim that this loss function forces the NN to learn label dependencies? [@Oquab2015] claims the following. Treating a multi-label classification problem as $K$ independent classification problems is often inadequate because it does not model label correlations. This is not an issue here because the classifiers share hidden layers and therefore are not independent. Such a network can model label correlations by tuning the overlap of the hidden state distribution given each label.

+ See [@Nam2013] for clearer explanation of rank loss minimisation. They also show how thresholding needs to be done with this network.

Plenty of years later, the authors of [@Nam2013] showed that BP-MLL's ranking loss could efficiently and effectively be replaced with the commonly used cross entropy error function and demonstrate that several advances in neural network training that have been developed in the realm of deep learning can be effectively employed in this setting. With these new developments in architectures, it is better to optimise the cross-entropy rather than the BP-MLL loss. Apparently the BP-MLL loss function is not consistent with the rank loss and cross entropy is. Cross-entropy is also computationally more efficient. Their experiments are more trustworthy because of comparsions over more datasets with more measures.

One of the first approaches to extending CNNs to a multi-label image classification problems was by minimising loss functions more suited for MLC, more specifically, the multi-label ranking loss [@Gong2013]. Found that weighted approximate ranking loss worked best for CNNs. Showed more than 10% increase to conventional BoW methods on NUS-WIDE dataset. Still need to read the rest of the article but apparently an effective model requires lots of training samples (they did not use transfer leanring).

They show that a significant performance gain could be obtained by combining convolutional architectures with approximate top-$k$ ranking objectives.

Focus on loss functions tailored for multi-label prediction tasks. The first loss function they considered was the softmax loss adopted to the MLC context. Consider the convolutional network, $f(\cdot)$, where the convolutional layers and dense connected layers filter the images. The output of $f(\cdot)$ is a scoring function of the data point $\boldsymbol{x}$, that produces a vector of activations. The posterior probability of an image, $\boldsymbol{x}_{i}$, and class $j$ (out of $K$ possible classes) can be expressed as

$$
p_{ij}=\frac{\exp(f_{j}(\boldsymbol{x}_{i}))}{\sum_{k=1}^{K}\exp(f_{k}(\boldsymbol{x}_{i}))},
$$
where $f_{j}(\boldsymbol{x}_{i})$ is the activation value for image $\boldsymbol{x}_{i}$ and class $j$. The KL-divergence between the predictions and the ground-truth probabilities can then be minimised. The ground-truth probabilities can be obtained by normalising $\boldsymbol{y}$ as $\boldsymbol{y}/||\boldsymbol{y}||_{1}$, where $\boldsymbol{y}$ is a binary vector of size $K$ indicating class presence/absence (1/0). (not sure about this ground truth transformation - seems like it is unncessary). Let the ground-truth probability for image $i$ and class $j$ be defined as $\bar{p}_{ij}$. The cost function then to be minimised is

$$
J=-\frac{1}{m}\sum_{i=1}^{n}\sum_{j=1}^{K}\bar{p}_{ij}\log(p_{ij})=-\frac{1}{m}\sum_{i=1}^{n}\sum_{j=1}^{c+}\frac{1}{c+}\log(p_{ij}),
$$
where $c+$ dentes the number of positive labels for each image. Technically, then $c+$ is dependent on $i$, but the authors chose this notation for ease of exposition.

The second loss was a simple modification of a pairwise-ranking loss, which takes multiple labels into account. The aim was to rank positive labels to always have higher scores than negative labels. This led to the problem of minimising

$$
J=\sum_{i=1}^{n}\sum_{j=1}^{c+}\sum_{k=1}^{c-}\max(0,1-f_{j}(\boldsymbol{x}_{i})+f_{k}(\boldsymbol{x}_{i})),
$$
where $c-$ is the number of negative labels. (can improve on this indexing). During the backpropogation they computed the sub-gradient of this loss function. One limitation of this loss is that it optimise the area under the ROC curve (AUC) but does not directly optimise the top-$k$ annotation accuracy.

The third loss function they experimented with is a multi-label variant of the weighted approximate ranking (WARP) loss, which uses a sampling trick to optimise top-$k$ annotation accuracy. It specifically optimises the top-$k$ accuracy for annotation by using a stochastic sampling approach. It minimises

$$
J=\sum_{i=1}^{n}\sum_{j=1}^{c+}\sum_{k=1}^{c-}L(r_{j})\max(0,1-f_{j}(\boldsymbol{x}_{i})+f_{k}(\boldsymbol{x}_{i})),
$$
where $L(\cdot)$ is a weighting function for different ranks and $r_{j}$ is the rank for the $j$-th class for image $i$. The weighting function used is defined as:

$$
L(r)=\sum_{j=1}^{r}\alpha_{j},
$$
with $\alpha_{1}\ge \alpha_{2}\ge\dots\ge 0$. They defined $\alpha_{j}$ as $1/j$. It is clear that $L(\cdot)$ will assign a small weight to the loss if a positive label is ranked top in the label list. Howeverm if a positive label is not ranked top, $L(\cdot)$ will assign a much larger weight to the lossm which pushes the positive label to the top.

To find the rank $r_{j}$, they followed the following sampling method. For a positive label, continue sampling negative labels until a violation is found and let $s$ be the number of trials sampled for negative labels. The rank was estimated by the following formulation

$$
r=\lfloor \frac{K-1}{s}\rfloor.
$$
Still need to figure this out.

They only compared on NUS-WIDE with no standard errors and only on per class precision and recall and overall precision and recall. Might be valuable to compare on more datasets and with more metrics. Also, these were tested with older CNN architectures, so it may be unfair to compare novel approaches utilising state-of-the-art CNN architectures to these approaches. They found that WARP loss gave the best results.

[@Zhu2017] used multi-label CNN for pedestrian attribute detection. The adaption they made to the CNN was also by redefining the loss function. The defined the loss function as

$$
F=\sum_{k=1}^{K}\lambda_{k}G_{k},
$$
where $G_{k}$ is the loss of the $k$-th attribute(/label). $K$ is the total number of labels and $\lambda_{k}\ge 0$ is a parameter controlling the contribution of each label. In their experiments they set $\lambda_{k}=1/K$ and defined $G_{k}$ as

$$
G_{k}=-\frac{1}{N}\sum_{n=1}^{N}\sum_{m=1}^{M^{k}}1\{y_{n}^{k}=m\}\cdot\log\frac{\exp((w_{m}^{k})^{T}x_{n}^{k})}{\sum_{m=1}^{M}\exp((w_{m}^{k})^{T}x_{n}^{k})}.
$$
$N$ is the number of training samples and $M^{k}$ represents the class number of $k$-th attribute. To avoid bias due to imbalanced data, they further extend $G_{k}$ as follows:

$$
G_{k}=-\frac{1}{N}\sum_{n=1}^{N}\sum_{m=1}^{M^{k}}1\{y_{n}^{k}=m\}\cdot \beta_{m}^{k}\log\frac{\exp((w_{m}^{k})^{T}x_{n}^{k})}{\sum_{m=1}^{M}\exp((w_{m}^{k})^{T}x_{n}^{k})},
$$
where $\beta_{m}^{k}=\frac{1/N_{m}^{k}}{\sum_{l=1}^{k}1/N_{l}^{k}}$. $N_{m}^{k}$ is the number of samples holding $m$-th class label of $k$-th attribute and it meets $\sum_{m=1}^{M}N_{m}^{k}=N^{k}$. Make sure first that this loss is much different to the others proposed and what the difference is between attribute and label. This might not be applicable to a pure MLC problem.

### Sparsemax ML loss

[@Martins2016] propose the sparsemax transformation. Sparsemax has the distinctive feature that it can return sparse posterior distributions, *i.e.* it may assign exactly zero scores to some of its ouput variables. This is a convenient feature for MLC, especially when the labelset is large. Added benefits of this transformation is that it preserves the attractive properties of the softmax: it is simple to evaluate, it is even cheaper to differentiate and it can be turned into a convex loss function.

Let $\Delta^{K-1}:=\{\boldsymbol{p}\in \mathbb{R}^{K}|\boldsymbol{1}^{T}\boldsymbol{p}=1,\boldsymbol{p}\ge\boldsymbol{0}\}$ be the ($K-1$)-dimensional simplex. We are interested in functions that map vectors in $\mathbb{R}^{K}$ to probability distributions in $\Delta^{K-1}$, such that we can obtain label posterior probabilities from label scores. We have already seen the softmax function defined as

$$
\text{softmax}_{i}(\boldsymbol{z})=\frac{\exp(z_{i})}{\sum_{j}\exp(z_{j})}.
$$
A limitation of the softmax transformation is that $\text{softmax}_{i}(\boldsymbol{z})\ne 0$ for every $\boldsymbol{z}$ and $i$. This is disadvantageous in applications where a sparse probability distribution is desired. An alternative is to use the sparsemax transformation:

$$
\text{sparsemax}(\boldsymbol{z}):=\arg\min_{\boldsymbol{p}\in\Delta^{K-1}}||\boldsymbol{p}-\boldsymbol{z}||^{2}.
$$
Therefore, the sparsemax returns the Euclidean projection of the input vector $\boldsymbol{z}$ onto the probability simplex. This projection is likely to hit the boundary of the simplex, in which the $\text{sparsemax}(\boldsymbol{z})$ becomes sparse. In the paper they show that the sparsemax retains most of the important properties of the softmax.

Now we need to show how to use the sparsemax transformation to design a new loss function that resembles the logistic loss but can yield sparse distributions. The loss function associated with the softmax is the logisitc loss (or negative log-likelihood):

$$
\begin{aligned}
L_{\text{softmax}}(\boldsymbol{z};k)&=-\log(\text{softmax}_{k}(\boldsymbol{z}))\\
&=-z_{k}+\log\left(\sum_{j=1}\exp(z_{j})\right)
\end{aligned}
$$
Can find the gradient ... Need something similar for sparsemax.

The authors derived a convex and differentiable loss names the softmax loss:

$$
L_{\text{softmax}}(\boldsymbol{z};k)=-z_{k}+\frac{1}{2}\sum_{j\in S(\boldsymbol{z})}(z_{j}^{2}-\tau^{2}(\boldsymbol{z}))+\frac{1}{2},
$$
where $\tau^{2}$ is the square of the threshold function (see paper). They show that in the binary case, the sparsemax reduces to the Huber classification loss. The generalisation to the MLC case can be given by:

$$
L_{\text{sparsemax}}(\boldsymbol{z};\boldsymbol{q})=-\boldsymbol{q}^{T}\boldsymbol{z}+\frac{1}{2}\sum_{j\in S(\boldsymbol{z})}(z_{j}^{2}-\tau^{2}(\boldsymbol{z}))+\frac{1}{2}||\boldsymbol{q}||^{2}.
$$
Details omitted here.

The results do not seem that convincing. 

[@Wang2017] experimented with the Hinge, Euclidean and Cross-Entropy losses. They found that the ouput is very sparse and that the network struggled with finding positive images. To alleviate this problem they suggested using a positive/negative balancing factor, $\beta_{P},\beta_{N}$, with the cross-entropy loss which results in the weighted cross-entropy loss:

$$
L(f(\boldsymbol{x}),\boldsymbol{y})=-\beta_{P}\sum_{y_{c}=1}\ln(f(\boldsymbol{x}_{c}))-\beta_{N}\sum_{y_{c}=0}\ln(1-f(\boldsymbol{x}_{c})),
$$
where $\beta_{P}=\frac{|P|+|N|}{|P|}$ and $\beta_{N}=\frac{|P|+|N|}{|N|}$. $|P|$ and $|N|$ are the number of positive and negative labels per batch of images, respectively. The weighted version worked better for them, especially for classes with fewer examples.

[@Szalkai2017] also used a weighting of the loss contribution per class to help the infrequent labels.

### F-measure maximisation

+ https://arxiv.org/pdf/1604.07759.pdf actually this is not a DNN loss so probably won't include it here.
+ https://arxiv.org/pdf/1608.04802.pdf shows how to optimise for the $F_{\beta}$-measure. But they don't mention MLC so not sure if it is applicable.

### Other

+ see https://arxiv.org/pdf/1701.05616.pdf
+ https://arxiv.org/pdf/1705.02315.pdf
+ see https://arxiv.org/pdf/1706.07960.pdf for pseudo huber loss

## Proposal Based Approaches

[@Razavian2014] and [@Sermanet2013] was the first to propose a CNN feature extraction approach. It consisted of feeding all the images of a multi-label image dataset to a CNN trained on ImageNet to get CNN activations as the off-the-shelf features for classification, where they used a SVM. Since ImageNet is a single label image classification problem, these features were not optimal for a multi-label problem (because of alignment and occlusion issues). An improvement to this idea is to annotate the images with bounding boxes indicating the presence of objects, such as in [@Oquab2014] and [@Girshick2013], but these bounding box annotations are very costly.

The HCP method described below requires no bounding box information for tranining and is robust to the possible noisy and/or redundant hypotheses. Much fewer hypotheses are alos required, giving a significant speed up in training.

If multiple labels are associated with a single image, it is fair to assume that the different labels are related to different visual regions of the image. Proposal based CNN methods attempt to cope with this problem. (Start with single to multi CNN paper). Proposal based methods are also very popular or object detection problems.

+ Hypotheses-CNN-Pooling (HCP)
+ takes an arbitrary number of object segment hypotheses as the inputs (use state-of-the-art objectiveness detection techniques, like BING)
+ and then a shared CNN is connected with each hypothesis.
+ The CNN output from each hypothesis is aggregated by max pooling
+ Pro: no ground truth is required which makes labelling cheaper
+ Pro: robust to noisy and/or redundant hypothesis (thanks to max pooling)
+ Pro: can take an arbritrary number of hypothesis as input
+ Pro: CNN can be pretrained on single image datasets.


However, these methods ignore semantic relations between labels. Next we will look at ways to capture these semantic relations in image classification.

## RNN-CNN

+ [@Wang2016]
+ note end to end frameworks are proven to be very effective.

Traditional approaches to multi-label image classification learn independent classifiers for each category and employ ranking or thresholding on the classification results. These techniques work well, but fail to explicitly exploit the label dependencies in an image.

So far it seems that this is the first paper on CNN for MLC that attempts to exploit label correlations. Previous attempts to model label dependency are mostly based on graphical models. This approach is prohibitive with large labelsets. This paper explicitly model label dependencies with recurrent neural networks (RNNs) to capture higher-order label relationships while keeping the computational complexity tractable.

To avoid problems like overfitting, previous methods normally restrict CNN classifiers to share the same image features for each class. When using the same image features to predict multiple labels, objects that are small in the images are easily ignored or hard to recognise independently. The RNNs framework is designed to adapt the image features based on the previous prediction results, by encoding the attention models implicitly in the CNN-RNN structure. The idea behind it is to implicitly adapt the attentional area in images so the CNNs can focus its attention on different regions of the images when predicting different labels. Small objects are hard to recognise by itself, but can be easily inferred given enough contexts.

> the following part may need to go under label embedding sections

In addition, many image labels have overlapping meanings. Exploiting the semantic redundancies reduce the computational cost and also imporves the generalisation ability because the labels with duplicate semantics can get more training data. The label semantic redundancy can be exploited by joint image/label embedding, which can be learned via canonical correlation analysis, metric learning or learning to rank methods. The joint image/lable embedding maps each label or image to an embedding vector in a joint low-dimensional Euclidean space such that the embeddings of semantically similar labels are close to each other, and the embedding of each image should be close to that of its associated labels in the same space. This is effective for exploiting label semantic redundancy because it essentially share classification parameters for semantically similar labels. But the label co-occurrence decpendency is largely ignored.

RNN-CNN is a unified framework for multi-label image classification which effectively learns both the semantic redundancy and the co-occurence dependency in an end-to-end way. The framework is as follows. The multi-label RNN model learns a joint low-dimensional image-label-embedding to model the semantic relevance between iamges and labels. The image embedding vectors are generated by a deep CNN while each label has its own label embedding vector. The high-order label co-occurence dpendency in this low-dimensional space is modeled with the long short term memory recurrent neurons, which maintains the information of label context in their internal memory states. The RNN framework computes the probability of a multi-label prediction sequentially as an ordered prediction path, where the a priori probability of a label each time step can be computed based on the image embedding and the output of the recurrent neurons. During prediction, the multi-label prediction with the highest probability can be approximately found with the beam search algorithm. This whole framework can be trained in an end-to-end fashion.

Other methods modelling label dependencies also only mostly model pairwise combinations.

Can visualise attentional regions.

Test on MS COCO, NUS-WIDE and PASCAL VOC 2007. No standard errors and report per class precision, recall, F1 and overall. Also MAP@10. Shows good performance but strangely does not use the best HCP on VOC2007 dataset to compare with. CNN-RNN and HCP2000 is actually quite equal. Should be tested on more datasets and maybe a combination should be considered. For example region proposals from HCP and label dependencies with RNN.

Have not read the full CNN-RNN paper. An extra detail is that the RNN require sequential input. Therefore the unordered labelset should be ordered. The original paper uses the frequent first order. [@Jin2016] uses the rarest first order, which apparently helps with the classification of the less frequently occurring classes. Read the rest of the paper. This order problem is mostly probaly solved by [@Chen2017]. 

There are some extensions to this RNN-CNN idea. The first we will look at is given by [@Liu2016]. Note, the CNN-RNN pattern is also commonly used in image captioning. The original CNN-RNN just discussed, utilises the weakly semantic CNN hidden layer or its transform as the image embedding that provides the interface between the CNN and RNN. This overstretches the RNN to complete two tasks: predicting the visual concepts of the image and modelling their correlations. This makes end-to-end training of the network slow and ineffective due to the difficulty of backpropogating gradients through the RNN to train the CNN. [@Liu2016] proposes a simple modification to improve the training of the RNN-CNN network. They propose a semantically regularised embedding layer as the interface between the CNN and RNN.

The original RNN-CNN uses the final feature layer of the CNN as an interface to the RNN. This has a number of adverse effects on learning an end-to-end recurrent image annotation model. First, since the CNN output feature is not explicitly sematincally meaningful, both the label prediction and label correlation modelling tasks are expected of the RNN model. This is difficult for the RNN especially when the number of labels is vast and their correlation rich. Also, altogether the CNN-RNN is a very deep network with only supervision at the final RNN layer. This makes convergence in training very slow. 

[@Liu2016] proposes to replace the image embedding layer and introduces semantic regularisation to the CNN-RNN model in order to produce significantly more accurate results and faster training times. Basically they follow a multi-task learning framework where the CNN-RNN network is also supervised after the CNN model by an auxiliary loss function.

Another benefit is that these models output the labels directly instead of the probability scores. This avoids the challenging thresholding calibration.

Now we should look at [@Chen2017] which is an improvement of this that does not depend on the ordering of the LSTM.

A fundamental and challenging issue for MLC is to identify and recover the co-occurrence of multiple labels, so that satisfactory prediction accuracy can be expected. Despite its effectiveness (in single-label image classification), how to extend CNNs for solving multi-label classification problems is still a research direction to explore.

Because of the use of a LSTM, a predefined label order is required during training. The labels are usually ordered by label frequency which is not necessarily a good proxy for label correlations. There is another concern with the above CNN-RNN which is that labels of objects which are in smaller scales in images would often be more difficult to be recovered. Can use the attention map as a solution. Still does not solve the label order problem. The other problem is that there is an inconsistency between the training and testing procedures of the CNN-RNN. In training, the labels are selected from the image ground-truth list, whereas in testing, the labels are selected from the full labelset. In other words, if a label is incorrectly predicted during a time step during prediction, such an error would propogate during the recurrent process.

[@Chen2017] present a novel deep learning framework of visually attended RNN, which consists of visual attention and confidence-ranked LSTM. This network is able to identify regions of interest associated with each label (even the smaller objects get attended). The order of the labels can automatically learned without any prior knowledge or assumption (with the confidence scores of the attenion maps?). This also alleviates the inconsistent procedures between training and testing.

Actually, does not look like a huge empirical improvement over the original CNN-RNN (compared on the common 3 ML image datasets). Again they compare with methods that utilise older CNN architectures.

However, these methods do not capture spatial relations between labels. This is a challenging problem because most of the times these spatial locations are not known beforehand, *i.e* the images are not annotated with these spatial locations. Spatial Regularisation Networks [@Zhua] attempt to capture both semantic and spatial relations between labels without any prior knowledge on the spatial locations of each label. This is discussed in the next section.

## Spatial Regularization Networks

+ This paper provides a unified deep neural network for exploiting both semantic and spatial relationships between labels with only image-level supervisions
+ SRN generates attention maps for all labels and captures the underlying relations between them via learnable convolutions.
+ Also aggregates regularised classification with original classifcation from RN101.
+ tests on 3 benchmark datasets that show sota performance and great generalisation capability.
+ can be trained end-to-end

In short, the SRN learns separate attention maps for each label, which associates related image regions to each label. By performing learnable convolutions on the attention maps of all labels, the SRN captures the underlying semantic and spatial relations between labels and act as a spatial regulariser for multi-label classification.

The attention mechanism adaptively focuses on related regions of the image when the deep networks are trained with spatially related labels (segmentation?). Intuitively this seems likely to work for ML image classification problems.

The main net has the same network structure as ResNet-101 [cite]. The SRN takes visual features from the main net as inputs and learns to regularise spatial relations between labels. Such relations are exploited based on the learned attention maps of each label. Label confidences from both the main net and SRN are aggregated to generate final classification scores.

![End-to-end architecture of the proposed SRN.](figures/SRN_architecture.jpg)

Not sure how in detail I should go here. 

Used $224 \times 224$ sized images. The $14\times 14$ feature map from layer named *res4b22_relu* of ResNet-101 is used as inputs to the SRN. The rest of the main net still continues to produce $K$ class scores, which is later combined with the output of the SRN. The SRN is composed of two sub-networks. The first sub-network learns label attention maps with image-level supervisions and the second sub-network captures spatial regularisations of labels based on the learned attention maps.

Multiple image regions are semantically related to different labels. The regions locations are generally not provided, but it is desirable that more attention is paid to the related regions. SRN attempts to predict such related regions using the attention mechanism. The attention maps are then used to learn spatial regularisations for the labels. The attention map for label $l$ related to an image should indicate the image regions related to $l$ by displaying higher attention values to that region. The attention estimator is modeled as 3 convolutional layers with 512 kernels of $1\times 1$, 512 kernels of $3\times 3$ and $K$ kernels of $1\times 1$, where $K=|\mathcal{L}|$. The ReLU activation function is applied after the first two convolutional layers, and the softmax after the third.

Since ground-truth annotations of attention maps are not available, the network is learned with only image-level label annotations. A weighted global average is computed for each label attention maps (similar to global average pooling in ResNet). This results in a 1024 sized vector on which a linear classifier is learned to obtain class scores. They are learned by minimising the cross-entropy loss between these predicted class scores and the ground-truth labels. (seems like the attention maps work from example given in paper).

Also compute a $1\times 1\times 1024$ convolutional layer on the feature inputs to obtain a $14\times 14\times K$ confidence map. $A$ and $S$ are multiplied element-wise and then spatially sum-pooled to obtain the label confidence scores (after Sigmoid activation).

Then combine weighted confidence scores with attention map, by element-wise multiplication and feed as input to another series of convolutional layers. These sizes should be chosen carefully in order not to have too many parameters. Authors suggest 3 convolutional layers with ReLU, followed by one fully-connected layer. See \autoref{fig:srn_fst} for size of convolutions. Empirically showed the weighted attention maps work.

![fst\label{fig:srn_fst}](figures/SRN_fst.png)

The final label scores are the weighted sum of the outputs of the main net and SRN, weighted by learnable paramter $\alpha$. However, $\alpha$ can be set to 0.5 without observable performance drop.

The training is done in three parts. Training is done first by fine-tuning only the main net (but the full main net), which was pretrained on ImageNet. Then learning the attention map and confidence map simultaneously and then the convolution of the combination of the weighted attention map and confidence map. (This description will be much easier with notation).

Used data augmentation. Random crops of $256\times 256$ image from the four corners and center, then rescaled to $224\times 224$. See paper ref for detail.

Used SGD with batch size 96, momentum 0.9 and weight decay of 0.0005. Initial learning rate is set to 0.001 and decreases by factor 0.1 when validation loss reaches a plateau, until 0.00001. Testing is done by resizing image to $224\times 224$.

Evaluates on 3 benchmark datasets, with multiple measures and shows promising results. Compared to pure ResNet and CNN-RNN and is the best on almost all measures.

Here is another paper on exploiting spatial relations: https://arxiv.org/pdf/1612.01082.pdf. However, I think they are using Recurrent Nets.

Attention maps interpretable.

[@Wang2017] designed a DCNN for multi-label classification of diseases from chest x-rays. They used a transition layer between the pretrained CNN activations and the classification layer. They also experimented with the Hinge Loss, Euclidean loss and Cross-entropy loss. See the section on loss functions above. The other things they tried were using an alternative pooling layer, named Log-Sum-Exp (LSE) pooling. Can be seen as weighted combination of max and average pooling. There is an optimal weight that achieve the best results, but I suspect it might be quite variable and difficult to determine. Also see their approach to heat map localisation.

## Mixture of Experts

The original baseline solution to the YT8M competition [@Abu-El-Haija2016] used an approach called Mixture-of-Experts (MoE) as a classification unit to their DNN. Subsequently, all the top solutions to the competition used a variation of MoE. Therefore, it is worth looking into here. Especially, since it can easily be built on top of a CNN while the complete networks is still trainable in an end-to-end fashion.

MoEs were first introduced in [@Jordan1994]. The binary classifier for an entity $e$ is composed of a set of hidden states, or experts, $\mathcal{H}_{e}$. A softmax is typically used to model the probability of choosing each expert. Given an expert, we can use a sigmoid to model the existence of the entity. Thus, the final probablity for entity $e$'s existence is

$$
p(e|\boldsymbol{x})=\sum_{h\in\mathcal{H}_{e}}p(h|\boldsymbol{x})\sigma(\boldsymbol{u}_{h}^{T}\boldsymbol{x}).
$$

$p(h|\boldsymbol{x})$ is the softmax over $|\mathcal{H}_{e}|+1$ states, *i.e.*

$$
p(h|\boldsymbol{x})=\frac{\exp(\boldsymbol{w}_{h}^{T}\boldsymbol{x})}{1+\sum_{h'\in\mathcal{H}_{e}}\exp(\boldsymbol{w}_{h'}^{T}\boldsymbol{x})}.
$$
The last, ($|\mathcal{H}_{e}|+1$)-th, state is as dummy state that always results in the non-existence of the entity. In the paper, they show how to derive the gradient in terms of the log-loss.

The number of experts, $|\mathcal{H}_{e}|$, is a parameter to be tuned. [@Abu-El-Haija2016] found a 0.5%-1% increase on all performance metrics when the number of experts increased from 1 to 2 to 4. However, the number of model parameters also increases with this increase and therefore they settled on choosing 2 mixtures as a good compromise between performance and number of parameters. (The increase seems rather small. From the results, it looks like the increase is in absolute percentage points, which is better than percentage increases.)

[@Hong2014] defines MoE to be a mixture model that consists of a set of experts that are combined via a gating (or switching) module to represent the conditional distribution $P(y|\boldsymbol{x})$. They were the first to apply MoE to the MLC problem, but they used conditional tree-structured bayesian networks as there experts. The MoE model defines a soft-partitioning of the input space via the gating module on which each expert represent different input-output relations. Therefore, MoE models are especially useful when individual experts are effective for certain subsets of input-output relations, but not all of them. The ability to switch among experts in different regions of the input space allows to compensate for individual experts 'weak spots' and increases the overall performance. The gating module chooses, given the input, which expert is the expert for that input space. (maybe some of this should be mentioned in the Chapter \ref{chp:mlc})

+ https://arxiv.org/pdf/1707.01408.pdf
+ https://arxiv.org/pdf/1707.03296.pdf

## Context Gating

In multi-label classification settings, an example may be annotated many labels. Some labels tend to appear in the same example at the same time, some tend not to. Such information can be used to improve the performance of multi-label classification models [@Wang2017a].

The winner [@Miech2017] of the YouTube 8M video classification challenge [^yt8m], also hosted on Kaggle, proposed a network unit to better capture non-linear interdependencies between features as well as among output variables. The context gating layer transforms the input represetation $X$ into a new representation, $Y$, in the following way:

$$
Y=\sigma(WX+b)\circ X,
$$
where $X\in \mathbb{R}^{n}$ is a vector of input feature activations, $\sigma$ is the element-wise sigmoid activation and $\circ$ is the element-wise multiplication operator. $W\in \mathbb{R}^{n\times n}$ and $b\in \mathbb{R}^{n}$ are trainable parameters. The vector of weights ouput by $\sigma(\cdot)$ acts as a set of learnt gates with values between 0 and 1 on the individual dimensions of the input feature $X$.

[^yt8m]: https://www.kaggle.com/c/youtube8m

There are two reasons why this transformation be effective. First, it introduces non-linear interactions among activations of the input representation. Second, it recalibrates the strengths of different activations of the input representation through a self-gating mechanism. [Miech2017] used conext gating to both transform the feature vector before passing it to the classification module, and after the classification layer to capture the prior structure of the output label space.

The aim of the context gating of the feature vector is to capture the dependencies among the features. For example, the context gating can learn to suppress features likely to be on background and emphasise the foreground objects. For instance, if features corresponding to 'Trees', 'Skier' and 'Snow' have high co-occurring activations in a skiing video, context gating could learn to suppress the background features such as 'Trees' and 'Snow', which are less important for the classification.

The aim of the context gating unit after the classification layer is to downweight unlikely combinations such as 'Car' and 'Makeup'. It does this by reweighting the output probabilities. Note, the improvements observed was only in terms of global average precision. It will be interesting to see how it performs with other metrics and on other datsets.

## Chaining

The second place solution to the YouTube-8M competition can be found in [@Wang2017a]. The main contribution made in terms of MLC their method of capturing interactions between labels, named Chaining. They also propose methods for capturing multi-scale information and attention pooling. Their chaining unit is inspired by classifier chains discussed in Chapter \ref{chp:mlc}. As noted in the section on classifier chains, the direction of dependence for the labels is unknown and an ensemble of CC are usually used to alleviate this problem. [@Read2014a] proposes a network structure that mimics CC, but this results in very deep networks, at least as deep as the number of labels.

In a chaining model, several representations are joined by a chain of classifiers (they used MoEs). The predictions are projected to features of lower dimension and used in the following stages (probably only necessaray if there are many labels). The output of each chaining unit is supervised by auxiliary cross-entropy losses to speed up convergence. The final loss is a weighted average of the auxiliary losses and the final prediction loss. They found that allocating 10-20% to the auxiliary losses gave the best results. The number of chaining units can be experimented with. Again, this method is only tested on the yt8m dataset and only in terms of the GAP.

To me it seems that this process is more related to stacked BR classifiers (BR+,2BR) instead of CCs.

Their method to exploit multiple scales is to build the chaining unit on different feature representations of multiple scales (after different convolution-pooling combinations).

+ https://arxiv.org/pdf/1707.03296.pdf 

## Label Concept Learning

+ https://arxiv.org/pdf/1707.01408.pdf

## Label Processing Layer

+ https://arxiv.org/pdf/1706.07960.pdf

## Nearest Labelset

+ see https://arxiv.org/pdf/1702.04684.pdf for nearest labelset approach

## Label Embedding Approaches

+ suggestion from CNN-RNN paper: [@Gong2012], [@Weston2011]

Multi-label classification can also be achieved by learning a joint image/label embedding. Multiview Canonical Correlation Analysis [] is a three-way canonical analysis that maps the image, label, and the semantics into the same latent space. WASABI [] and DEVISE [] learn the joint embedding using the learning to rank framework with WARP loss. Metric learning [] learns a discriminative metric to measure the image/label similarity. Matrix completion [] and bloom filter [] can also be employed as label encodings. These methods effectively exploit the label semantic redundancy, but they fall short on modeling the label co-occurrence dependency.

+ Learning Deep Latent Spaces for Multi-Label Classification: https://arxiv.org/pdf/1707.00418.pdf
+ Direct Binary embedding: https://arxiv.org/pdf/1703.04960.pdf
+ cost sensitive: https://arxiv.org/pdf/1603.09048.pdf

## Unsorted

+ [@Hu2015] for structured inference neural networks with label relations
+ https://arxiv.org/pdf/1704.08756.pdf for stratification
+ https://pdfs.semanticscholar.org/92f5/bd6aa3544c36490e2dac798513055233b02c.pdf for Multi-Label Transfer Learning with Sparse Representation
+ http://www.cripac.ia.ac.cn/irds/People/lwang/M-MCG/Publications/2013/YH2013ICIP.pdf for Mutli-Task but not very good paper.
+ see Correlative multi-label video annotation and Correlated label propagation with application to multi-label learning for possible evidence that label correlations can be exploited.
