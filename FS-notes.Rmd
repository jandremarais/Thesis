---
output:
  pdf_document:
    fig_caption: yes
    number_sections: yes
bibliography: library.bib
csl: harvard-stellenbosch-university.csl
urlcolor: black
linkcolor: black
fontsize: 11pt
geometry: margin = 2cm
mainfont: Times New Roman
sansfont: Times New Roman
header-includes:
- \usepackage{placeins}
- \usepackage{fancyhdr}
- \usepackage{setspace}
- \usepackage{chngcntr}
- \usepackage{microtype}
- \onehalfspacing
- \counterwithin{figure}{section}
- \counterwithin{table}{section}
---

# Multi-label Feature Selection

[@Trohidis2008]:

+ typically applied to text categorisation due to typical high dimensionality of the data
+ standard approach: consider each label separately, use an attribute evaluation statistic for each label and then combine the results (ex max or average).
+ these do not consider label correlations
+ propose the label powerset transform and then do the feature selection (attribute evaluation in there case)
+ choose the best x number of features - probably through validation set approach


[@Bhowmick2009]:

+ filter out the noisy features
+ used $\chi^{2}$-statistic
+ don't say how the optimal set was chosen

[@Cherman2011a]:

+ use decision trees as base learners (since it has embedded feature selection) - other FS techniques might still help efficiency
+ report standard errors (10 fold CV)

[@Katakis2008]:

+ want to apply FS in future research
+ no se

[@Al-Salemi2015]:

+ information gain (with BOW model) and used top 10% (did not say why)
+ mention the banditBoost algorithm which does FS with mutli-armed bandit inner FS

[@Tsoumakasc]:

+ $\chi^{2}$ feature ranking for each label separately - chose the top 500 features with the best ranking over all labels
+ apparently worked well before with text data

[@Zhanga]:

+ uses information gain
+ reports se (on many datasets and algorithms)

[@Berger]:

+ also reports se's

[@Li2013]:

+ FS still needs to be explored

[@Gibaja2014]:

+ mentions the wrapper approach and filter
+ gives some references
+ search a subset of features optimising a multilabel loss functions

[Feature Selection for Multi-label Classification Problems]:

+ FS not received much attention
+ improves interpretability, performance and computing time
+ proposes a mutual information method - first transform using PPT method, then use a greedy forward search strategy with MI search criterion.
+ this consider label and feature dependence which feature rankings typically do not do.
+ MI is the measure of quantity of information two variable contain about each other
+ widely used in FS since it can detect non-linear dependencies (for example the correlation coefficient can't do that)
+ MI has to be estimated from the dataset
+ PPT is the pruned problem transformation, discards labels appearing less than $t$ times in the dataset.
+ this transformation crucial for MI estimation
+ also mentions after transformation, traditional single label FS can be employed - they used a greedy forward search based on MI
+ stopping criterion should be decided on

[Feature selection for multi-label classification using multivariate mutual information]:

+ propose a mutual information based ml fs criterion. 
+ does not involve any transformation
+ selects a subset by maximising the dependency between selected features and labels
+ lots of theory
+ applied FS then MLNB, did much better than the other methods first doing a single label transform
+ did not show se's
+ did not see mention of stopping criterions
+ the proposed method only considers up to three level interactions

[A systematic review of multi-label feature selection and a new method based on label construction]:

+ ML problems also suffer from the curse of dimensionality - use FS
+ overcoming the label dependence problem of BR - construct new labels based on relationships between original labels and include these in the FS stage.
+ little knowledge on label construction
+ call it LCFS
+ they also compared to random FS, which in my opinion is not very fair
+ used an information gain criterion
+ from the survey they found that FS techniques taking label dependence into consideration perform the best.
+ also the IG criterion is the one most used
+ wrapper and embedded approaches are designed to improve a certain learning algorithm and filter methods are algorithm independent.
+ also just choose an arbitray number of features it seems
+ did some comparisons
+ gives table on how many of each methods are represented in the literature.


[Label Construction for Multi-label Feature Selection]:

+ very similar to previous

[Multi-label learning with label-specific feature reduction]:

+ proposes fuzzy rough sets - LIFT, (FRS-LIFT). Still need to study
+ reported se with many comparisions
+ doesn't seem like they compared to PT methods


[Multi-Label Feature Selection for Graph Classification]:

+ study graph classification first

[Feature selection for multi-label naive Bayes classification]:

+ two-stage filter-wrapper: first perform PCA like feature extraction and the a genetic algorithm for feature selection
+ report se's

[Evaluating Feature Selection Methods for Multi-Label Text Classification]:

+ research on ML FS is still scarce
+ compares over many text ML datasets (8 FS methods)
+ actually just 2 feature evaluation methods $\times$ 4 aggregation strategies
+ filter methods can save times - most popular
+ arbitrary percentage of features chosen

[Effective and Efficient Multilabel Classification in Domains with Large Number of Labels]:

+ $\chi^{2}$ feature ranking for each label separately - chose the top 500 features with the best ranking over all labels

[Correlated Multi-Label Feature Selection]:

+ mention it is crucial for ML FS methods to model label dependence
+ LaRank SVM


[Using Information Gain to Build Meaningful Decision Forests for Multilabel Classification]:

+ still need to study

[@Yu]

+ although the number of labels might be large, there typically exist significant label correlations, thus reducing the effective number of parameters required to model them.


[Feature-aware Label Space Dimension Reduction for Multi-label Classification]

+ mentions the feature space dimension reduction techniques (FSDR)
+ two groups: supervised vs unsupervised
+ unsupervised, like PCA
+ supervised, like supervised PCS
+ canonical correlation analysis is a leading supervised FSDR approach see [Canonical Correlation Analysis for Multilabel Classification: A Least-Squares Formulation, Extensions, and Analysis]


[@A Comparison of Multi-label Feature Selection Methods using the Problem Transformation Approach]:

+ suggest Information Gain and ReliefF for BR and LP


[@Linear Dimensionality Reduction for Multi-label Classification]:

+ limitaion of filter methods is that they are not well connected to the classification algorithm
+ minimises a regularised loss function

[@Multi-Label Dimensionality Reduction via Dependence Maximiztion]

+ seems complicated but does well
+ review
+ MDDM


[@Feature Selection for Multi-Label Learning]

+ label construction then FS (LCFS) to account for label correlations


[@Filter Approach Feature Selection Methods to Support Multi-label Learning Based on ReliefF and Information Gain]


[@Symptom selection for multi-label data of inquiry diagnosis in traditional Chinese medicine]:

+ suggest a hybrid optimisation technique
+ still need to study
+ also mention the MDDM method does well

More notes:

[http://users.auth.gr/espyromi/publications/slides/spyromitrosWISE2014slides.pd]

+ used $\chi^{2}$ - only 6% of labels, reduced F-measure by 4% and reduced running time from 3 days to 2 hours.
+ BR with SVM's minimising L2 pen

[https://yanirseroussi.com/2014/10/07/greek-media-monitoring-kaggle-competition-my-approach/]

+ interesting approach













