# Multi-Label Learning

## Introduction

+ why this chapter
+ aim is to cover the core of MLL literature
+ update of previous reviews
+ to highlight areas to focus on for further studies

> cannot find good literature on multi-target learning - might not want to add the above paragraph

The birth of the ML field (around 1999) came from the need to assign multiple labels to text documents. Contributions in [@Schapire99improvedboosting] and [@Schapire2000a] adapted a boosting algorithm to handle ML data. [@Elisseeff] defined a ranking based SVM to deal with ML problems in the areas of text mining and also bioinformatics. [@Lewis2004] released an important benchmark collection for ML text classification. Another highly cited ML SVM implementation is [@Boutell2004a], with application in scene/image classification. [@Zhang2006] showed how to apply neural networks to a ML problem and [@Zhang2007] adapted the KNN algorithm for ML input. The first overview on the subject was given in [@Tsoumakas] where the author discussed the most relevant ML learning approaches. Then came applications to music, [@Trohidis2008] and [@Turnbull2008]. [@Vens2008] showed how to use decision trees for hierarchical ML classification. Important papers introducing unique ML approaches are [@Tsoumakas2007b], [@Furnkranz2008] and [Read2011a]. A crucial step for ML learning was to make it accesible and useable to more reasearchers. The authors of [@Tsoumakas2011] developed a Java library for ML learning. Later on [@Madjarov2012a] did a empirical study on the most important ML algorithms up to that date, comparing 12 multi-label learning methods using 16 evaluation measures over 11 benchmark datasets. More recent extensive reviews of ML learning are given in [@Zhang2014] and [@Gibaja2014]. 

> not sure which papers to include here. The aim is to highlight the most important. Difficult to determine. Should I visualise timeline of contributions?

> what does recent research focus on? Extreme ML learning?

The rapid growth of the ML learning is probably owed to the vast and expanding range of ML application domains, the biggest being text and multimedia categorisation especially those generated and/or stored on the web. Other application domains common to ML learning are: biology, chemical data analysis, social network mining and E-learning amongst others. A thorough list of applications and their citations can be found in [@Gibaja2014].

> Say something of video classification? Create my own list of applications?

The key challenge in ML learning is to exploit dependecies amongst labels, *e.g.* using the information on the relevance of label $l_{i}$ to predict label $l_{j}$, $i,j \in \{1,2,\dots,K\}$, $i\neq j$. This is especially difficult for a ML learner when there are many labels. It is not uncommon for ML datasets to have hundreds of thousands of labels. Proof of this can be found at [this](https://manikvarma.github.io/downloads/XC/XMLRepository.html) ML data repository. Algorithms that can accurately and efficiently model label dependence on these datasets are rare [@Sorower]. This is a focus area of recent ML research, called extreme multi-label learning [@Xu2016]. A more formal definition of label dependence will be given later on. An in-depth discussion on the unique challenges (thorough list by [@Gibaja2014]) that arise from dealing with label dependence and some of the possible strategies to follow will also be covered.

### Basic Idea

### Applications

+ thorough list by [@Gibaja2014]
+ text; multimedia; biology; chemical data analysis; social network mining; e-learning; other
+ examples and citations
+ mention importance

### In the Literature

+ history
+ database analysis
+ show trend
+ most cited papers
+ mention important reviews:

[@Tsoumakas2007], [@Sorower], [@Zhang2014], [@Gibaja2014], Tutorial: [@CarvalhoAndreCPLFde2009], [@Gibaja2015]

+ see [@Gibaja2015] for different MLL resources 

### Challenges and Ongoing Research

+ key challenge = output space
+ mention challenges casused by key challenges
+ from [@Gibaja2014]
+ worth highlighting that recent research has shifted its focus to problems on large scale labels - focus on scalability
+ Label Dependence: conditional vs marginal; mentions order categorisation; asymetric and local
+ Dimensionality Reduction
+ Reduction of the Input Space: feature selection; feature extraction
+ Reduction of the Output Space: HOMER; Label reduction with association rules; CCA; compressed sensing; principal label space transform; compressed labelling
+ MIML
+ Semi-supervised and Active Learning
+ Online MLL and Data Streams
+ Hierachical MLC
+ Dealing with Class Imbalance: label skew
+ Mention extreme MLC and maybe add a section somewhere in chapter
+ list also in [@Alazaidah2016]

### Aim of this Chapter

+ the goal of this chapter: review + highlight limitations to be studied further
+ outline

## The Paradigm

+ consider adding the settings described by Read and [@Gibaja2015]

### Learning Framework



### Multi-Label Data and Repositories

+ properties of a ML data set: label cardinality, label density, label diversity
+ Simulating [@TorresTomas2014] (also gives citations to other papers)
+ above has no control over label correlation
+ partitioning mentioned in [@Gibaja2015] - referred to [@Sechidis]
+ [@Sorower]
+ details of benchmark datasets in appendix (learn how to link)

### Key Challenge

+ key challenge is the size of the output space: label sets grows exponentially with increase in labels
+ solution is to exploit label correlations efficiently
+ [@Sorower] has a section on exploiting label dependence
+ mention label dependence chapter
+ first-order, second-order, high-order strategies grouping

### Threshold Calibration

+ calibrate real-valued output against thresholding function output in order to determine labels of unseen instances.
+ constant vs induced from training data + ad hoc specific to certain learning algorithms
+ alternative to choose top k labels

## Evaluation Metrics



## Learning Algorithms

### Simple Catergorisation

There are numerous multi-label learning algorithms. It is difficult to keep up with the all the latest proposed methods. These algorithm can be categorised in a number of ways, *e.g.* the review [@Zhang2014] and the tutorials [@Gibaja2015] and [@CarvalhoAndreCPLFde2009], all have different ways of grouping the algorithms. The categorisation for this thesis is chosen to satisfy the criteria of being common, simple and intuitive. Nevertheless, the characteristics of the algorithms leading to the other grouping variants will still be given in the remarks of the algorithms.

```{r mll-tax, eval=FALSE, include=FALSE}
library(DiagrammeR)
library(DiagrammeRsvg)
library(magrittr)
library(svglite)
library(rsvg)
library(png)

grViz('figures/MLL-tax.gv') %>% export_svg() %>% 
  charToRaw %>% rsvg %>% png::writePNG('figures/mll-tax.png')
```

![Categorisation of multi-label learning taxonomy (this is just an example) \label{fig:mll-tax}](figures/mll-tax.png)

+ still need to edit \autoref{fig:mll-tax}

+ want to keep it simple and representative but also give table with full list of methods
+ many proposals
+ scrutinise 8 representative algorithms for feasibility concerns
+ representativeness criteria: broad spectrum; primitive impact; favourable impact
+ introduce PT vs AA
+ diagram of categorisation
+ very thorough one in [@Gibaja2015]
+ mention ensemble category

### Problem Transformation Methods




#### Calibrated Label Ranking

+ basic idea
+ notation
+ process for unseen instance
+ threshold function
+ enriched version of pairwise comparison
+ psuedo-code
+ remarks: second-oder; one-vs-one; mitigates class imbalance issue; quadratic increase in classifiers and improvements thereof; computational complexity

#### Random k-Labelsets

+ basic idea
+ notation
+ LP
+ limitations of LP: incompleteness and inefficiency
+ why rakel improves it
+ psuedo-code
+ remarks: high-order; ensembling; computational complexity

The other widely known problem transformation approach is the label powerset (LP) algorithm. Each combination of the labels is seen as a distinct class and then a standard multiclass classification learner can be applied. More formally, the transformation $h:L\to P(L)$ is applied [@Tsoumakasc]. Thus label correlations are taken into account but LP has other limitations. The number of possible classes increase exponentially with the increase in $K$ and some of the classes/combinations are under-represented (if represented at all) in the training set. This leads to the difficult problem of learning from unbalanced classes and also restricts the algortihm to only predict combinations of labels present in the training set. Labels (or labelsets) that only occur a limited number of times are called tail labels. These are generally the ones difficult to model and a classifier can easily neglect their importance [@Xu2016].

One way to reduce the number of resulting classes after a label powerset transformation is to create meta-labels (not to be confused with meta in the stacking sense) [@Read]. Meta-labels represent partitions of the label set, but I still do not fully understand the concept. Seems like after the transformation we still end up with a multi-label problem. Investigate further.

Another option is to throw away the combinations that appear infrequently in the training set. This obviously limits the possible output of the mutli-label algorithm even more. Sounds like PPT [@Reada].

The best way, according to the literature [@Zhang2014], to overcome the two main limitations of LP is another ensemble based approach (similar idea to stacking) called random $k$-labelsets (RA$k$EL). It was first introduced in [@Tsoumakasc]. It reduces the number of classes to predict from and allows each class to have more training instances compared to the LP method [@Lo2013]. Each learner in the ensemble constructs an LP classifier from a random subset of $k$ labels, referred to as $k$-labelsets. The randomness of this approach can also be its downfall, since the chosen subsets may not cover all labels and inter-label correlations [@Rokach]. We will use the term $L^{k}$ to denote the set of all distinct $k$-labelsets on $L$. The size of $L^{k}$ is given by $|L^{k}| = {{|L|}\choose{k}}$. The RA$k$EL algorithm iteratively constructs $m$ LP classifiers from a randomly select $k$-labelset, where each $k$-labelset is sampled from $L^{k}$ without replacement. The parameters to be tuned for RA$k$EL is the size of the ensemble (number of iterations), $m$, and the size of the labelsets, $k$. If $k=1$ and $m=|L|=K$ this is just an ensemble of BR classifiers. If $k=|L|$ then it becomes the ordinary LP method. [@Tsoumakasc] suggests a small $k$ with a sufficient number of ensembles, $m$, which they show to manage label correlations efficient- and effectively.

To make a prediction on a new instance, the new instance is feeded to each classifier in the ensemble and then their outputs are combined. To determine if label $j$ is relevant for this new instance, the proportion of classifiers in the ensemble indicating that label $l$ is relevant is obtained and then the final output is 1 if this proportion is greater than some threshold $t$. Intuitively $t=0.5$ makes sense, since this is equivalent to a majority vote, but RA$k$EL has been shown to work for a wide range of $t$-values.

Another factor to consider is the value of $k$. We do not want it to big. Also how many models should be included in the ensemble?

There are other ways of choosing subsets of the labelset, references in [@Rokach].

Note, with all these ensemble extensions, we can still try different ways of ensembling/stacking, especially with RA$k$EL. Not only taking the average but also by assigning weights to each model or by fitting a model to the predictions. Think [@Lo2013] is an example of this with generalised $k$-labelsets ensemble.

+ LP takes the label dependence into account, but the conditional one: it is well-tailored for the subset 0/1 loss, but fails for the Hamming loss. 
+ LP may gain from the expansion of the feature or hypothesis space.
+ One can easily tailor LP for solving the Hamming loss minimization problem, by marginalization of the joint probability distribution that is a by-product of this classifier.

### Algorithm Adaption Methods



#### Multi-Label k-Nearest Neighbour (ML-kNN)

+ basic idea
+ procedure
+ psuedo-code
+ remarks: first-order; merits of lazy learning and Bayesian reasoning; mitigate class-imbalance; extensions/variations; computational complexity

#### Multi-Label Decision Tree (ML-DT)

+ basic idea
+ procedure
+ psuedo-code
+ remarks: first-oders; efficient; improve with pruning and or ensembling; computational complexity

#### Ranking Support Vector Machine (Rank-SVM)

+ basic idea
+ procedure
+ psuedo-code
+ remarks: second-order; variants; computational complexity

#### Collective Multi-Label Classifier (CML)

+ basic idea
+ procedure
+ psuedo-code
+ remarks: second-order; conditional random field model; DAG; computational complexity

### Summary

+ table: basic idea; order; complexity (test/train); domains; optimised metric
+ interpretation
+ comment on applicability to data sets
+ first-order AA is not BR

## Related Learning Strategies

+ multi-instance
+ ordinal classification
+ multi-task learning
+ data streams classification
+ multi-task learning; multiple-labels learning; multi-intance learning; reverse multi-label learning; preferential text classification; weak label problem; multi-valued multi-label; graded multi-label; multi-view learning

## Pitfalls and Guidelines

+ choosing algo is hard
+ more empirical evidence is needed; with with wide range of data sets, algos and measures; compare with statistical tests and consider computation time (training and test)
+ part on statistical tests in [@Gibaja2015]
+ gives reccomendations from empirical study
+ mentions label dependence modelling from dembcsz
+ trees for efficiency, ensembles for predictive performance, transformation methods for flexibility

## Conclusion

+ what was done in the paper
+ no formal charaterisation on the underlying concept or any principled mechanism on the appropriate usage of label correlations
+ correlations might be asymmetric and or local
+ label correlation understanding is holy grail of ML
+ complement of this paper would be a broad empirical study

+ what follows in next chapter(s)
+ highlight challenges again and mention those we are going to focus on: label dependence, high dimensionality
+ consider the addition of empirical study 
+ will actually contribute empirically but with specific questions in mind (end of each chapter)











