# Trash

# MLC

## History

The birth of MLC (around 1999) came from the need to assign multiple labels to text documents. Contributions in [@Schapire1999] and [@Schapire2000] adapted a boosting algorithm to handle multi-labelled data. [@Elisseeff2001] defined a ranking based SVM to deal with multi-label problems in the areas of text mining and also bioinformatics. [@Lewis2004] released an important benchmark collection for multi-label text classification. Another highly cited multi-label SVM implementation is [@Boutell2004a], with application in scene/image classification. [@Zhang2006] showed how to apply neural networks to a multi-label problem and [@Zhang2007] adapted the KNN algorithm for multi-labelled input. The first overview on the subject was given in [@Tsoumakas] where the author discussed the most relevant MLC approaches. Then came applications to music, [@Trohidis2008] and [@Turnbull2008]. [@Vens2008] showed how to use decision trees for hierarchical MLC. Important papers introducing unique MLC approaches are [@Tsoumakas2007a], [@Furnkranz2008] and [@Read2011]. A crucial step for MLC was to make it accesible and useable to more reasearchers. The authors of [@Tsoumakas2011] developed a Java library for MLC. Later on, [@Madjarov2012] did a empirical study on the most important MLC algorithms up to that date, comparing 12 MLC methods using 16 evaluation measures over 11 benchmark datasets. More recent extensive reviews of MLC are given in [@Zhang2014] and [@Gibaja2014]. 


## labels as hidden nodes

Nou opsomming van [@Reade] - sodra klaar, probeer in hoofstuk inkorporeer.

**Introduction**

+ $n$-th feature vector $\boldsymbol{x}^{(n)}=[x_{1}^{(n)},\dots,x_{p}^{(n)}]$, where $x_{j}\in \mathcal{R}$, $j=1,\dots,p$.
+ in the traditional binary classification task we are intersted in having a model $h$ to provide a prediction for test instances $\tilde{\boldsymbol{x}}$, *i.e.* $\hat{y}=h(\tilde{\boldsymbol{x}})$. In MLC there are $K$ binary output class variables (labels) and thus $\hat{\boldsymbol{y}}=[\hat{y}_{1},\dots,\hat{y}_{K}]=h(\boldsymbol{x})$.
+ probabilistic speaking $h$ seeks the expecctation $E[\boldsymbol{y}|\boldsymbol{x}]$ of unknown $p(\boldsymbol{y}|\boldsymbol{x})$. This task is typically posed as a MAP estimate of the joint posterior mode
$$
\hat{\boldsymbol{y}}=[\hat{y}_{1},\dots,\hat{y}_{K}]=h(\tilde{\boldsymbol{x}})=\arg\max_{\boldsymbol{y}\in \{0,1\}^{p}}p(\boldsymbol{y}|\tilde{\boldsymbol{x}})
$$
This corresponds to minimizing the susbet 0/1 loss.

+ $h_{BR}(\tilde{\boldsymbol{x}}):=[h_{1}(\tilde{\boldsymbol{x}}),\dots,h_{K}(\tilde{\boldsymbol{x}})]$
+ entirety of ML literature point out that BR obtain suboptimal performance because it assumes labels are independent.
+ several approaches attempt to correct/regularize BR, SBR.
+ others attempt to learn the labels together, LP. $\hat{\boldsymbol{y}}=h_{LP}(\tilde{\boldsymbol{x}})$
+ another example is CC done using a greedy search:
$$
h_{CC}(\boldsymbol{\tilde{x}}):=[h_{1}(\tilde{\boldsymbol{x}}),h_{2}(\tilde{\boldsymbol{x}},h_{1}(\tilde{\boldsymbol{x}})),\dots,h_{K}(\tilde{\boldsymbol{x}},\dots,h_{K-1}(\tilde{\boldsymbol{x}}))]
$$
+ PCC formulates CC as the joint distribution using the chain rule,
$$
h_{CC}(\boldsymbol{x}):=\arg\max_{\boldsymbol{y}}p(y_{1}|\boldsymbol{x})\prod_{k=2}^{K}p(y_{k}|\boldsymbol{x},y_{1},\dots,y_{K-1})
$$
and show that it is indeed possible to make a Bayes-optimal search with guarantees to the optimal solution for 0/1 loss. Several search techniques exist to make the seach optimal, but greedy is still popular.
+ order and structure of chains in cc is the main focus point.
+ although in theory the chain rule holds regardless of the order of variables, each $p(y_{k}|\boldsymbol{x},y_{1},\dots,y_{K-1})$ is only an approximation of the true probability because it is modelled from finite data under a constrainded class of model, and consequently a different indexing of labels can lead to different results in practice.
+ many approaches try to find the best order and show better empirical results, but the reason why is not quite clear
+ LP can be viewed as modelling the joint probability directly, 
$$
h_{LP}(\boldsymbol{x}):=\arg\max_{\boldsymbol{y}}p(\boldsymbol{y},\boldsymbol{x})
$$
+ two main points from previous papers: (1) the best label order is impossible to obtain from observational data only. (2) the high performance of classifier chains is due to leveraging earlier labels in the chain as additional feature attributes.

**The role of label dependence in multi-label classification**

+ marginal dependence: frequency of co-occurence among labels
+ conditional dependence: after conditioning on the input
+ modelling complete dependence is intractable
+ rather attempt pairwise marginal dependence or use of ensemble.
+ many new methods do not outperform each other over a reasonable amount of datasets.
+ improvements of prediction on standard multi-label datasets reached a plateau (maybe investigate).
+ question the logic, if the ground truth label dependence could be known and modelled, multi-label predictive performance would be optimal and therefore as more technique and computational  effort is invested into modelling label dependence, the lead of the new methods over BR and other predecessors will widen.
+ BR might be underrated
+ modelling label dependence is a compensation of lack of training data and one could only assume that given infinite data two separate binary models on labels $y_{k}$ and $y_{l}$ could achieve as good performance as one that models them together.
+ the 'intuitive' understanding actually seems quite flawed: if we take two labels and wish to tag images with them, the assumption that label dependence is key to optimal multi-label accuracy is analogous to assuming that an expert trained for visually recognising one label will make optimum classifications only if having viewed the classiication of an expert trained on the other label.
+ in reality, modelling label dependence only helps when a base classifier behind one or more labels is inadequate.
+ depends on the base classifier
+ there is no guarantee that an ideal structure based on label dependence can be found at all given any amount of training data.
+ see XOR problem
+ take the view that BR can perform as well as any other method when there is no dependence among the outputs given the inputs.
+ not to say that BR should perform as well as other methods if there is no dependence *detected*. Due to noisy data or insufficient model dependence may be missed or even introduced.
+ if a ML method outperforms BR under the same base classifier then we can say that it using label dependence to compensate for the inadequacy in its base classifiers.
+ attempt to remove the dependence among the labels
+ dependence generated by inadequate base classifiers

**Binary relevance as a state-of-the-art classifier**

+ CC and LP are representative of PT problems. Succesful on many fronts and can be builded on. Still has some drawbacks. Discusses them.
+ BR less parameters to tune.
+ multi-label classifiers can be comprised of individual binary models that perform equally as well as models explicitly linked together based on label dependence or even a single model that learns labels together (intrinsic label dependence modelling).
+ claim this is the case for example and label based metrics. (not what the previous paper found)
+ proposition with proof: given $X=x$, there exists a classifier $h_{2}'(x)\approx \arg\max_{y_{2}\in \{0,1\}}p(Y_{2}|X)$ that achieves at least as small error as classifier $h_{2}(x)\approx \arg\max_{y_{2}\in \{0,1\}}p(Y_{2}|Y_{1},X)$, under loss $L(y_{2},\hat{y}_{2})=I(y_{2}\neq \hat{y}_{2})=I(y_{2}\neq h_{2}(x))$. Instances of $X,Y_{1},Y_{2}$ are given in the training data but only $\tilde{x}$ is given at test time. (see proof in paper)
+ This means that if we are interested in a model for any particular label, best accuracy can be obtained in ignorance of other labels.
+ proposition and proof: under observations $X=x$, there exists two individually constructed classifiers $h_{1}'\approx \arg\max_{y_{1}}p(Y_{1}|X)$ and $h_{2}'\approx \arg\max_{y_{2}}p(Y_{2}|X)$ such that under 0/1 loss, $[h_{1}(x),h_{2}(x)]\equiv \hat{\boldsymbol{y}}\equiv \boldsymbol{h}(x)$ are equivalent, where $\boldsymbol{h}\approx \arg\max_{[y_{1},y_{2}]}p(Y_{1},Y_{2}|X)$ models labels together. Instances of $X,Y_{1},Y_{2}$ are given in the training data but only $x$ (tilde) is given at test time. (see proof in paper)
+ following examples, $X$ represents some document and $Y_{1},Y_{2}$ represent the relevance of two subject categories for it. Latent variable $Z$ represents the unobservable current events which may affect both the observation $X$ and the decisions for labelling it. (illustration of all of the scenarios)
+ ignore case where input and all labels are independent.
+ case of conditional independence - a text document is given independently to two human labelers who each independently identify if the document is relevant to their expert domain. 
$$
\begin{aligned}
p(\boldsymbol{y},x)&=p(y_{1},y_{2})\\
&=p(y_{1}|x)p(y_{2}|y_{1},x)\\
&=p(y_{1}|x)p(y_{2}|x)
\end{aligned}
$$
  which obviously can be solved with BR, where $h_{k}(\tilde{x}):=\arg\max_{y_{k}}p(y_{k}|\tilde{x})$.
+ a text document is labelled by the first labeller and afterwards by the second expert - potentially biasing the decision to label relevance or not with this second label. If we do not impose any restriction on any $h_{k}(x)$, it is straightforward to make some latent $z\equiv h_{1}(x)$ such that $h_{2}(x, z)\equiv h_{2}(x, h_{1}(x))$. We speak of equivalence in the sense that given $Z$ we can recover $Y_{2}$ to the same degree of accuracy (probably compared to case without $Z$). In this analogy the second labeller must learn also the first labeller's knowledge and thus makes the first labeller redundant. If we drop $Y_{1}$ we return to the original structure.
+ two experts label a document $X$ but both are biased by each other and - possibly to alternate degrees - by an external source of information $Z$. Can also introduce latent variables $Z_{1},Z_{2}$ to break the dependence between the labels.
+ note the dependence between any variable can be broken by introducing hidden variables not just the label variables. Hence we can further break dependence between $X$ and $Y_{1}$ in the same way - if we desire.
+ universal approximation: with a finite number of neurons, even with even with a linear output layer, a network can approximate any continuous function. Implies for ML - given a large enough but finite feature representation in the form of a middle layer, any of the labels can be learned independently of the others, *i.e.* a linear BR layer can suffice for optimal classification performance.
+ to summarise: if we find dependence between labels it can be seen as a result of marginalizing out hidden variables that generated them. Also, we can add hidden variables to remove the dependence between labels.
+ this does not mean we have a method to learn this structure. Which is learning latent variables powerful enough. 
+ EM and MCMC sampling under energy models to learn latent variables by minimizing the energy and thus maxmimizing the joint probability with observed variables. (iterative procedures). 
+ unsupervised part more difficult than supervised
+ **existing methods to obtain conditional independence among labels.**
+ task: making outputs independent of each other by using a different input space to the original such that a simpler classifier can be employed to predict outputs.
+ deep learning to learn a powerful higher-level feature representations of the data. (uses multiple hidden layers)
+ in MLC the labels can be seen as high-level feature representations.
+ **the equivalence of loss metrics under independent outputs**
+ if outputs are independent of each other given the input, then minimizing Hamming loss and 0/1 loss is equivalent.
+ the risk of Hamming loss is minimized by BR
$$
\hat{y}_{k}=\arg\max_{y_{k}\in\{0,1\}}p(y_{k}|\boldsymbol{x})
$$
  for each label. The 0/1 loss on the other hand, is minimized by taking the mode of the distribution,
$$
\hat{\boldsymbol{y}}=\arg\max_{\boldsymbol{y}\in \{0,1\}^{K}}p(\boldsymbol{y}|\boldsymbol{x})
$$
  equivalently written as
$$
\hat{\boldsymbol{y}}=\arg\max_{\boldsymbol{y}\in \{0,1\}^{K}}p(y_{1}|\boldsymbol{x})\prod_{k=2}^{K}p(y_{k}|\boldsymbol{x},y_{1},\dots ,y_{K-1}).
$$
+ Noting that when all outputs are independent of each other given the input ($p(y_{k}|\boldsymbol{x},y_{l})\equiv p(y_{k}|\boldsymbol{x})$), then for all $k,l$ it becomes
$$
\begin{aligned}
\hat{\boldsymbol{y}}&=\arg\max_{\boldsymbol{y}\in\{0,1\}^{K}}\prod_{k=1}^{K} p(y_{k}|\boldsymbol{x})\\
&=\left[\arg\max_{y_{1}\in \{0,1\}}p(y_{1}|\boldsymbol{x}), \dots ,\arg\max_{y_{K}\in\{0,1\}}p(y_{k}|\boldsymbol{x})\right].
\end{aligned}
$$
+ here input refers to the input into the model and not the original features.
+ we can replace the input with hidden variables derived from the original feature space in order to make them independent. If this is successful, the above holds, and using BR will achieve the same result as CC on either measure.
+ suppose only the third of three outputs is successfully made independent, then prediction of independent models is optimizing 
$$
\hat{\boldsymbol{y}}=\left[\arg\max_{y_{1},y_{2}\in \{0,1\}^{2}}p(y_{1},y_{2}|\boldsymbol{x}),\arg\max_{y_{3}\in\{0,1\}}p(y_{3}|\boldsymbol{x})\right].
$$
+ if this is the case it could be handled elegantly by RAkELd - disjoint labelset segmentations RAkEL. But detecting these mixed dependence sets is difficult.
+ RAkEL and ECC benefit from the ensemble effect of reducing variance of estimates but it is not clear what loss measure is being optimized.

**Classifier chains augmented with synthetic labels (CCASL)**

+ difficult to search for good order in CC
+ if 'difficult' label is at start of chain, all other labels may suffer.
+ present a method that adds synthetic labels to the beginning of the chain and builds up a non-linear representation, which can be leveraged by other classifiers further down the chain. CCASL
+ create $H$ synthetic labels.
+ many options - they used threshold linear unit (TLU) to make binary, can also try others like ReLU with continuous output. or sigmoid and radial basis.
+ the synthetic labels can be interpreted as random cascaed basis functions, except that at prediction time the values are predicted and thus we refer to them as synthetic labels.
+ synthetic label $z_{k}=I(a_{k}>t_{k})$ with activation values 
$$
a_{k}=\left([B* W]^{T}_{k,1:(p+(k-1))}\cdot\boldsymbol{x}_{k}'\right)
$$
  where $W$ is a random weight matrix (sampled from multivariate normal) with identically sized masking matrix $B$ where $B_{i,j}\sim Bernoulli(0.9)$, input $\boldsymbol{x}_{k}'=[x_{1},\dots , x_{p}, z_{1},\dots ,z_{k-1}]$ (not the same $k$ as label index), and threshold $t_{k}\sim \mathcal{N}(\mu_{k},\sigma_{k}\cdot0.1)$
+ want to use synthetic labels at beginning of chain to improve prediction of the real labels.
+ $\boldsymbol{y}'=[z_{1},\dots ,z_{H},y_{1},\dots , y_{K}]$ and from the predictions $\hat{\boldsymbol{y}}'$ we extract the real labels $\hat{\boldsymbol{y}}=[\hat{y}_{H+1}', \dots , \hat{y}_{H+K}]=[\hat{y}_{1},\dots, \hat{y}_{K}]$.
+ $\hat{y}_{j}=\arg\max_{y_{j}\in \{0,1\}}p(y_{j}|x_{1},\dots ,x_{p},z_{1},\dots, z_{H},y_{1},\dots,y_{j-1})$
+ use LR as base classifier
+ label order less of an issue.
+ does well on complex non linear synthetic data - overfits on simple linear synthetic data.
+ lots of tunable parameters
+ few hidden labels are necessary for CCASL, empirical suggests $H=K$.
+ **CCASL + BR**
+ guards against overfitting, removes connections among the output
+ advantages of BR, stacking and CC
+ no back prop necessary.
+ **CCASL+AML**
+ CCASL strucutre is powerful for modeling non-linearities. CCASL+BR regularizes but otherwise does not offer a more powerful classifier.
+ whereas we created synthetic labels from feature space, we can do the same from the label space.
+ layer of binary nodes which are feature functions created from the label space for each subset
+ see rest in paper.
+ section on other network based literature
+ back prop bad
+ simply using a powerful non-linear base classifier may remove the need for transformations of the feature space altogether.

**Experiments**

+ done in python and sklearn
+ synthetic dataset and music, scene, yeast, medical, enron, reuters (max K = 103)
+ 10 iterations for each datset 60/40 split
+ report parameters
+ all out-perform BR and CC
+ BR_{RF} does best under hamming loss! RF are adequately powerful to model each layer
+ CCASL are quite expensive
+ the main advantage brought by modelling label dependence via connections among outputs is that of creating a stronger learner.
+ did not investigate ensembles


## Label dependence theory

### Two types of label dependence

As mentioned, most mutli-label learning papers display merely an intuitive understanding of *label dependence*, in the sense that in predicting a specific label, the information on the rest of the labels may be helpful. For example in an image recognition problem, if a picture is labelled with *beach* and *ocean*, *sand* will most likely be a relevant label. Clearly, this understanding is insufficient to gain advances in the multi-label learning literature (later on it will also be pointed out why this may indeed not make intuitive sense). In this section, a formal statistical definition of the two types of label dependence will be given. First, we briefly revisit the task of multi-label classification (MLC), in mathematical(?) terms.

#### Marginal vs. conditional dependence

First note that we denote the conditional distribution of $\boldsymbol{Y}=\boldsymbol{y}$ given $\boldsymbol{X}=\boldsymbol{x}$ as
$$
P(\boldsymbol{Y}=\boldsymbol{y}|\boldsymbol{X}=\boldsymbol{x})=P(\boldsymbol{y}|\boldsymbol{x})
$$
and the corresponding conditional marginal distribution of $Y_{k}$ (conditioned on $\boldsymbol{x}$) as
$$
P(Y_{k}=b|\boldsymbol{x})=\sum_{y_{i}=b}P(\boldsymbol{y}|\boldsymbol{x}).
$$
(can probably also write as $P(Y_{k}|\boldsymbol{x})$ since $b$ is either 0 or 1?)

[@Dembcz2012] defines two types of dependence among lables, namely, conditional dependence and marginal dependence. Their definitions follow:

\newtheorem{thm}{Theorem}
\newtheorem{defn}{Definition}
\newtheorem{exmp}{Example}

\begin{defn}
A random vector of labels $\boldsymbol{Y}=(Y_{1},Y_{2},\dots,Y_{K})$ is called marginally independent if 

\begin{equation}
\label{eq-mdep}
P(\boldsymbol{Y})=\prod_{k=1}^{K}P(Y_{k}).
\end{equation}\\
\end{defn}

Marginal dependence is also known as unconditional dependence and can be thought of as a measure of the frequency of co-occurrence among labels. Conditional dependence captures the dependence of the labels given a specific observation $\boldsymbol{x}$.  
  
\begin{defn}
A random vector of labels is called conditionally independent, given $\boldsymbol{x}$ if 

\begin{equation}
\label{eq-cdep}
P(\boldsymbol{Y}|\boldsymbol{x})=\prod_{k=1}^{K}P(Y_{k}|\boldsymbol{x}).
\end{equation}\\
\end{defn}

The conditional joint distribution of a random vector $\boldsymbol{Y}=(Y_{1},Y_{2},\dots,Y_{K})$ can be expressed by the product rule of probability ($P(AB)=P(A|B)P(B)$):

\begin{equation}
\label{eq-jointdist}
P(\boldsymbol{Y}|\boldsymbol{x})=P(Y_{1}|\boldsymbol{x})\prod_{k=2}^{K}P(Y_{k}|Y_{1},\dots,Y_{k-1},\boldsymbol{x}).
\end{equation}

A similar expression can be given for $P(\boldsymbol{Y})$. If $Y_{1},Y_{2},\dots,Y_{K}$ are conditionally independent, then \autoref{eq-jointdist} will simplify to \autoref{eq-cdep}.

Marginal and conditional dependence are closely related - it can be written as:

\begin{equation}
\label{eq-cdep-mdep}
P(\boldsymbol{Y})=\int_{\mathcal{X}}P(\boldsymbol{Y}|\boldsymbol{x})d\mu(\boldsymbol{x}),
\end{equation}

where $\mu$ is the probability measure on the input space $\mathcal{X}$ induced by the joint probability distribution $P$ on $\mathcal{X}\times\mathcal{Y}$. Marginal dependence can roughly be viewed as an 'expected dependence' over all instances. Nevertheless, marginal dependence does not imply conditional independence, or *vice versa*. Two examples from [@Dembcz2012] are given to illustrate this.  
  
\begin{exmp}
Suppose two labels, $Y_{1}$ and $Y_{2}$, are independently generated from $P(Y_{k}|\boldsymbol{x})=\left(1+\exp(-\phi f(\boldsymbol{x})\right)^{-1}$, where $\phi$ controls the Bayes error rate. Thus, by definition, the two labels are conditionally independent with conditional joint distribution, $P(\boldsymbol{Y}|\boldsymbol{x})=P(Y_{1}|\boldsymbol{x})\times P(Y_{2}|\boldsymbol{x})$. However, as $\phi\to\infty$, the Bayes error tends to zero and the marginal dependence increases to an almost deterministic case of $y_{1}=y_{2}$. Showing, conditional independence does not imply marginal independence.\\
\end{exmp}  
\begin{exmp}
Suppose two labels, $Y_{1}$ and $Y_{2}$, are to be predicted by using a single binary feature, $x_{1}$. Let the joint distribution $P(X_{1},Y_{1},Y_{2})$ be given by the following table:

```{r, echo=FALSE, results = "asis", fig.align = 'center'}
x <- rep(c(0, 1), each = 4)
y1 <- rep(rep(c(0, 1), each = 2), 2)
y2 <- rep(c(0, 1), 4)
P <- c(0.25, 0, 0, 0.25, 0, 0.25, 0.25, 0)

library(xtable)
options(xtable.floating = TRUE)
options(xtable.comment = FALSE)
xtab <- xtable(data.frame(x,y1,y2,P), digits = c(0, 0, 0, 0, 2), align = "ccccc")
colnames(xtab) <- c("$x_{1}$", "$y_{1}$", "$y_{2}$", "$P$")
print(xtab, include.rownames = FALSE, sanitize.text.function = function(x) {x})

```

Thus, the labels are not conditionally independent, 
$$
P(Y_{1}=0,Y_{2}=0|x_{1}=1)=0\neq P(Y_{1}=0|x_{1}=1)\times P(Y_{2}=0|x_{1}=1)=0.25\times 0.25,
$$
but it can be shown that they are indeed marginally independent. For example,
$$
P(Y_{1}=0,Y_{2}=0)=0.25=P(Y_{1}=0)\times P(Y_{2}=0)=0.5\times 0.5. 
$$
This holds for all the combination of labels, showing that marginal independence does not imply conditional independence.
\end{exmp}

This distinction between marginal and conditional dependence is crucial in the attempt to model label dependence in multi-label classification. We describe a multi-output model with the following notation, similar to [@Hastie2009]:

\begin{equation}
\label{eq-multiout}
Y_{k}=h_{k}(\boldsymbol{X})+\epsilon_{k}(\boldsymbol{X}),
\end{equation}

for all $k = 1,2,\dots,K$. $h_{k}:\boldsymbol{X}\to\{0,1\}$ will be referred to as the structural part and $\epsilon_{k}(\boldsymbol{x})$ as the stochastic part of the model. Note that a common assumption in multi-variate regression (real-outputs) is that

\begin{equation}
\label{eq-experr}
E[\epsilon_{k} (\boldsymbol{x})]=0.
\end{equation}

for all $\boldsymbol{x}\in\boldsymbol{X}$ and $k=1,2,\dots,K$. This is not a reasonable assumption in mutli-label classification [@Dembcz2012] - the distribution of the noise terms can depend on $\boldsymbol{x}$ and two or more noise terms can depend on each other. Classifier $h_{k}$ might also be very similar to $h_{l}$, $l\neq k;l=1,2,\dots,K$. Thus there are two possible sources of label dependence: the structural part and the stochastic part of the model.

It seems that marginal dependence between labels is caused by the similarity between the structural parts. This assumption is made since it is reasonable to assume that the structural part will dominate the stochastic part. Suppose there exists a function $f(.)$ such that $h_{k}\approx f\circ h_{l}$, *i.e.*

\begin{equation}
\label{eq-fdep}
h_{k}(\boldsymbol{x})=f(h_{l}(\boldsymbol{x})) + g(\boldsymbol{x}),
\end{equation}

with $g(.)$ being negligible in the sense that $g(\boldsymbol{x})=0$ with high probability. Then this $f(.)$-*dependence* between the classifiers is likely to dominate the averaging process in \autoref{eq-cdep-mdep}, compared to $g(.)$ and the stochastic parts. This is what happens in Example 1 when $\phi \to \infty$. Thus we see that even if the dependence between $h_{k}$ and $h_{l}$ is only probable, it can still induce a dependence between the labels $Y_{k}$ and $Y_{l}$ (verstaan nie presies wat hier bedoel word nie). Another example illustrating idea is given from [@Dembcz2012].  

\begin{exmp}
Consider a problem with a 2-dimensional input $\boldsymbol{x}=(x_{1},x_{2})$, where $x_{i}$ is uniformly distributed in $[-1,1]$ for $i=1,2$, and two labels, $Y_{1},Y_{2}$, determined as follows. $Y_{1}$ is set to 1 for all positve values of $x_{1}$, i.e. $Y_{1}=I(x_{1}>0)$. The second label is generated similarly but with the decision boundary of $Y_{1}$ ($x_{1}=0$) rotated by an angle of $\alpha\in [0, \pi]$ (give illustration). In addition, let the two error terms of the model be independent and both flip the label with a probability of $0.1$. If $\alpha$ is close to zero, the labels will almost be identical and a high correlation will be observed between them. But if $\alpha = \pi$, the decision boundaries of the labels are orthogonal and a low correlation will be observed.\\
\end{exmp}

With regards to \autoref{eq-fdep}, in Example 3, $f(.)$ is the identity function and $g(.)$ given by the $\pm 1$ in the regions between the decision boundaries. From this point of view, marginal dependence can be seen as a kind of soft constraint that a learning algorithm can exploit for the purpose of regularization [@Dembcz2012]. (verstaan nie wat dit beteken nie)

For the conditional dependence, it seems that the stochastic part of the model is the cause. In Example 3, $Y_{1}$ and $Y_{2}$ is conditionally independent because the error terms are assumed to be independent. However, if there is a close relationship between $\epsilon_{1}$ and $\epsilon_{2}$, this conditional independence will be lost. [@Dembcz2012] proves the proposition that a vector of labels is conditionally dependent given $\boldsymbol{x}$ if and only if the error terms in \autoref{eq-multiout} are conditionally dependent given $\boldsymbol{x}$, *i.e.*
$$
E\left[\epsilon_{1}(\boldsymbol{x})\times \dots \times \epsilon_{K}(\boldsymbol{x}) \right]\neq E\left[\epsilon_{1}(\boldsymbol{x})\right]\times\dots\times E\left[\epsilon_{K}(\boldsymbol{x})\right].
$$

(Include proof?) It should also be noted that conditional independence can also cause marginal dependence because of \autoref{eq-cdep-mdep}. Thus the similarity between models is not the only source of of marginal dependence. 

What we have learned thus far is that there is a difference between marginal and conditional label dependence. The presence of marginal dependence does not imply conditional label dependence and *vice versa*. If label correlations are observed it can only be assumed that marginal dependence between the labels exist. It does not necessarily imply that there are any dependencies among the error terms (although it could be the cause). On the other hand, if conditional dependence is observed, one can safely assume that there are dependencies among the error terms. Next, we see how to exploit both types of label dependence to improve predictive accuracy.  

### Link between label dependence and loss minimization

One can view the MLC task from different persepectives in terms of loss minimizations. [@Dembcz2012] describes three such views, determined by the type of loss function to be minimized, the type of dependence taken into account and the distinction between marginal and joint distribution estimation. The three views and the main questions to consider for each of them are:

1. The individual label view: How can we improve the predictive accuracy of a single label by using information about other labels?
2. The joint label view: What type of non-decomposable MLC loss functions is suitable for evaluating a multi-label prediction as a whole and how to minimize such loss functions?
3. The joint distribution view: Under what conditions is it reasonable to estimate the joint conditional probability distribution over all label combinations?

#### The individual label view

With this view, the goal is to minimize a loss function that is label-wise decomposable and we want to determine whether or not it will help taking label relationships into account. The most common and intuitive label-wise decomposable loss function is the Hamming loss, which is defined as the fraction of labels whose relevance is incorrectly predicted:

\begin{equation}
\label{eq-hloss}
L_{H}\left(\boldsymbol{y}, \hat{\boldsymbol{y}}\right)=\frac{1}{K}\sum_{k=1}^{K}I\left(y_{k}\neq \hat{y}_{k}\right).
\end{equation}

\autoref{eq-hloss} is only the Hamming loss for one observation. To compute the Hamming loss over an entire dataset, \autoref{eq-hloss} is averaged over all the observations. 

It is easy to see that the Hamming loss is minimized when

$$
\hat{\boldsymbol{y}}=(\hat{y}_{1},\dots,\hat{y}_{K}),
$$

where 
$$
\hat{y}_{k}=\arg\max_{y_{k}\in\{0,1\}}p(y_{k}|\boldsymbol{x}),
$$
for $k=1,2,\dots,K$. This shows that it is enough to take only the conditional marginal distribution $P(Y_{k}|\boldsymbol{x})$ into account to solve the problem, at least on a population level. Thus the Hamming loss is minimized by BR. [@Dembcz2012] also gives a similar result for label-wise decomposable loss functions in general (thus also relevant for F-measure, AUC, *etc.*). This result implies that the multiple single label predictions problem can be solved on the basis of $P(Y_{k}|\boldsymbol{x})$ alone. Hence, with a proper choice of base classifiers and parameters for estimating the conditional marginal probabilities, there is in principle no need for modelling conditional dependence between the labels. However, in cases where the base classifiers are inadequate, dependence between the errors will exist and BR will give a suboptimal solution (make sure this statement is used correctly). Methods exist to improve BR in these situations and will be discussed shortly.

#### The joint label view

Here we are interested in non-decomposable (label-wise) MLC loss functions such as rank loss and the subset 0/1 loss. We discuss when they are appropriate and how to minimize them. First, consider the rank loss. Suppose the true labels constitute a ranking in which all relevant labels ideally precede all irrelevant ones and $\boldsymbol{h}(\boldsymbol{x})=(h_{1}(\boldsymbol{x}),\dots,h_{K}(\boldsymbol{x}))$ is seen as a ranking function representing a degree of label relevance sorted in a decreasing order. The rank loss simply counts the number of label pairs that disagree in these two rankings:

\begin{equation}
\label{eq-rloss}
L_{r}\left(\boldsymbol{y},\boldsymbol{h}(\boldsymbol{x})\right)=\sum_{(k,l);y_{k}>y_{l}}\left(I\left(h_{k}(\boldsymbol{x})<h_{l}(\boldsymbol{x})\right)+\frac{1}{2}I\left(h_{k}(\boldsymbol{x})=h_{j}(\boldsymbol{x})\right)\right).
\end{equation}

This function is not convex nor differentiable, thus an alternative would be to minimize a convex surrogate like the hinge or exponentional function. However, [@Dembcz2012] proves that it is enough to minimize \autoref{eq-rloss} by sorting the labels by their probability of relevance:

\begin{thm}
A ranking function that sorts the labels according to their probability of relevance, i.e. using the scoring function $\boldsymbol{h}(.)$ with $h_{k}(\boldsymbol{x})=P(Y_{k}=1|\boldsymbol{x})$, minimizes the expected rank loss.\\
\end{thm}

(include proof?) This implies again (just like in the case for the label-wise decomposable loss functions) that, in principle, it is not necessary to know the joint label distribution $P(\boldsymbol{Y}|\boldsymbol{x})$ when training a multi-label classifier, *i.e.* risk-minimizing predictions can be made without any knowledge about the conditional dependency between labels. Thus, to minimize the rank loss, one can simply use any approach minimizing the single label losses. Note this results does not hold for the normalized version of rank loss.

Next, we look at the extremely stringent multi-label loss function, the subset 0/1 loss:

\begin{equation}
\label{eq-s01}
L_{S}\left(\boldsymbol{y},\hat{\boldsymbol{y}}\right)=I\left(\boldsymbol{y}\neq \hat{\boldsymbol{y}}\right).
\end{equation}

Although most would agree that this is not a fair measure for MLC performance, since it does not disinguish between almost correct and completely wrong, it is still interesting to study with regards to exploiting label dependence. The risk-minimizing prediction for \autoref{eq-s01} is given by the mode of the distribution: 

\begin{equation}
\label{eq-smin}
h_{s}^{*}(\boldsymbol{x})=\arg\max_{\boldsymbol{y}}P(\boldsymbol{Y}|\boldsymbol{x}).
\end{equation}

This implies that the entire distribution of $\boldsymbol{Y}$ given $\boldsymbol{X}$ is needed to minimize the subset 0/1 loss. Thus a risk minimizing prediction requires the modelling of the joint distribution and hence the modelling of the conditional dependence between labels. Later on we will show an important results that under independent outputs, minimizing the Hamming loss and the subset 0/1 loss is equivalent, implying that BR will indeed also minimize the subset 0/1 loss (consider to show it here).

The cases for F-measure loss and the Jaccard distance is a bit more complicated and will not be discussed here. (give citation of where this can be found)

#### The joint distribution view

We just saw that minimzing the subset 0/1 loss requires the estimation of the entire conditional joint distribution, $P(\boldsymbol{Y}|\boldsymbol{X})$. Generally, if the joint distribution is known, a risk-minimizing prediction can be derived for any loss function in an explicit way:

$$
h^{*}(\boldsymbol{x})=\arg\min_{\boldsymbol{y}}E_{\boldsymbol{Y}|\boldsymbol{x}}\left[L(\boldsymbol{Y},\boldsymbol{y})\right].
$$

In some applications modelling the joint distribution may result in using simpler classifiers, potentially leading to a lower cost and a better performance compared to directly estimating marginal probabilities by means of more complex classifiers. Nevertheless, it remains a difficult task. One has to estimate 2^{K} values to estimate for a given $\boldsymbol{x}$.

**Theoretical insights into MLC**

+ proposition (with proof in paper): The hamming loss and subset 0/1 loss have the same risk-minimizer, *i.e.* $\boldsymbol{h}_{H}^{*}(\boldsymbol{x})=\boldsymbol{h}_{s}^{*}(\boldsymbol{x})$, if one of the following conditions holds: (1) Labels $Y_{1},\dots,Y_{K}$ are conditionally independent, *i.e.* $P(\boldsymbol{Y}|\boldsymbol{x})=\prod_{k=1}^{K}P(Y_{k}|\boldsymbol{x})$. (2) The probability of the mode of the joint probability is greater than or equal to 0.5, *i.e.* $P(\boldsymbol{h}_{S}^{*}(\boldsymbol{x})|\boldsymbol{x})\geq 0.5$.
+ corollary (with proof in paper): In the separable case (*i.e.* the joint conditional distribution is deterministic, $P(\boldsymbol{Y}|\boldsymbol{x})=I(\boldsymbol{Y}=\boldsymbol{y})$), the risk minimizers of the hamming loss and subset 0/1 loss coincide.

**MLC algorithms for exploiting label dependence**

+ in general not able to yield risk-mininizing predictions for multi-label losses but is well suited for loss functions whose risk-minimizer can solely be expressed in terms of marginal (conditional) distributions.
+ may be sufficient, but exploiting marginal dependencies may still be beneficial especially for small-sized problems.
+ several methods that exploit similarities between structural parts of the label models.
+ general scheme: 

\begin{equation}
\label{eq-sl}
\boldsymbol{y}=\boldsymbol{b}(\boldsymbol{h}(\boldsymbol{x}), \boldsymbol{x}),
\end{equation}

where $\boldsymbol{h}(\boldsymbol{x})$ is the binary relevance learner and $\boldsymbol{b}(.)$ is an additional classifier that shrinks or regularizes the solution of BR. Or

\begin{equation}
\label{eq-sl-inv}
\boldsymbol{b}^{-1}(\boldsymbol{y},\boldsymbol{x})=\boldsymbol{h}(\boldsymbol{x}),
\end{equation}

where the output space is first transformed and then the BR classifiers are trained and then transformed back to original.
+ Stacking follows first scheme. Form of regularization or feature expansion. Not clear which inputs should all be use for second level.
+ compressive sensing

**Experimental evidence**

+ marginal independence: stacking does improve on BR, CC similar to SBR, LP also bad. Error increases with number of labels. hamming and subset 0/1 coincide.
+ conditional independence: again loss functions coincide. SBR improves over BR, even higher when structural parts are more similar.Supports theoretical claim that the higher the structural similarties the more prominent effect of stacking. Study rest of results.


# Results

1. How do visual features learned from single-label image classification datasets compare to features learned from object detection datasets in terms of their transferability to multi-label image classification tasks?

## Transfer Learning for Multi-Label Image Classification

### Method

We have mentioned that features learned from single-label image classification datasets may not be optimal to use for transfer learning to a multi-label image classification problem. This may be the case since multi-label images are far more complex than single label images. They may have multiple regions in the image that are relevant, which can be of different scales, at various positions and have complex interactions with the other regions. These complexities are also present in images part of object detection datasets. A natural question to ask would be ""