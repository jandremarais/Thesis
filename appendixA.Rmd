# Benchmark Datasets for Multi-Label Image Classification
\label{app:data}

The progress of areas in machine/statistical learning is highly dependent on the availability of quality and diverse benchmark data sets. This enables researchers to compare their methods in a wide variety of environments. Recently, a decent amount of ML data sets has been published, but not without critique. [@Luaces] argues that the MULAN[^mulan] ML data set repository does not have data sets that are truely ML and that most of the data sets are very similar to each other. Most of the data sets have low cardinality and low label dependence. The problem with this is that these data sets may not show the true performance of ML algorithms. In [@Gibaja2015] the authors also comments on the lack of thorough, comparative empirical studies on these benchmark sets.

[^mulan]: A Java library for ML learning - http://mulan.sourceforge.net/datasets-mlc.html.

Some of the most popular and recent ML benchmark data sets for image classification will be introduced here along with their unique properties.

> mention something of simulating ML data. Hard

+ Simulating [@TorresTomas2014] (also gives citations to other papers)
+ partitioning mentioned in [@Gibaja2015] - referred to [@Sechidis]
+ [@Luaces] Therefore they created a ML data generator to simulate ML data on which algorithms can be evaluated.
+ very important for stratification: https://arxiv.org/pdf/1704.08756.pdf and more ML metrics

## Multi-Label Indicators

As with all supervised learning problems, no one ML algorithm performs optimally on all problems. It is common practice in classical single output supervised learning to first consider, for example, the number of features ($p$) and the number of observations ($n$) in a data set before deciding on which model(s) to fit to the data. The same naturally holds for a ML problem but with added complexity. The multiple outputs of the data introduces many more factors to consider before continuing to the modelling phase. Some ML data sets have only a few labels per observation, while others have plenty. In some ML data sets the number of label combinations is small, whereas in others it can be very large. Some labels appear more frequently than others. Moreover, the labels can be correlated or not. These characteristics can have a serious impact on the performance of a ML classifier. This is the reason why several specific indicators have been designed to assess ML data set properties.

The two standard measures for the multi-labeledness of a data set are *label cardinality* and *label density*, introduced by [@Tsoumakas]. The label cardinality of a ML data, $D$, set is the average number of labels per observation:

$$
LCard(D)=\frac{1}{n}\sum_{i=1}^{n}\sum_{k=1}^{K}y_{ik}.
$$
This measure can be normalised to be independent of the label set size, which results in the label density indicator:

$$
LDens(D)=\frac{1}{K}LCard(D)=\frac{1}{nK}\sum_{i=1}^{n}\sum_{k=1}^{K}y_{ik}.
$$
According to [@Tsoumakas] it is important to distinguish between these two measures, since two data sets with the same label cardinality but with a great difference in the number of labels might not exhibit the same properties and cause different behaviour to the ML classification methods. These two measures give a good indication of the label frequency of a data set, but we are also interested in the uniformity and regularity of the labeling scheme. The authors of [@Read2011a] suggested measuring the proportion of distinct label sets and the proportion of label sets with the maximum frequency. Consider the number of distinct label sets, also referred to as the label diversity [@Zhang2014], which can be defined as:

> there are multiple ways this is defined in the literature - still need to decide on which one I want to use

$$
LDiv(D)=|\{Y|\exists \boldsymbol{x}:(\boldsymbol{x},Y)\in D\}|,
$$
by [@Zhang2014]. ([@Read2011a] uses $\exists !$ instead of $\exists$ and $Y$ as a vector $\boldsymbol{y}$. I want to consider a way of defining it in matrix notation. Maybe with an indicator function. Some papers define it as $DL$ instead of $LDiv$.) The proportion of distinct label sets in $D$ is then

$$
PLDiv\{/PUniq/PDL\}(D)=\frac{1}{n}LDiv(D).
$$

The proportion of label sets with the maximum frequency is defined by [@Read2011a] as:

$$
PMax(D)=\max_{\boldsymbol{y}}\frac{\mathrm{count}(\boldsymbol{y},D)}{n},
$$
where $\mathrm{count}(\boldsymbol{y},D)$ is the frequency that label combination $\boldsymbol{y}$ is found in data set $D$. This represents the proportion of observations associated with the most frequently occurring label sets. High values of $PLDiv$ and $PMax$ indicate an irregular and skewed labeling scheme, respectively, *i.e.* a relatively high number of observations are associated with infrequent label sets and a relatively high number of observations are associated with the most common label sets. (*think about this again*) When this is the case, and the labels are modelled separately, the classifiers will suffer from the class imbalance problem, a common problem in supervised classification tasks. More detail about this will be adressed shortly.

Very little research has been done on how all these ML indicators affect the performance of a ML classifier. [@Chekina2011] made a worthy attempt. Their goal was to find a way of determining which ML algorithm to use given a data set with specific properties and with a specific evaluation metric to optimise. They approached this problem by training a so called meta-learner on a meta-data set containing the performance of multiple ML algorithms on benchmark data sets with different properties. This trained meta-learner is then able to predict which ML algorithm is most likely to give the best results in terms of a specific evaluation metric, given the properties of the data set to be analysed. Although we will not use their meta-learner for this thesis, we will consider some of the additional findings in their research. They found that the following properties (among others) of a ML data set was important to their trained meta-model (which was based on classification trees) in predicting which ML algorithm is most appropriate: $K$; $LDiv(D)$; $LCard(D)$; the standard deviation, skewness and kurtosis of the number of labels per observation in $D$; number of unconditionally dependent label pairs (based on what?); average of $\chi^{2}$ -scores of all dependent label pairs; number of classes with less than 2, 5 and 10 observations; ratio of classes with less than 2, 5, 10 and 50 observations; average, minimal and maximal entropy of labels (def of entropy?); average observations per class. This strengthens the argument that it is important to take ML indicators into account before the training process.

> Some rules that they found that I might refer to later:

+ for micro-AUC target evaluation measure if label cardinality of training data is above 3.028 then the 2BR method (among the single-classifiers) should be used.
+ Another example for an extracted rule is for ranking loss evaluation measure: if minimum of label entropies is zero (i.e. there is at least one certain label in the training set), number of labels is less than 53 and skewness of label cardinality is below or equal to 2.49 then the EPS method (among ensembles) should be used.

+ create table with properties of data to be used in this study.

+ [@Read2011a] defines a complexity measure as $n\times p\times K$

+ [@Gibaja2015a] long list of datasets. Other than MULAN: Plant and Human, Slashdot, LangLog, IMDB
+ [@Sorower]
+ https://manikvarma.github.io/downloads/XC/XMLRepository.html
+ yelp dataset: http://www.ics.uci.edu/~vpsaini/
+ also new yt8m

## Amazon

### Image Format

The data for this task comes from a set of images (also referred to as chips). Each chip is a small excerpt from a larger image of a specific scene in the Amazon taken by satellites. The chip size in pixels is $256\times 256$, representing roughly 90 hectares of land, and is taken from a larger scene of $6600\times 2200$ pixels. All of the satellite images were taken between January 1, 2016 and February 1, 2017. The format of these images differ from the standard image format. Each image contains four spectral bands: red (R), green (G), blue (B) and near infrared (NIR), where the standard format images usually only contain R, G and B. The additional NIR colour channel is common in remote sensing[^remsens] applications and supposedly allows for clear distinction between water and vegetation in satellite images, for example. 

Another difference between these images and the usual format is that these have pixel intensities in 16-bit digital number format as opposed to the usual 8-bit of standard RGB images. This allows the colours in the images to have a much higher range since 16-bit pixel intensities have 65536 ($2^{16}$) levels, compared to 256 levels of 8-bit images. This becomes useful, for example, to distinguish between very dark or very bright areas in an image. If the pixel values of a chip gets flattened out into a vector, it will be of size 262144 ($256\times 256 \times 4$). However, CNNs take the images in their array form as input.

[^remsens]: The use of satellite- or aircraft-based sensor technologies to detect and classify objects on Earth [https://en.wikipedia.org/wiki/Remote_sensing].

### Collection and Labelling of the Images

The image collection was created by first specifying a "wish list" of scenes containing the phenomena the creators wanted to be included, in addition to a rough estimate of the number of such scenes that are necessary for a sufficient representation in the final collection. This set of scenes was then searched for manually on Planet Explorer[^planexp]. From these scenes the 4-band chips were created. A schematic of this process can be seen in \autoref{fig:chipcreate}. The chips were labelled manually by crowd sourcing. The utmost care was taken to get a large and well-labelled dataset, but that does not mean the labels all correspond to the ground-truth, *i.e.* the data will contain some inherent error. The creators believe that the data has a reasonable high signal to noise ratio.

![Schematic of the image collection process.\label{fig:chipcreate}](figures/chipdesc.jpg)

[^planexp]: A web based interactive map of Earth consisting of satellite images, similar to Google Earth - www.planet.com/explorer

Note, the training and test splits were determined by the Kaggle competition creators. The training chips are labeled but at the time of writing this, the test chips are not yet made available to competitors. Predicted labels for the test chips can be submitted to Kaggle to evaluate in terms of the $F_{2}$-score, a metric which will be discussed in Chapter ??. This setup prevents competitors from using the test chips for training a classifier. There are 40479 training chips and 61191 test chips.

### Class Labels

The class labels for the images can be divided into three groups: atmospheric conditions, common land cover/use phenomena and rare land cover/use phenomena. In total there are 17 posssible labels. Each chip will have one atmospheric label and zero or more common and rare labels. Chips that are labeled as cloudy should have no other labels.

The atmospheric condition labels are: *clear*, *haze*, *partly cloudy* and *cloudy*. They are relevant to a chip when:

+ **clear**: there are no evidence of clouds.
+ **haze**: clouds are visible but they are not so opaque as to obscure the ground.
+ **partly cloudy**: scenes show opaque cloud cover over any portion of the image but the land cover/use phenomena are still visible.
+ **cloudy**: 90% of the image is obscured with opaque cloud cover.

```{r, include=FALSE, eval = FALSE}
library(tidyverse)
library(stringr)
train_labels <- read_csv("~/Documents/Kaggle/Amazon/train.csv")
label_list <- strsplit(train_labels$tags, " ")

atmos_labels <- c("clear", "haze", "partly_cloudy", "cloudy")
atmos_files <- lapply(atmos_labels, function(a) {
  lab_ind <- sapply(label_list, function(b) a %in% b)
  train_labels$image_name[lab_ind][1:3]
})

library(jpeg)
atmos_paths <- lapply(atmos_files, function(a) paste0("~/Documents/Kaggle/Amazon/train-jpg/", a, ".jpg"))
atmos_imgs <- lapply(atmos_paths, function(a) lapply(a, readJPEG))
library(ggmap)
library(gridExtra)
atmos_grobs <- lapply(atmos_imgs, function(a) lapply(a, ggimage))

grobs_arranged <- lapply(1:length(atmos_grobs), function(a){
  arrangeGrob(grobs = atmos_grobs[[a]], ncol = length(atmos_grobs[[a]]), respect = TRUE, left = atmos_labels[a])
})

ggsave("figures/atmos.png", grid.arrange(grobs = grobs_arranged, ncol = 1))
```

![Examples of chips with atmospheric labels. These (along with all the other chips plotted throughout the thesis) are the JPEG conversions of the original 4-band, 16-bit images.\label{fig:atmos-egs}](figures/atmos.png)

Examples of chips with atmospheric labels can be found in \autoref{fig:atmos-egs}. Each chip should only have one atmospheric label and therefore this classifying task simplifies to a multiclass problem. This allows for the option to break up the labeling task of all the labels into two tasks: a multiclass classification problem for the atmospheric labels and a multi-label classification problem for the land cover/use labels. This approach might save some computational time and give extra information to the multi-label learners for classifying the land cover/use labels. We will experiment with these approaches in Chapter ??.

The common land cover/use labels are: *primary*, *agriculture*, *water*, *habitation*, *road*, *cultivation* and *bare ground*. They are relevant to a chip when:

+ **primary**: it is primarily consisting of rain forest (virgin forest), *i.e.* dense tree cover.
+ **agriculture**: it contains any land cleared of trees that is being used for agriculture or range land.
+ **water**: it contains any one of the following: rivers, reservoirs, or oxbow lakes.
+ **habitation**: it contains human homes or buildings.
+ **road**: it contains any type of road.
+ **cultivation**: it shows signs of smaller-scale/informally cleared land for farming.
+ **bare ground**: it contains naturally (not the caused by humans) occurring tree-free areas.

```{r, include=FALSE, eval = FALSE}
common_cover_labels <- c("primary", "agriculture", "water", "habitation", "road", "cultivation", "bare_ground")
common_cover_files <- lapply(common_cover_labels, function(a) {
  lab_ind <- sapply(label_list, function(b) a %in% b)
  train_labels$image_name[lab_ind][1:3]
})

common_cover_paths <- lapply(common_cover_files, function(a) paste0("~/Documents/Kaggle/Amazon/train-jpg/", a, ".jpg"))
common_cover_imgs <- lapply(common_cover_paths, function(a) lapply(a, readJPEG))
common_cover_grobs <- lapply(common_cover_imgs, function(a) lapply(a, ggimage))

grobs_arranged <- lapply(1:length(common_cover_grobs), function(a){
  arrangeGrob(grobs = common_cover_grobs[[a]], ncol = length(common_cover_grobs[[a]]), respect = TRUE, left = common_cover_labels[a])
})

ggsave("figures/common-cover.png", grid.arrange(grobs = grobs_arranged, ncol = 1))
```

![Examples of chips with common land cover/use labels. \label{fig:common-cover-egs}](figures/common-cover.png)

Examples of chips with common land cover/use labels are found in \autoref{fig:common-cover-egs}. According to the competition page on Kaggle, small, single-dwelling habitations are often difficult to spot but usually appear as clumps of a few pixels that are bright white. Roads sometimes look very similar to rivers and therefore these two labels might be noisy. The NIR band might give a classifier additional information to help distinguish between the two. Cultivation is a subset of agriculture and is normally found near smaller villages, along major rivers or at the outskirts of agricultural areas. It typically covers very small areas.

The less common land cover/use labels are: *slash and burn*, *selective logging*, *blooming*, *conventional mine*, *artisinal mine* and *blow down*. Chips are tagged with these labels when:

+ **slash and burn**: there are signs of the farming method that involves the cutting and burning of the forest to create a field. These look like cultivation patches with black or dark brown areas.
+ **selective logging**: winding dirt roads are present adjacent to bare brown patches in otherwise primary rain forest. Selective logging is the practice of selectively removing high values tree species from the rainforest.
+ **blooming**: there are signs of trees flowering. Blooming is a natural phenomena where particular species of flowering trees bloom, fruit and flower at the same time. These trees are quite big and the phenomena can be seen in the chips. They usually appear as white dots.
+ **conventional mine**: it contains signs of large-scale legal mining operations.
+ **artisinal mine**: it contains signs of small-scale (sometimes illegal) mining operations.
+ **blow down**: there are signs of trees uprooted or broken by wind. High speed winds (~160km/h) in the Amazon are generated when the cold dry air from the Andes settles on top of the warm moist air in the rainforest and then sinks down with incredible force, toppling larger rainforest trees. These open areas are visible from space.

```{r, include=FALSE, eval = FALSE}
rare_cover_labels <- c("slash_burn", "selective_logging", "blooming", "conventional_mine", "artisinal_mine", "blow_down")
rare_cover_files <- lapply(rare_cover_labels, function(a) {
  lab_ind <- sapply(label_list, function(b) a %in% b)
  train_labels$image_name[lab_ind][1:3]
})

rare_cover_paths <- lapply(rare_cover_files, function(a) paste0("~/Documents/Kaggle/Amazon/train-jpg/", a, ".jpg"))
rare_cover_imgs <- lapply(rare_cover_paths, function(a) lapply(a, readJPEG))
rare_cover_grobs <- lapply(rare_cover_imgs, function(a) lapply(a, ggimage))

grobs_arranged <- lapply(1:length(rare_cover_grobs), function(a){
  arrangeGrob(grobs = rare_cover_grobs[[a]], ncol = length(rare_cover_grobs[[a]]), respect = TRUE, left = rare_cover_labels[a])
})

ggsave("figures/rare-cover.png", grid.arrange(grobs = grobs_arranged, ncol = 1))
```

![Examples of chips with less common land cover/use labels. \label{fig:rare-cover-egs}](figures/rare-cover.png)

Examples of chips with these less common land cover/use labels are given in \autoref{fig:rare-cover-egs}. These labels are more challenging to identify in the chips and since they also appear less frequently, it might be difficult for the classifier to learn these labels. The imbalance in the class distribution is apparent in \autoref{fig:classdist}.

```{r, cache=TRUE, echo=FALSE, fig.cap="Class distribution of the labels in the training set. \\label{fig:classdist}"}
library(tidyverse)
library(stringr)
train_labels <- read_csv("data/train_v2.csv")
label_list <- strsplit(train_labels$tags, " ")

as.data.frame(table(unlist(label_list))) %>% 
  rename(Label = Var1) %>% 
  mutate(Label = factor(Label, levels = Label[order(-Freq)])) %>% 
  ggplot(aes(Label, Freq)) +
  geom_histogram(stat="identity") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)) +
  ylab("Frequency") #+ ggtitle()
```

## NUS-WIDE

+ Nus-wide: a real-world web image database from national university of singapore [@Chua2009]

## Pascal VOC

+ 2007: The pascal visual object classes (voc) challenge [@Everingham2010]
+ 2012: [@Everingham2012]

## MS-COCO

+ Microsoft coco: Common objects in contex [@Lin2014]

## WIDER-Atribute

+ Human attribute recognition by deep hierarchical contexts [@Li2016]
