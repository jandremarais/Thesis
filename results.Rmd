# Results (/Application)
\label{chp:results}

## Introduction

Main aim is to compare methods on more datasets and in terms of more evaluation metrics. The datasets to be evaluated on are described in Appendix \ref{app:data}. These are the most popular and most recent multi-labelled image datasets.

+ mention the importance of evaluating on multiple ml datasets with different properties.
+ mention the importance of comparing in terms of many evaluation metrics. Which am I going to use here? Probably one of each, example-based, label-based (micro and macro).

Where possible, the basic framework will be to extract features from input images with pretrained (on ImageNet) CNN. The proposals in the literature will be built on top of these features (see transfer learning in Chapter \ref{chp:dnn}). This will not give the best results because these features are trained for single label images. However, they are sufficient for comparison purposes and are much less time consuming. (Maybe I can indentify the most promosing method and see what effect fine-tuning convolutional layers will have on it. This will also allow other bells and whistles such as data augmentation.) The architecture chosen to do the feature extraction is VGG16 because of its simplicity and proven effectiveness in transfer learning. 

+ maybe also shrink images for faster computations.
+ add time taken for learning as a metric.
+ 5- or 10-fold cross-validation for more accurate errors and standard deviations.
+ baseline: sigmoid classification layer optimising binary cross-entropy

## Optimising Multi-Label Loss Functions

+ see the effect of optimising ML loss functions on different metrics.

## Label Embedding Approaches

+ evaluate the effect of label embedding approaches
+ does it help with class imbalance?

## Novel Architectures

+ evaluate the following classification heads:
+ spatial regulrisation network
+ cnn-rnn and improved version
+ Hypothesis CNN Pooling
+ Context gating
+ Chaining
+ Label Processing layers

## Discussion

+ is there a 'best' method?
+ are there any patterns?
+ do they perform as reported by original papers?

## Experiments

First want to look at how the important papers did their experiments.

[@Gong2013] did their experiments in the following way:

+ NUS-WIDE:

    + ignored images without tags
    + used 150000 images for training and the rest for testing
    + evaluated using per-class precision and recall, and overall precision and recall
    + used a custom cnn similar to overfeat for feature extraction
    + compared to visual feature extraction + classifier, and CNN extractor plus classifier with different loss functions.

[@Wei2014] did their experiments in the following way:

+ Pascal VOC2007 and 2012

    + seems like they used predefined splits for train/val/test
    + evaluated using AP and mAP.
    + pretrained on ImageNet
    + copmared to other manual feature extractions and CNN extractions plus classifier
    + did not really follow the hypothesis pooling part.
    + also giver per class score seprately
    + as second glance, does not look like real MLC since parts of images are extracted and then single label classification is done.
