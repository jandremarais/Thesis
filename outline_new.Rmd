---
output: pdf_document
---

# Neural Networks \label{chp:nn}

## Introduction

In \autoref{chp:intro} we have introduced all of the basic components for building a Neural Network. Recall the linear classifier that mapped the inputs, $\boldsymbol{x}$, to a vector of class scores, $\boldsymbol{s}$, $\boldsymbol{s}=W\boldsymbol{x}$, where $W$ is a matrx of weights. Here $W$ has $K$ rows and $p$ columns corresponding to the number of classes and size of the inputs respectively. As mentioned in \autoref{chp:intro}, a Neural Network has a more complicated mapping from the inputs to the class scores. An example Neural Network would instead have a mapping like, $\boldsymbol{s}=W_{2}\max(0,W_{1}\boldsymbol{x})$. This time, for example, $W_{1}$ could be a matrix transforming the inputs to a 100-dimensional vector, thus of size $100\times p$. The function $\max(0, \cdot)$ introduces a non-linearity that is applied element-wise. It simply thresholds all values below zero to zero. There are several types of non-linearities that can be applied, but this is a common choice. Finally, $W_{2}$, is a matrix of size $K\times 100$, mapping the intermediate vector to the final class scores. The key difference here is the non-linearity. If it is left out, the two matrices could be collapsed into one and therefore the predicted class scores would again be a linear function of the input. $W_{1}$, $W_{2}$ is learned through stochastic gradient descent (SGD), their gradients are derived with the chain rule and computed with backpropogation.

The above mapping is an example of a two-layer network. A three-layer neural network may look something like this:

$$
\boldsymbol{s}=W_{3}\max(0, W_{2}\max(0, W_{1}\boldsymbol{x})).
$$
Now there are two non-linearities and $W_{1}$, $W_{2}$, $W_{3}$ are all parameters to be learned. Their sizes, which determine the size of the intermediate layers (vectors), are seen as hyperparameters and how they can be determined will be discussed shortly. In the next section we will show where the name, Neural Networks, come from and ...




# Convolutional Neural Networks

## Introduction

## Core Layers

### Convolutional Layers

### Pooling Layers

### Activation Layers

### Fully Connected Layers

### Learning Rates

+ cyclical
+ decay
+ momentum

### Freezing Layers

+ https://arxiv.org/abs/1706.04983

## Summary

# Convolutional Neural Networks in Practice (other title)

## Introduction

## Visualizing CNN's

## Transfer Learning

+ ImageNet

## Famous Architectures

+ AlexNet, Inception, ...

### VGG

### ResNet

### DenseNet

## Regularization

### Normalization Layers (maybe move to core)

### Data Augmentaion

### Pseudo-Labelling and Knowledge-Distillation

### Dropout

## Generalization (?)

+ https://arxiv.org/abs/1706.01350

# Multi-Label Convolutional Neural Networks

## General Multi-Label Learning Approaches

## Spatial Regularization Networks

+ https://arxiv.org/pdf/1702.05891.pdf

## From Single to Multi Output Paper ()

## RNN-CNN paper ()

+ Other object detection

# Things that need a place:

+ challenges for image classification: (maybe in CNNs in practive)

    + http://cs231n.github.io/classification/

+ Feature learning
+ one-shot learning:

    + https://github.com/sorenbouma/keras-oneshot
    + https://github.com/fchollet/keras/blob/master/examples/mnist_siamese_graph.py
    + https://sorenbouma.github.io/blog/oneshot/
    
+ multi-task learning:

    + https://arxiv.org/abs/1706.05137
    
+ test time augmentation
+ relational learning: 

    + https://arxiv.org/pdf/1706.01427.pdf

+ Fully-Convolutional Networks
+ Spatial Pyramid Pooling: https://github.com/yhenon/keras-spp
+ AutoML:

    + https://research.googleblog.com/2017/05/using-machine-learning-to-explore.html

