# Multi-Label Learning

+ more theoretical work at [@Gasse2015]. Mentions: Finding theoretically correct algorithms for other non label-wise decomposable loss functions is still a great challenge.

+ more theory: Optimizing the F-Measure in Multi-Label Classification: Plug-in Rule Approach versus Structured Loss Minimization

Other solutions: exploit correlation of labels from both types conditional and unconditional
dependencies, features selection methods that are designed
especially to handle multi label datasets, and having new
stratification methods that are suitable to the nature of multi
label datasets (copied from [@Alazaidah2016])

+  most of these algorithms suffer from high complexity in the learning process [10]. Based on that, the true challenge is to exploit high order labels correlations locally and maintain a linear complexity at the same time [2].(copied from [@Alazaidah2016])

+ Ensembles are well known for their effect of increasing overall accuracy and overcoming over-fitting, as well as allowing parallelism. The main idea behind ensembles is to exploit the fact that different classifiers may do well in different aspects of the learning task so combining them could improve overall performance. Ensembles have been extensively used in literature [13] with stacking [14], bagging [15] and boosting [16] being the main methods employed. In the context of multi-label problems, [17] proposes a fusion method where the probabilistic outputs of heterogeneous classifiers are averaged and the labels above a threshold are chosen. Copied from [@Papanikolaou] (can maybe use to explain why these methods perform better and not because of label dependence)
+ evidence of stacking working [@Tsoumakase]. Read conclusions chapter. Ensembling effective. Linear models good for text classification. Thresholding important.
