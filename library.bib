Automatically generated by Mendeley Desktop 1.17.9
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Read2011a,
abstract = {The widely known binary relevance method for multi-label classification, which considers each label as an independent binary problem, has often been overlooked in the literature due to the perceived inadequacy of not directly modelling label correlations. Most current methods invest considerable complexity to model interdependencies between labels. This paper shows that binary relevance-based methods have much to offer, and that high predictive performance can be obtained without impeding scalability to large datasets. We exemplify this with a novel classifier chains method that can model label correlations while maintaining acceptable computational complexity. We extend this approach further in an ensemble framework. An extensive empirical evaluation covers a broad range of multi-label datasets with a variety of evaluation metrics. The results illustrate the competitiveness of the chaining method against related and state-of-the-art methods, both in terms of predictive performance and time complexity.},
author = {Read, Jesse and Pfahringer, Bernhard and Holmes, Geoff and Frank, Eibe and {Brodley Read}, Carla J and Pfahringer, B and Holmes, G and Frank, E},
doi = {10.1007/s10994-011-5256-5},
file = {:home/jan/Documents/Mendeley Desktop/Read et al/Mach Learn/Read et al. - 2011 - Classifier chains for multi-label classification.pdf:pdf},
journal = {Mach Learn},
keywords = {Ensemble methods {\textperiodcentered},Multi-label classification {\textperiodcentered},Problem transformation {\textperiodcentered},Scalable methods},
number = {85},
title = {{Classifier chains for multi-label classification}},
url = {http://download.springer.com/static/pdf/44/art{\%}253A10.1007{\%}252Fs10994-011-5256-5.pdf?originUrl=http{\%}3A{\%}2F{\%}2Flink.springer.com{\%}2Farticle{\%}2F10.1007{\%}2Fs10994-011-5256-5{\&}token2=exp=1490608886{~}acl={\%}2Fstatic{\%}2Fpdf{\%}2F44{\%}2Fart{\%}25253A10.1007{\%}25252Fs10994-011-5256-},
volume = {85},
year = {2011}
}
@article{Godbole,
abstract = {In this paper we present methods of enhancing existing discriminative classifiers for multi-labeled predictions. Discriminative methods like support vector machines perform very well for uni-labeled text classification tasks. Multi-labeled classification is a harder task subject to relatively less attention. In the multi-labeled setting, classes are often related to each other or part of a is-a hierarchy. We present a new technique for combining text features and features indicating relationships between classes, which can be used with any discriminative algorithm. We also present two enhancements to the margin of SVMs for building better models in the presence of overlapping classes. We present results of experiments on real world text benchmark datasets. Our new methods beat accuracy of existing methods with statistically significant improvements.},
archivePrefix = {arXiv},
arxivId = {10.1007/978-3-540-24775-3{\_}5},
author = {Godbole, Shantanu and Sarawagi, Sunita},
doi = {10.1007/978-3-540-24775-3_5},
eprint = {978-3-540-24775-3{\_}5},
file = {:home/jan/Documents/Mendeley Desktop/Godbole, Sarawagi/Lecture Notes in Computer Science/Godbole, Sarawagi - 2004 - Discriminative Methods for Multi-labeled Classification.pdf:pdf},
isbn = {978-3-540-22064-0},
issn = {03029743},
journal = {Lecture Notes in Computer Science},
pages = {22--30},
primaryClass = {10.1007},
title = {{Discriminative Methods for Multi-labeled Classification}},
url = {http://link.springer.com/10.1007/978-3-540-24775-3{\_}5},
volume = {3056},
year = {2004}
}
@article{Berger,
abstract = {Multi-label text classification has been applied to a multitude of tasks, including document indexing, tag suggestion, and sentiment classification. However, many of these methods disregard word order, opting to use bag-of-words models or TF- IDF weighting to create document vectors. With the advent of powerful semantic embeddings, such as word2vec and GloVe, we explore how word embeddings and word order can be used to improve multi-label learning. Specifically, we explore how both a convolutional neural network (CNN) and a recurrent network with a gated recurrent unit (GRU) can independently be used with pre-trained word2vec embeddings to solve a large scale multi-label text classification problem. On a data set of over two million documents and 1000 potential labels, we demonstrate that both a CNN and a GRU provide substantial improvement over a Binary Relevance model with a bag-of-words representation.},
author = {Berger, Mark J},
file = {:home/jan/Documents/Mendeley Desktop/Berger/Technical Report/Berger - 2014 - Large Scale Multi-label Text Classification with Semantic Word Vectors.pdf:pdf},
journal = {Technical Report},
keywords = {Multi-Label,Text Classification},
mendeley-tags = {Multi-Label,Text Classification},
pages = {1--8},
title = {{Large Scale Multi-label Text Classification with Semantic Word Vectors}},
year = {2014}
}
@article{Diplaris2005,
abstract = {Nowadays, the number of protein sequences being stored in central protein databases from labs all over the world is constantly increasing. From these proteins only a fraction has been experimentally analyzed in order to de- tect their structure and hence their function in the corresponding organism. The reason is that experimental determination of structure is labor-intensive and quite time-consuming. Therefore there is the need for automated tools that can classify new proteins to structural families. This paper presents a comparative evaluation of several algorithms that learn such classification models from data concerning patterns of proteins with known structure. In addition, several ap- proaches that combine multiple learning algorithms to increase the accuracy of predictions are evaluated. The results of the experiments provide insights that can help biologists and computer scientists design high-performance protein classification systems of high quality.},
author = {Diplaris, Sotiris and Tsoumakas, Grigorios and Mitkas, Pericles A. and Vlahavas, Ioannis},
doi = {10.1007/11573036_42},
file = {:home/jan/Documents/Mendeley Desktop/Diplaris et al/Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)/Diplaris et al. - 2005 - Protein classification with multiple algorithms.pdf:pdf},
isbn = {3540296735},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {448--456},
title = {{Protein classification with multiple algorithms}},
volume = {3746 LNCS},
year = {2005}
}
@article{DeCarvalho2009,
abstract = {Industries have to design and produce performing and reliable systems. Nevertheless, designers suffer from the diversity of methods, which are not really adequate to their needs. Authors highlight the need of close interactions between product and project design, often treated either independently or sequentially, necessary to improve system design, and logistics in this context. Strengthening the links between product design and project management processes is an ongoing challenge, and this situation relies on perfect control of methods, tools and know-how, both on the technical side as well as on the organizational side. The aim of our work is to facilitate the project manager's decision making, thus allowing him to define, follow and adapt a working plan, while still considering various organizational options. From these options, the project manager chooses the scheme that best encompasses the project's objectives with respect to costs, delay and risks, without neglecting performance and safety. To encourage the project manager to explore various possibilities, we developed and tested a heuristic based on ant colony optimization and evolutionary algorithm adapted for multi-objective problems. Its hybridization with a tabu search and a greedy algorithm were performed in order to accelerate convergence of the research study and to reduce the cost engendered by the evaluation process. The experiments carried out reveals that it was possible to offer the decision maker a reduced number of solutions that he can evaluate more accurately in order to choose one according to technical, economic and financial criteria. {\textcopyright} 2009 Springer-Verlag Berlin Heidelberg.},
author = {de Carvalho, Andr{\'{e}} C. P. L. F. and Freitas, Alex A.},
doi = {10.1007/978-3-642-01536-6_8},
isbn = {9783642015359},
issn = {1860949X},
journal = {Studies in Computational Intelligence},
pages = {177--195},
title = {{A tutorial on multi-label classification techniques}},
url = {http://link.springer.com/10.1007/978-3-642-01536-6{\_}8},
volume = {205},
year = {2009}
}
@article{Read,
abstract = {—The area of multi-label classification has rapidly developed in recent years. It has become widely known that the baseline binary relevance approach can easily be outperformed by methods which learn labels together. A number of methods have grown around the label powerset approach, which models label combinations together as class values in a multi-class problem. We describe the label-powerset-based solutions under a general framework of meta-labels and provide some theoret-ical justification for this framework which has been lacking; explaining how meta-labels essentially allow a random projection into a space where non-linearities can easily be tackled with established linear learning algorithms. The proposed framework enables comparison and combination of related approaches to different multi-label problems. We present a novel model in the framework and evaluate it empirically against several high-performing methods, with respect to predictive performance and scalability, on a number of datasets and evaluation metrics. This deployment obtains competitive accuracy for a fraction of the computation required by the current meta-label methods for multi-label classification.},
author = {Read, Jesse and Puurula, Antti and Bifet, Albert},
doi = {10.1109/ICDM.2014.38},
file = {:home/jan/Documents/Mendeley Desktop/Read, Puurula, Bifet/2014 IEEE International Conference on Data Mining/Read, Puurula, Bifet - 2014 - Multi-label Classification with Meta-Labels.pdf:pdf},
isbn = {978-1-4799-4302-9},
journal = {2014 IEEE International Conference on Data Mining},
pages = {941--946},
title = {{Multi-label Classification with Meta-Labels}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7023427},
year = {2014}
}
@article{Bhowmick2009,
abstract = {Multiple emotions are often triggered in readers in response to text stimuli like news article. In this paper, we present a novel method for classifying news sentences into multiple emotion categories using an ensemble based multi-label classification technique called RAKEL. The emotion data consists of 1305 news sentences and the emotion classes considered are disgust, fear, happiness and sadness. Words are the most obvious choice as feature for emotion recognition. In addition to that we have introduced two novel feature sets: polarity of subject, verb and object of the sentences and semantic frames. Experiments concerning the comparison of features revealed that semantic frame feature combined with polarity based feature performs best in emotion classification. Experiments on feature selection over word and semantic frame features have been performed in order to handle feature sparseness problem. In both word and semantic frame feature, improvements in the overall performance have been observed after optimal feature selection.},
author = {Bhowmick, Plaban Kumar and Basu, Anupam and Mitra, Pabitra},
file = {:home/jan/Documents/Mendeley Desktop/Bhowmick, Basu, Mitra/Computer and Information Science/Bhowmick, Basu, Mitra - 2009 - Reader Perspective Emotion Analysis in Text through Ensemble based Multi-Label Classification Framework.pdf:pdf},
issn = {19138997},
journal = {Computer and Information Science},
keywords = {Emotion Analysis,Multi-Label,emotion classification,ensemble classifier,feature selection,multi-label classification},
mendeley-tags = {Emotion Analysis,Multi-Label},
number = {4},
pages = {64--74},
title = {{Reader Perspective Emotion Analysis in Text through Ensemble based Multi-Label Classification Framework}},
url = {http://ccsenet.org/journal/index.php/cis/article/view/3872/0},
volume = {2},
year = {2009}
}
@article{Zhanga,
abstract = {—Multi-label learning deals with the problem where each example is represented by a single instance (feature vector) while associated with a set of class labels. Existing approaches learn from multi-label data by manipulating with identical feature set, i.e. the very instance representation of each example is employed in the discrimination processes of all class labels. However, this popular strategy might be suboptimal as each label is supposed to possess specific characteristics of its own. In this paper, another strategy to learn from multi-label data is studied, where label-specific features are exploited to benefit the discrimination of different class labels. Accordingly, an intuitive yet effective algorithm named LIFT, i.e. multi-label learning with Label specIfic FeaTures, is proposed. LIFT firstly constructs features specific to each label by conducting clustering analysis on its positive and negative instances, and then performs training and testing by querying the clustering results. Comprehensive experiments on a total of seventeen benchmark data sets clearly validate the superiority of LIFT against other well-established multi-label learning algorithms as well as the effectiveness of label-specific features.},
author = {Zhang, Min-Ling and Wu, Lei},
file = {:home/jan/Documents/Mendeley Desktop/Zhang, Wu/Unknown/Zhang, Wu - Unknown - LIFT Multi-Label Learning with Label-Specific Features.pdf:pdf},
keywords = {Index Terms—machine learning,label correlations,label-specific features,multi-label learning},
title = {{LIFT: Multi-Label Learning with Label-Specific Features}}
}
@article{Dembczynski2010,
abstract = {In multi-label classification (MLC), each instance is associated with$\backslash$na subset of labels instead of a single class, as in conventional$\backslash$nclassification, and this generalization enables the definition of$\backslash$na multitude of loss functions. Indeed, a large number of losses has$\backslash$nalready been proposed and is commonly applied as performance metrics$\backslash$nin experimental studies. However, even though these loss functions$\backslash$nare of a quite different nature, a concrete connection between the$\backslash$ntype of multi-label classifier used and the loss to be minimized$\backslash$nis rarely established, implicitly giving the misleading impression$\backslash$nthat the same method can be optimal for different loss functions.$\backslash$nIn this paper, we elaborate on risk minimization and the connection$\backslash$nbetween loss functions in MLC, both theoretically and empirically.$\backslash$nIn particular, we compare two important loss functions, namely the$\backslash$nHamming loss and the subset 0/1 loss. We perform a regret analysis,$\backslash$nshowing how poor a classifier intended to minimize the subset 0/1$\backslash$nloss can become in terms of Hamming loss and vice versa. The theoretical$\backslash$nresults are corroborated by experimental studies, and their implications$\backslash$nfor MLC methods are discussed in a broader context. 漏 2010 Springer-Verlag$\backslash$nBerlin Heidelberg.},
author = {Dembczy{\'{n}}ski, Krzysztof and Waegeman, Willem and Cheng, Weiwei and H{\"{u}}llermeier, Eyke},
doi = {10.1007/978-3-642-15880-3_24},
isbn = {364215879X},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 1},
pages = {280--295},
title = {{Regret analysis for performance metrics in multi-label classification: The case of hamming and subset zero-one loss}},
volume = {6321 LNAI},
year = {2010}
}
@article{Tsoumakasa,
abstract = {Nowadays, multi-label classification methods are increasingly required by modern applications, such as protein function classification, music categorization and semantic scene classification. This paper intro-duces the task of multi-label classification, organizes the sparse related literature into a structured presentation and performs comparative ex-perimental results of certain multi-label classification methods. It also contributes the presentation of an undocumented method and the defi-nition of a concept for the quantification of the multi-label nature of a data set.},
author = {Tsoumakas, Grigorios and Katakis, Ioannis and Vlahavas, Ioannis},
file = {:home/jan/Documents/Mendeley Desktop/Tsoumakas, Katakis, Vlahavas/Proceedings of the 2nd ADBIS Workshop on Data Mining and Knowledge Discovery (ADMKD 2006)/Tsoumakas, Katakis, Vlahavas - Unknown - A Review of Multi-Label Classification Methods.pdf:pdf},
title = {{A Review of Multi-Label Classification Methods}}
}
@article{Elisseeff2001,
abstract = {ML-learning; SVM;Ranking based system},
author = {Elisseeff, a. and Weston, Jason},
file = {:home/jan/Documents/Mendeley Desktop/Elisseeff, Weston/Advances in neural information processing systems/Elisseeff, Weston - 2001 - A kernel method for multi-labelled classification.pdf:pdf},
isbn = {0262042088},
issn = {10495258},
journal = {Advances in neural information processing systems},
pages = {681--687},
title = {{A kernel method for multi-labelled classification}},
year = {2001}
}
@article{Yu,
abstract = {Abstract The multi-label classification problem has generated significant interest in recent years. However, existing approaches do not adequately address two key challenges:(a) scaling up to problems with a large number (say millions) of labels, and (b) handling data ...},
author = {Yu, H F and Jain, P and Kar, P and Dhillon, I S},
file = {:home/jan/Documents/Mendeley Desktop/Yu et al/Icml/Yu et al. - 2014 - Large-scale Multi-label Learning with Missing Labels.pdf:pdf},
journal = {Icml},
title = {{Large-scale Multi-label Learning with Missing Labels.}},
url = {http://www.jmlr.org/proceedings/papers/v32/yu14.pdf},
year = {2014}
}
@article{Schapire2000a,
abstract = {This work focuses on algorithms which learn from examples to perform multiclass text and speech categorization tasks. Our approach is based on a new and improved family of boosting algorithms. We describe in detail an implementation, called BoosTexter, of the new boosting algorithms for text categorization tasks. We present results comparing the performance of BoosTexter and a number of other text-categorization algorithms on a variety of tasks. We conclude by describing the application of our system to automatic call-type identification from unconstrained spoken customer responses.},
author = {Schapire, Robert E and Singer, Y},
file = {:home/jan/Documents/Mendeley Desktop/Schapire, Singer/Machine learning/Schapire, Singer - 2000 - BoosTexter A boosting- based system for text categorization(2).pdf:pdf},
journal = {Machine learning},
keywords = {boosting algorithms,multiclass classification problems,text and speech categorization},
number = {23},
pages = {135--168},
title = {{BoosTexter: A boosting- based system for text categorization}},
url = {http://link.springer.com/article/10.1023/A:1007649029923},
volume = {39},
year = {2000}
}
@article{Roth2007,
abstract = {BACKGROUND: We develop a probabilistic model for combining kernel matrices to predict the function of proteins. It extends previous approaches in that it can handle multiple labels which naturally appear in the context of protein function.$\backslash$n$\backslash$nRESULTS: Explicit modeling of multilabels significantly improves the capability of learning protein function from multiple kernels. The performance and the interpretability of the inference model are further improved by simultaneously predicting the subcellular localization of proteins and by combining pairwise classifiers to consistent class membership estimates.$\backslash$n$\backslash$nCONCLUSION: For the purpose of functional prediction of proteins, multilabels provide valuable information that should be included adequately in the training process of classifiers. Learning of functional categories gains from co-prediction of subcellular localization. Pairwise separation rules allow very detailed insights into the relevance of different measurements like sequence, structure, interaction data, or expression data. A preliminary version of the software can be downloaded from http://www.inf.ethz.ch/personal/vroth/KernelHMM/.},
author = {Roth, Volker and Fischer, Bernd},
doi = {10.1186/1471-2105-8-S2-S12},
file = {:home/jan/Documents/Mendeley Desktop/Roth, Fischer/BMC bioinformatics/Roth, Fischer - 2007 - Improved functional prediction of proteins by learning kernel combinations in multilabel settings.pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Algorithms,Artificial Intelligence,Computer Simulation,Fungal Proteins,Fungal Proteins: chemistry,Fungal Proteins: classification,Fungal Proteins: metabolism,Models, Biological,Sequence Analysis, Protein,Sequence Analysis, Protein: methods,Signal Transduction,Signal Transduction: physiology,Structure-Activity Relationship,Yeasts,Yeasts: metabolism},
pages = {S12},
pmid = {17493250},
title = {{Improved functional prediction of proteins by learning kernel combinations in multilabel settings.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1892070{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {8 Suppl 2},
year = {2007}
}
@article{Tsoumakas2007,
author = {Tsoumakas, Grigorios and Katakis, Ioannis},
doi = {10.4018/978-1-60566-058-5.ch021},
file = {:home/jan/Documents/Mendeley Desktop/Tsoumakas, Katakis/Database Technologies/Tsoumakas, Katakis - 2007 - Multi-Label Classification.pdf:pdf},
isbn = {978-1-4244-1065-1},
journal = {Database Technologies},
keywords = {Multi-Label,data forecasting,data mining,decision models,knowledge discovery,text data-},
mendeley-tags = {Multi-Label},
number = {3},
pages = {309--319},
title = {{Multi-Label Classification}},
url = {http://services.igi-global.com/resolvedoi/resolve.aspx?doi=10.4018/978-1-60566-058-5.ch021},
volume = {3},
year = {2007}
}
@article{Chen2012a,
abstract = {Label space dimension reduction (LSDR) is an efficient and effective paradigm for multi-label classification with many classes. Existing approaches to LSDR, such as compressive sensing and principal label space transformation, exploit only the label part of the dataset, but not the feature part. In this paper, we propose a novel approach to LSDR that considers both the label and the feature parts. The approach, called conditional principal label space transformation, is based on minimizing an upper bound of the popular Hamming loss. The minimization step of the approach can be carried out efficiently by a simple use of singular value decomposition. In addition, the approach can be extended to a kernelized version that allows the use of sophisticated feature combinations to assist LSDR. The experimental results verify that the proposed approach is more effective than existing ones to LSDR across many real-world datasets. 1},
annote = {NULL},
author = {Chen, Yn and Lin, Ht},
file = {:home/jan/Documents/Mendeley Desktop/Chen, Lin/Advances in Neural Information Processing Systems/Chen, Lin - 2012 - Feature-aware Label Space Dimension Reduction for Multi-label Classification.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {1538--1546},
title = {{Feature-aware Label Space Dimension Reduction for Multi-label Classification}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2012{\_}0728.pdf},
year = {2012}
}
@article{McCallum1999,
abstract = {In many important document classication tasks, documents may each be associated with multiple class labels. This paper describes a Bayesian classication approach in which the multiple classes that comprise a document are represented by a mixture model. While the labeled training data indicates which classes were responsible for generating a document, it does not indicate which class was responsible for generating each word. Thus we use EM to ll in this missing value, learning both the...},
author = {McCallum, Andrew},
doi = {10.1.1.35.888},
file = {:home/jan/Documents/Mendeley Desktop/McCallum/AAAI'99 Workshop on Text Learning/McCallum - 1999 - “Multi-label text classification with a mixture model trained by EM.pdf:pdf},
journal = {AAAI'99 Workshop on Text Learning},
pages = {1--7},
title = {{Multi-label text classification with a mixture model trained by EM}},
url = {http://www.kyriakides.net/CBCL/references/Papers/mccallum99multilabel.pdf},
year = {1999}
}
@article{Li2013,
abstract = {Multi-label classification has attracted an increasing amount of attention in recent years. To this end, many algorithms have been developed to classify multi-label data in an effective manner. However, they usually do not consider the pairwise relations indicated by sample labels, which actually play important roles in multi-label classification. Inspired by this, we naturally extend the traditional pairwise constraints to the multi-label scenario via a flexible thresholding scheme. Moreover, to improve the generalization ability of the classifier, we adopt a boosting-like strategy to construct a multi-label ensemble from a group of base classifiers. To achieve these goals, this paper presents a novel multi-label classification framework named Variable Pairwise Constraint projection for Multi-label Ensemble (VPCME). Specifically, we take advantage of the variable pairwise constraint projection to learn a lower-dimensional data representation, which preserves the correlations between samples and labels. Thereafter, the base classifiers are trained in the new data space. For the boosting-like strategy, we employ both the variable pairwise constraints and the bootstrap steps to diversify the base classifiers. Empirical studies have shown the superiority of the proposed method in comparison with other approaches. ?? 2012 Elsevier Inc. All rights reserved.},
author = {Li, Ping and Li, Hong and Wu, Min},
doi = {10.1016/j.ins.2012.07.066},
file = {:home/jan/Documents/Mendeley Desktop/Li, Li, Wu/Information Sciences/Li, Li, Wu - 2013 - Multi-label ensemble based on variable pairwise constraint projection.pdf:pdf},
issn = {00200255},
journal = {Information Sciences},
keywords = {Boosting,Constraint projection,Ensemble learning,Multi-Label,Multi-label classification,Variable pairwise constraints},
mendeley-tags = {Multi-Label},
pages = {269--281},
title = {{Multi-label ensemble based on variable pairwise constraint projection}},
volume = {222},
year = {2013}
}
@article{Papanikolaou,
abstract = {This paper documents the systems that we developed for our participation in the BioASQ 2014 large-scale bio-medical semantic indexing and question answering challenge. For the large-scale semantic indexing task, we employed a novel multi-label ensemble method con-sisting of support vector machines, labeled Latent Dirichlet Allocation models and meta-models predicting the number of relevant labels. This method proved successful in our experiments as well as during the compe-tition. For the question answering task we combined different techniques for scoring of candidate answers based on recent literature.},
author = {Papanikolaou, Yannis and Dimitriadis, Dimitrios and Tsoumakas, Grigorios and Laliotis, Manos and Markantonatos, Nikos and Vlahavas, Ioannis},
file = {:home/jan/Documents/Mendeley Desktop/Papanikolaou et al/CEUR Workshop Proceedings/Papanikolaou et al. - 2014 - Ensemble approaches for large-scale multi-label classification and question answering in biomedicine.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
keywords = {BioASQ,Ensemble methods,Latent Dirichlet Allocation,Multi-label learning,Support vector machines},
pages = {1348--1360},
title = {{Ensemble approaches for large-scale multi-label classification and question answering in biomedicine}},
volume = {1180},
year = {2014}
}
@article{Yang1999,
abstract = {This paper focuses on a comparative evaluation of a wide-range of text categorization methods, including previously published results on the Reuters corpus and new results of additional experiments. A controlled study using three classifiers, kNN, LLSF and WORD, was conducted to examine the impact of configuration variations in five versions of Reuters on the observed performance of classifiers. Analysis and empirical evidence suggest that the evaluation results on some versions of Reuters were significantly affected by the inclusion of a large portion of unlabelled documents, mading those results difficult to interpret and leading to considerable confusions in the literature. Using the results evaluated on the other versions of Reuters which exclude the unlabelled documents, the performance of twelve methods are compared directly or indirectly. For indirect compararions, kNN, LLSF and WORD were used as baselines, since they were evaluated on all versions of Reuters that exclude the unlabelled documents. As a global observation, kNN, LLSF and a neural network method had the best performance; except for a Naive Bayes approach, the other learning algorithms also performed relatively well.},
author = {Yang, Yiming},
doi = {10.1023/A:1009982220290},
file = {:home/jan/Documents/Mendeley Desktop/Yang/Information Retrieval/Yang - 1999 - An Evaluation of Statistical Approaches to Text Categorization.pdf:pdf},
isbn = {1386-4564},
issn = {1612-1880},
journal = {Information Retrieval},
keywords = {comparative study,evaluation,statistical learning algorithms,text categorization},
number = {1},
pages = {69--90},
title = {{An Evaluation of Statistical Approaches to Text Categorization}},
url = {http://www.cs.cmu.edu/{~}{\%}5Cnhttp://dx.doi.org/10.1023/A:1009982220290},
volume = {1},
year = {1999}
}
@article{Koyejo2015,
abstract = {Multilabel classification is rapidly developing as an important aspect of modern predictive modeling, motivating study of its theoretical aspects. To this end, we propose a framework for constructing and analyzing multilabel classification met-rics which reveals novel results on a parametric form for population optimal clas-sifiers, and additional insight into the role of label correlations. In particular, we show that for multilabel metrics constructed as instance-, micro-and macro-averages, the population optimal classifier can be decomposed into binary classi-fiers based on the marginal instance-conditional distribution of each label, with a weak association between labels via the threshold. Thus, our analysis extends the state of the art from a few known multilabel classification metrics such as Ham-ming loss, to a general framework applicable to many of the classification metrics in common use. Based on the population-optimal classifier, we propose a compu-tationally efficient and general-purpose plug-in classification algorithm, and prove its consistency with respect to the metric of interest. Empirical results on synthetic and benchmark datasets are supportive of our theoretical findings.},
author = {Koyejo, Oluwasanmi O. and Natarajan, Nagarajan and Ravikumar, Pradeep K. and Dhillon, Inderjit S},
file = {:home/jan/Documents/Mendeley Desktop/Koyejo et al/Advances in Neural Information Processing Systems/Koyejo et al. - 2015 - Consistent Multilabel Classification.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {3303--3311},
title = {{Consistent Multilabel Classification}},
url = {http://papers.nips.cc/paper/5883-consistent-multilabel-classification},
year = {2015}
}
@article{Schapire2000,
author = {Schapire, R E and Singer, Y},
file = {:home/jan/Documents/Mendeley Desktop/Schapire, Singer/Machine learning/Schapire, Singer - 2000 - BoosTexter A boosting- based system for text categorization.pdf:pdf},
journal = {Machine learning},
keywords = {boosting algorithms,multiclass classification problems,text and speech categorization},
pages = {135--168},
title = {{BoosTexter: A boosting- based system for text categorization}},
url = {http://link.springer.com/article/10.1023/A:1007649029923},
volume = {39},
year = {2000}
}
@article{Godbole2004,
abstract = {In this paper we present methods of enhancing existing discriminative classifiers for multi-labeled predictions. Discriminative methods like support vector machines perform very well for uni-labeled text classification tasks. Multi-labeled classification is a harder task subject to relatively less attention. In the multi-labeled setting, classes are often related to each other or part of a is-a hierarchy. We present a new technique for combining text features and features indicating relationships between classes, which can be used with any discriminative algorithm. We also present two enhancements to the margin of SVMs for building better models in the presence of overlapping classes. We present results of experiments on real world text benchmark datasets. Our new methods beat accuracy of existing methods with statistically significant improvements.},
archivePrefix = {arXiv},
arxivId = {10.1007/978-3-540-24775-3{\_}5},
author = {Godbole, Shantanu and Sarawagi, Sunita},
doi = {10.1007/978-3-540-24775-3_5},
eprint = {978-3-540-24775-3{\_}5},
file = {:home/jan/Documents/Mendeley Desktop/Godbole, Sarawagi/Lecture Notes in Computer Science/Godbole, Sarawagi - 2004 - Discriminative Methods for Multi-labeled Classification.pdf:pdf},
isbn = {978-3-540-22064-0},
issn = {03029743},
journal = {Lecture Notes in Computer Science},
pages = {22--30},
primaryClass = {10.1007},
title = {{Discriminative Methods for Multi-labeled Classification}},
url = {http://link.springer.com/10.1007/978-3-540-24775-3{\_}5},
volume = {3056},
year = {2004}
}
@article{Feng2006,
abstract = { Many computer vision applications, such as scene analysis and medical image interpretation, are ill-suited for traditional classification where each image can only be associated with a single class. This has stimulated recent work in multi-label learning where a given image can be tagged with multiple class labels. A serious problem with existing approaches is that they are unable to exploit correlations between class labels. This paper presents a novel framework for multi-label learning termed Correlated Label Propagation (CLP) that explicitly models interactions between labels in an efficient manner. As in standard label propagation, labels attached to training data points are propagated to test data points; however, unlike standard algorithms that treat each label independently, CLP simultaneously co-propagates multiple labels. Existing work eschews such an approach since naive algorithms for label co-propagation are intractable. We present an algorithm based on properties of submodular functions that efficiently finds an optimal solution. Our experiments demonstrate that CLP leads to significant gains in precision/recall against standard techniques on two real-world computer vision tasks involving several hundred labels.},
author = {Feng, Kang and Rong, Jin and Sukthankar, Rahul},
doi = {10.1109/CVPR.2006.90},
file = {:home/jan/Documents/Mendeley Desktop/Feng, Rong, Sukthankar/Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition/Feng, Rong, Sukthankar - 2006 - Correlated label propagation with application to multi-label learning.pdf:pdf},
isbn = {0769525970},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
number = {c},
pages = {1719--1726},
title = {{Correlated label propagation with application to multi-label learning}},
volume = {2},
year = {2006}
}
@article{Zhang,
abstract = {In multi-label learning, each training example is associated with a set of labels and the task is to predict the proper label set for the unseen example. Due to the tremendous (exponential) number of possible label sets, the task of learning from multi-label examples is rather challenging. Therefore, the key to successful multi-label learning is how to effectively exploit correlations between different labels to facilitate the learning process. In this paper, we propose to use a Bayesian network structure to efficiently encode the condi- tional dependencies of the labels as well as the feature set, with the feature set as the common parent of all labels. To make it practical, we give an approximate yet efficient procedure to find such a network structure. With the help of this network, multi-label learning is decomposed into a series of single-label classification problems, where a classifier is constructed for each label by incorporating its parental labels as additional features. Label sets of unseen examples are predicted recursively according to the label ordering given by the network. Extensive experiments on a broad range of data sets validate the effectiveness of our approach against other well-established methods.},
author = {Zhang, Min-Ling and Zhang, Kun},
doi = {10.1145/1835804.1835930},
file = {:home/jan/Documents/Mendeley Desktop/Zhang, Zhang/Kdd/Zhang, Zhang - 2010 - Multi-label learning by exploiting label dependency.pdf:pdf},
isbn = {9781450300551},
issn = {9781577355687},
journal = {Kdd},
keywords = {Learning—concept learn-ing,induction General Terms Algorithms},
pages = {999--1007},
title = {{Multi-label learning by exploiting label dependency}},
url = {http://dl.acm.org/citation.cfm?doid=1835804.1835930},
year = {2010}
}
@article{Zhang2005,
abstract = {— In multi-label learning, each instance in the training set is associated with a set of labels, and the task is to output a label set whose size is unknown a priori for each unseen instance. In this paper, a multi-label lazy learning approach named ML-kNN is presented, which is derived from the traditional k-nearest neighbor (kNN) algorithm. In detail, for each new instance, its k nearest neighbors are firstly identified. After that, according to the label sets of these neighboring instances, maximum a posteriori (MAP) principle is utilized to determine the label set for the new instance. Experiments on a real-world multi-label bioinformatic data show that ML-kNN is highly comparable to existing multi-label learning algorithms.},
author = {Zhang, ML},
file = {:home/jan/Documents/Mendeley Desktop/Zhang/Granular Computing, 2005 IEEE/Zhang - 2005 - A k-nearest neighbor based algorithm for multi-label classification.pdf:pdf},
isbn = {0780390172},
journal = {Granular Computing, 2005 IEEE},
pages = {718--721},
title = {{A k-nearest neighbor based algorithm for multi-label classification}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1547385},
year = {2005}
}
@article{Clare2001,
abstract = {The biological sciences are undergoing an explosion in the amount of available data. New data analysis methods are needed to deal with the data. We present work using KDD to analyse data from mutant phenotype growth experiments with the yeast S. cerevisiae to predict novel gene functions. The analysis of the data presented a number of challenges: multi-class labels, a large number of sparsely populated classes, the need to learn a set of accurate rules (not a complete classification), and a very large amount of missing values. We developed resampling strategies and modified the algorithm C4.5 to deal with these problems. Rules were learnt which are accurate and biologically meaningful. The rules predict function of 83 putative genes of currently unknown function at an estimated accuracy of {\textgreater} 80{\%}.},
author = {Clare, Amanda and King, Ross D.},
doi = {10.1007/3-540-44794-6_4},
file = {:home/jan/Documents/Mendeley Desktop/Clare, King/Pkdd/Clare, King - Unknown - Knowledge Discovery in Multi-Label Phenotype Data.pdf:pdf},
isbn = {3-540-42534-9},
issn = {16113349},
journal = {Pkdd},
pages = {42--53},
title = {{Knowledge Discovery in Multi-label Phenotype Data}},
url = {http://www.springerlink.com/index/10.1007/3-540-44794-6},
volume = {2168},
year = {2001}
}
@article{Reada,
abstract = {Multi-label classification has gained significant interest in recent years, paralleled by the increasing use of manual multi-labelling, often known as applying "tags" to documents. Well known examples include Flickr 1, YouTube2, CiteULike3 and Google Bookmarks4. This paper focuses on Problem Transformation (PT) as an approach to multi-label classification and details these methods as well as their respective advantages and disadvantages. A Pruned Problem Transformation method (PPT) is presented, along with several extensions, designed to overcome such disadvantages. This new method is empirically compared with existing methods, both in terms of accuracy and training time, and the results are encouraging.},
author = {Read, J},
file = {:home/jan/Documents/Mendeley Desktop/Read/New Zealand Computer Science Research Student Conference, NZCSRSC 2008 - Proceedings/Read - 2008 - A pruned problem transformation method for multi-label classification.pdf:pdf},
journal = {New Zealand Computer Science Research Student Conference, NZCSRSC 2008 - Proceedings},
keywords = {Computer science,Information retrieval systems,Multi-label classifications; Problem transformatio},
number = {April},
pages = {143--150},
title = {{A pruned problem transformation method for multi-label classification}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84880106608{\&}partnerID=40{\&}md5=ccdd19f7d0f111a2fdda8df7af0a8075},
year = {2008}
}
@article{Wang2016,
author = {Wang, Lei and Ren, Fuji and Miao, Duoqian},
doi = {10.1002/tee.22204},
file = {:home/jan/Documents/Mendeley Desktop/Wang, Ren, Miao/IEEJ Transactions on Electrical and Electronic Engineering/Wang, Ren, Miao - 2016 - Multi-label emotion recognition of weblog sentence based on Bayesian networks.pdf:pdf},
issn = {19314973},
journal = {IEEJ Transactions on Electrical and Electronic Engineering},
keywords = {Emotion Recognition,Multi-Label,emotion recognition,gibbs sampling,latent dirichlet allocation,multi-label classification,received 27 july 2013,revised 14 december 2014},
mendeley-tags = {Emotion Recognition,Multi-Label},
number = {2},
pages = {178--184},
title = {{Multi-label emotion recognition of weblog sentence based on Bayesian networks}},
url = {http://doi.wiley.com/10.1002/tee.22204},
volume = {11},
year = {2016}
}
@article{Luaces,
abstract = {The goal of multilabel (ML) classification is to induce models able to tag objects with the labels that better describe them. The main baseline for ML classi-fication is Binary Relevance (BR), which is commonly criticized in the literature because of its label inde-pendence assumption. Despite this fact, this paper dis-cusses some interesting properties of BR, mainly that it produces optimal models for several ML loss functions. Additionally, we present an analytical study about ML benchmarks datasets, pointing out some shortcomings. As a result, this paper proposes the use of synthetic datasets to better analyze the behavior of ML meth-ods in domains with different characteristics. To sup-port this claim, we perform some experiments using synthetic data proving the competitive performance of BR with respect to a more complex method in difficult problems with many labels, a conclusion which was not stated by previous studies.},
author = {Luaces, Oscar and D{\'{i}}ez, Jorge and Barranquero, Jos{\'{e}} and {Del Coz}, Juan Jos{\'{e}} and Bahamonde, Antonio},
file = {:home/jan/Documents/Mendeley Desktop/Luaces et al/Unknown/Luaces et al. - Unknown - Binary Relevance Efficacy for Multilabel Classification.pdf:pdf},
keywords = {Binary,Classification {\textperiodcentered},Label dependency,Multilabel,Rele-vance {\textperiodcentered},Synthetic datasets {\textperiodcentered}},
title = {{Binary Relevance Efficacy for Multilabel Classification}}
}
@article{Tsoumakas2012,
author = {Tsoumakas, Grigorios and Zhang, Min-Ling and Zhou, Zhi-Hua},
doi = {10.1007/s10994-012-5292-9},
issn = {0885-6125},
journal = {Machine Learning},
month = {jul},
number = {1-2},
pages = {1--4},
title = {{Introduction to the special issue on learning from multi-label data}},
url = {http://link.springer.com/10.1007/s10994-012-5292-9},
volume = {88},
year = {2012}
}
@article{Huang,
abstract = {Multi-label learning arises in many real-world tasks where an object is naturally associated with multiple concepts. It is well-accepted that, in order to achieve a good performance, the relationship among labels should be exploited. Most existing approaches require the label relationship as prior knowledge, or exploit by counting the label co-occurrence. In this paper, we propose the MAHR approach, which is able to automatically discover and exploit label relationship. Our basic idea is that, if two labels are related, the hypothesis generated for one label can be helpful for the other label. MAHR implements the idea as a boosting approach with a hypothesis reuse mechanism. In each boosting round, the base learner for a label is generated by not only learning on its own task but also reusing the hypotheses from other labels, and the amount of reuse across labels provides an estimate of the label relationship. Extensive experimental results validate that MAHR is able to achieve superior performance and discover reasonable label relationship. Moreover, we disclose that the label relationship is usually asymmetric.},
author = {Huang, Sheng-jun and Yu, Yang and Zhou, Zhi-hua},
doi = {10.1145/2339530.2339615},
file = {:home/jan/Documents/Mendeley Desktop/Huang, Yu, Zhou/Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '12/Huang, Yu, Zhou - 2012 - Multi-label hypothesis reuse.pdf:pdf},
isbn = {9781450314626},
journal = {Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '12},
keywords = {hypothesis reuse,label relationship,multi-label learning},
pages = {525},
title = {{Multi-label hypothesis reuse}},
url = {http://dl.acm.org/citation.cfm?id=2339530.2339615},
year = {2012}
}
@article{Yu2014,
abstract = {Nowadays, multi-label classification methods are of increasing interest in the areas such as text categorization, image annotation and protein function classification. Due to the correlation among the labels, traditional single-label classification methods are not directly applicable to the multi-label classification problem. This paper presents two novel multi-label classification algorithms based on the variable precision neighborhood rough sets, called multi-label classification using rough sets (MLRS) and MLRS using local correlation (MLRS-LC). The proposed algorithms consider two important factors that affect the accuracy of prediction, namely the correlation among the labels and the uncertainty that exists within the mapping between the feature space and the label space. MLRS provides a global view at the label correlation while MLRS-LC deals with the label correlation at the local level. Given a new instance, MLRS determines its location and then computes the probabilities of labels according to its location. The MLRS-LC first finds out its topic and then the probabilities of new instance belonging to each class is calculated in related topic. A series of experiments reported for seven multi-label datasets show that MLRS and MLRS-LC achieve promising performance when compared with some well-known multi-label learning algorithms. ?? 2013 Elsevier Ltd. All rights reserved.},
author = {Yu, Ying and Pedrycz, Witold and Miao, Duoqian},
doi = {10.1016/j.eswa.2013.10.030},
file = {:home/jan/Documents/Mendeley Desktop/Yu, Pedrycz, Miao/Expert Systems with Applications/Yu, Pedrycz, Miao - 2014 - Multi-label classification by exploiting label correlations.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Correlation,Multi-label classification,Rough sets,Uncertainty},
number = {6},
pages = {2989--3004},
title = {{Multi-label classification by exploiting label correlations}},
volume = {41},
year = {2014}
}
@article{Zhou2011,
abstract = {Multi-label learning has attracted much attention during the past few years. Many multilabel learning approaches have been developed, mostly working with surrogate loss functions since multi-label loss functions are usually difficult to optimize directly owing to non-convexity and discontinuity. Though these approaches are effective, to the best of our knowledge, there is no theoretical result on the convergence of risk of the learned functions to the Bayes risk. In this paper, focusing on two well-known multi-label loss functions, i.e., ranking loss and hamming loss, we prove a necessary and sufficient condition for the consistency of multi-label learning based on surrogate loss functions. Our results disclose that, surprisingly, none convex surrogate loss is consistent with the ranking loss. Inspired by the finding, we introduce the partial ranking loss, with which some surrogate functions are consistent. For hamming loss, we show that some recent multi-label learning approaches are inconsistent even for deterministic multi-label classification, and give a surrogate loss function which is consistent for the deterministic case. Finally, we discuss on the consistency of learning approaches which address multi-label learning by decomposing into a set of binary classification problems},
archivePrefix = {arXiv},
arxivId = {1204.1688},
author = {Gao, Wei and Zhou, Zhi-Hua},
doi = {10.1214/13-AOS1142},
eprint = {1204.1688},
file = {:home/jan/Documents/Mendeley Desktop/Gao, Zhou/Annals of Statistics/Gao, Zhou - 2011 - On the Consistency of Multi-Label Learning.pdf:pdf},
isbn = {9781605589077},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Consistency,hamming loss,multi-label learning,ranking loss,surrogate loss},
pmid = {14121148},
title = {{On the Consistency of Multi-Label Learning}},
year = {2011}
}
@article{Dembczynski,
abstract = {The aim of this paper is to elaborate on the important issue of label dependence in multi-label classification (MLC). Looking at the problem from a statistical perspective, we claim that two different types of label depen-dence should be distinguished, namely con-ditional and unconditional. We formally ex-plain the differences and connections between both types of dependence and illustrate them by means of simple examples. Moreover, we given an overview of state-of-the-art algo-rithms for MLC and categorize them accord-ing to the type of label dependence they seek to capture.},
author = {Dembczynski, Krzysztof and Waegeman, Willem and Cheng, Weiwei and H{\"{u}}llermeier, Eyke},
file = {:home/jan/Documents/Mendeley Desktop/Dembczynski et al/Unknown/Dembczynski - 2010 - On Label Dependence in Multi-Label Classification.pdf:pdf},
title = {{On Label Dependence in Multi-Label Classification}}
}
@article{Tsoumakasc,
abstract = {This paper proposes an ensemble method for multilabel clas-sification. The RAndom k-labELsets (RAKEL) algorithm constructs each member of the ensemble by considering a small random subset of labels and learning a single-label classifier for the prediction of each element in the powerset of this subset. In this way, the proposed algorithm aims to take into account label correlations using single-label classifiers that are applied on subtasks with manageable number of labels and adequate number of examples per label. Experimental results on common multil-abel domains involving protein, document and scene classification show that better performance can be achieved compared to popular multilabel classification approaches.},
author = {Tsoumakas, Grigorios and Vlahavas, Ioannis},
file = {:home/jan/Documents/Mendeley Desktop/Tsoumakas, Vlahavas/Unknown/Tsoumakas, Vlahavas - 2007 - Random k-labelsets An Ensemble Method for Multilabel Classification.pdf:pdf},
title = {{Random k-Labelsets: An Ensemble Method for Multilabel Classification}}
}
@article{Zhang2006,
author = {Zhang, Min-ling and Zhou, Zhi-hua and Member, Senior},
file = {:home/jan/Documents/Mendeley Desktop/Zhang, Zhou, Member/Unknown/Zhang, Zhou, Member - 2006 - Multilabel Neural Networks with Applications to Functional Genomics and Text Categorization.pdf:pdf},
number = {10},
pages = {1338--1351},
title = {{Multilabel Neural Networks with Applications to Functional Genomics and Text Categorization}},
volume = {18},
year = {2006}
}
@article{Gibaja2014,
abstract = {Multi-label learning is quite a recent supervised learning paradigm. Owing to its capabilities to improve performance in problems where a pattern may have more than one associated class, it has attracted the attention of researchers, producing an increasing number of publications. This study presents an up-to-date overview about multi-label learning with the aim of sorting and describing the main approaches developed till now. The formal definition of the paradigm, the analysis of its impact on the literature, its main applications, works developed, pitfalls and guidelines, and ongoing research are presented.},
author = {Gibaja, Eva and Ventura, Sebasti{\'{a}}n},
doi = {10.1002/widm.1139},
file = {:home/jan/Documents/Mendeley Desktop/Gibaja, Ventura/Wiley Interdisciplinary Reviews Data Mining and Knowledge Discovery/Gibaja, Ventura - 2014 - Multi-label learning A review of the state of the art and ongoing research.pdf:pdf},
issn = {19424795},
journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
keywords = {multi - label learning {\textperiodcentered} review},
number = {6},
pages = {411--444},
title = {{Multi-label learning: A review of the state of the art and ongoing research}},
volume = {4},
year = {2014}
}
@article{Cherman2011a,
abstract = {Traditional classification algorithms consider learning problems that contain only one label, i.e., each example is associated with one single nominal target variable characterizing its property. However, the number of practical applications involving data with multiple target variables has increased. To learn from this sort of data, multi-label classification algorithms should be used. The task of learning from multi-label data can be addressed by methods that transform the multi-label classification problem into several single-label classification problems. In this work, two well known methods based on this approach are used, as well as a third method we propose to overcome some deficiencies of one of them, in a case study using textual data related to medical findings, which were structured using the bag-of-words approach. The experimental study using these three methods shows an improvement on the results obtained by our proposed multi-label classification method.},
author = {Cherman, Everton Alvares and Monard, Maria Carolina and Metz, Jean},
file = {:home/jan/Documents/Mendeley Desktop/Cherman, Monard, Metz/CLEI ELECTRONIC JOURNAL/Cherman, Monard, Metz - 2011 - Multi-label Problem Transformation Methods a Case Study.pdf:pdf},
journal = {CLEI ELECTRONIC JOURNAL},
keywords = {binary relevance,label dependency,machine learning,multi-label classification},
number = {4},
title = {{Multi-label Problem Transformation Methods: a Case Study}},
volume = {14},
year = {2011}
}
@article{Huangb,
abstract = {Label embedding (LE) is an important family of multi-label classification algorithms that digest the label information jointly for better perfor-mance. Different real-world applications evalu-ate performance by different cost functions of in-terest. Current LE algorithms often aim to opti-mize one specific cost function, but they can suf-fer from bad performance with respect to other cost functions. In this paper, we resolve the performance issue by proposing a novel cost-sensitive LE algorithm that takes the cost func-tion of interest into account. The proposed algo-rithm, cost-sensitive label embedding with multi-dimensional scaling (CLEMS), approximates the cost information with the distances of the embed-ded vectors using the classic multidimensional scaling approach for manifold learning. CLEMS is able to deal with both symmetric and asym-metric cost functions, and effectively makes cost-sensitive decisions by nearest-neighbor decoding within the embedded vectors. Theoretical results justify that CLEMS achieves the cost-sensitivity and extensive experimental results demonstrate that CLEMS is significantly better than a wide spectrum of existing LE algorithms and state-of-the-art cost-sensitive algorithms across different cost functions.},
author = {Huang, Kuan-Hao and Lin, Hsuan-Tien},
file = {:home/jan/Documents/Mendeley Desktop/Huang, Lin/Unknown/Huang, Lin - Unknown - Cost-sensitive Label Embedding for Multi-label Classification.pdf:pdf},
title = {{Cost-sensitive Label Embedding for Multi-label Classification}}
}
@article{Gasse2015,
abstract = {The benefit of exploiting label dependence in multi-label classification is known to be closely dependent on the type of loss to be minimized. In this paper, we show that the subsets of labels that appear as irreducible factors in the factor-ization of the conditional distribution of the la-bel set given the input features play a pivotal role for multi-label classification in the context of 0/1 loss minimization, as they divide the learning task into simpler independent multi-class prob-lems. We establish theoretical results to charac-terize and identify these irreducible label factors for any given probability distribution satisfying the Composition property. The analysis lays the foundation for generic multi-label classification and optimal feature subset selection procedures under this subclass of distributions. Our conclu-sions are supported by carefully designed exper-iments on synthetic and benchmark data.},
author = {Gasse, Maxime and Aussem, Alex and Elghazel, Haytham},
file = {:home/jan/Documents/Mendeley Desktop/Gasse, Aussem, Elghazel/ICML/Gasse, Aussem, Elghazel - 2015 - On the Optimality of Multi-Label Classification under Subset Zero-One Loss for Distributions Satisfying.pdf:pdf},
isbn = {9781510810587},
journal = {ICML},
keywords = {Markov bounda,Multi-label learning,Zero-one loss},
title = {{On the Optimality of Multi-Label Classification under Subset Zero-One Loss for Distributions Satisfying the Composition Property}},
volume = {37},
year = {2015}
}
@article{Cherman2011,
author = {Cherman, Everton Alvares and Monard, Maria Carolina and Metz, Jean},
file = {:home/jan/Documents/Mendeley Desktop/Cherman, Monard, Metz/CLEI ELECTRONIC JOURNAL/Cherman, Monard, Metz - 2011 - Multi-label Problem Transformation Methods a Case Study.pdf:pdf},
issn = {0717-5000},
journal = {CLEI Electronic Journal},
number = {1},
pages = {4--4},
publisher = {Centro Latinoamericano de Estudios en Inform{\'{a}}tica},
title = {{Multi-label Problem Transformation Methods: a Case Study}},
volume = {14},
year = {2011}
}
@article{Lo2013,
abstract = {—Label powerset (LP) method is one category of multi-label learning algorithm. This paper presents a basis expansions model for multi-label classification, where a basis function is a LP classifier trained on a random k-labelset. The expansion coefficients are learned to minimize the global error between the prediction and the ground truth. We derive an analytic solution to learn the coefficients efficiently. We further extend this model to handle the cost-sensitive multi-label classification problem, and apply it in social tagging to handle the issue of the noisy training set by treating the tag counts as the misclassification costs. We have conducted experiments on several benchmark datasets and compared our method with other state-of-the-art multi-label learning methods. Experimental results on both multi-label classification and cost-sensitive social tagging demonstrate that our method has better performance than other methods.},
author = {Lo, Hung-Yi and Lin, Shou-De and Wang, Hsin-Min},
doi = {10.1109/TKDE.2013.112},
file = {:home/jan/Documents/Mendeley Desktop/Lo, Lin, Wang/IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING/Lo, Lin, Wang - 2013 - Generalized k-Labelsets Ensemble for Multi-Label and Cost-Sensitive Classification.pdf:pdf},
journal = {IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING},
keywords = {Index Terms—Multi-label classification,cost-sensitive learning,ensemble method,hypergraph,labelset,social tag,tag count},
number = {X},
title = {{Generalized k-Labelsets Ensemble for Multi-Label and Cost-Sensitive Classification}},
year = {2013}
}
@article{Zhang2014,
abstract = {Multi-label learning studies the problem where each example is represented by a single instance while associated with a set of labels simultaneously. During the past decade, significant amount of progresses have been made toward this emerging machine learning paradigm. This paper aims to provide a timely review on this area with emphasis on state-of-the-art multi-label learning algorithms. Firstly, fundamentals on multi-label learning including formal definition and evaluation metrics are given. Secondly and primarily, eight representative multi-label learning algorithms are scrutinized under common notations with relevant analyses and discussions. Thirdly, several related learning settings are briefly summarized. As a conclusion, online resources and open research problems on multi-label learning are outlined for reference purposes.},
author = {Zhang, Min Ling and Zhou, Zhi Hua},
doi = {10.1109/TKDE.2013.39},
file = {:home/jan/Documents/Mendeley Desktop/Zhang, Zhou/IEEE Transactions on Knowledge and Data Engineering/Zhang, Zhou - 2014 - A review on multi-label learning algorithms.pdf:pdf},
isbn = {1041-4347},
issn = {10414347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Multi-label learning,algorithm adaptation,label correlations-problem transformation},
number = {8},
pages = {1819--1837},
title = {{A review on multi-label learning algorithms}},
volume = {26},
year = {2014}
}
@article{Liu2015,
abstract = {A multi-label classification based approach for sentiment analysis is proposed in this paper. To the best of our knowledge, this work is the first to propose to use multi-label classification for sentiment classification of microblogs. The proposed prototype has three main components, text segmentation, feature extraction, and multi-label classification. Raw segmented words and sentiment features based on the three different sentiment dictionaries, Dalian University of Technology Sentiment Dictionary, National Taiwan University Sentiment Dictionary and HowNet Dictionary, are the features and the bag of words is the feature representation. A detailed empirical study of different multi-label classification methods on sentiment classification is conducted to compare their classification performances. Specifically, total 11 state of the art multi-label classification methods are compared on two microblog datasets and 8 evaluation metrics are used. The effects of the three sentiment dictionaries for multi-label classification are empirically studied and compared, which, to the best of our knowledge, have not been performed. The performed empirical comparisons show that Dalian University of Technology Sentiment Dictionary has the best performance among the three different sentiment dictionaries.},
author = {Liu, Shuhua Monica and Chen, Jiun-Hung},
doi = {10.1016/j.eswa.2014.08.036},
file = {:home/jan/Documents/Mendeley Desktop/Liu, Chen/Expert Systems with Applications/Liu, Chen - 2015 - A multi-label classification based approach for sentiment classification.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Microblogs,Multi-Label,Multi-label classification,Sentiment Classification,Sentiment analysis},
mendeley-tags = {Multi-Label,Sentiment Classification},
number = {3},
pages = {1083--1093},
publisher = {Elsevier Ltd},
title = {{A multi-label classification based approach for sentiment classification}},
url = {http://www.sciencedirect.com/science/article/pii/S0957417414005181},
volume = {42},
year = {2015}
}
@article{Tsoumakase,
abstract = {{\textcopyright} Springer International Publishing Switzerland 2014. The WISE 2014 challenge was concerned with the task of multi-label classification of articles coming from Greek print media. Raw data comes from the scanning of print media, article segmentation, and optical character segmentation, and therefore is quite noisy. Each article is examined by a human annotator and categorized to one or more of the topics being monitored. Topics range from specific persons, products, and companies that can be easily categorized based on keywords, to more general semantic concepts, such as environment or economy. Building multi-label classifiers for the automated annotation of articles into topics can support the work of human annotators by suggesting a list of all topics by order of relevance, or even automate the annotation process for media and/or categories that are easier to predict. This saves valuable time and allows a media monitoring company to expand the portfolio of media being monitored. This paper summarizes the approaches of the top 4 among the 121 teams that participated in the competition.},
author = {Tsoumakas, Grigorios and Papadopoulos, Apostolos and Qian, Weining and Vologiannidis, Stavros and D'yakonov, A. and Puurula, Antti and Read, Jesse and {\v{S}}vec, J. and Semenov, Stanislav},
doi = {10.1007/978-3-319-11746-1_40},
file = {:home/jan/Documents/Mendeley Desktop/Tsoumakas et al/Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)/Tsoumakas et al. - 2014 - WISE 2014 challenge Multi-label classification of print media articles to topics.pdf:pdf},
isbn = {978-3-319-11745-4},
issn = {16113349 03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {541--548},
title = {{WISE 2014 challenge: Multi-label classification of print media articles to topics}},
volume = {8787},
year = {2014}
}
@article{Sorower,
abstract = {Multi-label Learning is a form of supervised learning where the classification al-gorithm is required to learn from a set of instances, each instance can belong to multiple classes and so after be able to predict a set of class labels for a new in-stance. This is a generalized version of most popular multi-class problems where each instances is restricted to have only one class label. There exists a wide range of applications for multi-labelled predictions, such as text categorization, seman-tic image labeling, gene functionality classification etc. and the scope and interest is increasing with modern applications. This survey paper introduces the task of multi-label prediction (classification), presents the sparse literature in this area in an organized manner, discusses different evaluation metrics and performs a com-parative analysis of the existing algorithms. This paper also relates multi-label problems with similar but different problems that are often reduced to multi-label problems to have access to wide range of multi-label algorithms.},
author = {Sorower, Mohammad S},
file = {:home/jan/Documents/Mendeley Desktop/Sorower/Oregon State University, Corvallis/Sorower - Unknown - A Literature Survey on Algorithms for Multi-label Learning.pdf:pdf},
title = {{A Literature Survey on Algorithms for Multi-label Learning}}
}
@article{Tsoumakas2007b,
abstract = {This paper proposes an ensemble method for multilabel clas- sification. TheRAndomk-labELsets (RAKEL) algorithm constructs each member of the ensemble by considering a small randomsubset of labels and learning a single-label classifier for the prediction of each element in the powerset of this subset. In this way, the proposed algorithm aims to take into account label correlations using single-label classifiers that are applied on subtasks with manageable number of labels and adequate number of examples per label. Experimental results on common multilabel domains involving protein, document and scene classification showthat better per- formance can be achieved compared to popular multilabel classification approaches.},
author = {Tsoumakas, Grigorios and Vlahavas, Ioannis},
doi = {10.1007/978-3-540-74958-5_38},
file = {:home/jan/Documents/Mendeley Desktop/Tsoumakas, Vlahavas/Unknown/Tsoumakas, Vlahavas - 2007 - Random k-labelsets An Ensemble Method for Multilabel Classification.pdf:pdf},
isbn = {6463501476},
issn = {01681605},
journal = {European Conference on Machine Learning},
pages = {406--417},
pmid = {11518430},
title = {{Random k-labelsets: An Ensemble Method for Multilabel Classification}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-74958-5{\_}38},
year = {2007}
}
@article{Boutell2004,
abstract = {In classic pattern recognition problems, classes are mutually exclusive by definition. Classification errors occur when the classes overlap in the feature space. We examine a different situation, occurring when the classes are, by definition, not mutually exclusive. Such problems arise in semantic scene and document classification and in medical diagnosis. We present a framework to handle such problems and apply it to the problem of semantic scene classification, where a natural scene may contain multiple objects such that the scene can be described by multiple class labels (e.g., a field scene with a mountain in the background). Such a problem poses challenges to the classic pattern recognition paradigm and demands a different treatment. We discuss approaches for training and testing in this scenario and introduce new metrics for evaluating individual examples, class recall and precision, and overall accuracy. Experiments show that our methods are suitable for scene classification; furthermore, our work appears to generalize to other classification problems of the same nature. {\textcopyright} 2004 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.},
author = {Boutell, Matthew R. and Luo, Jiebo and Shen, Xipeng and Brown, Christopher M.},
doi = {10.1016/j.patcog.2004.03.009},
file = {:home/jan/Documents/Mendeley Desktop/Boutell et al/Pattern Recognition/Boutell et al. - 2004 - Learning multi-label scene classi{\"{y}}cation.pdf:pdf},
isbn = {0031-3203},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Cross-training,Image organization,Image understanding,Jaccard similarity,Multi-label classification,Multi-label evaluation,Multi-label training,Semantic scene classification},
number = {9},
pages = {1757--1771},
title = {{Learning multi-label scene classification}},
volume = {37},
year = {2004}
}
@article{Alazaidah2015,
abstract = {—Multi label classification is concerned with learning from a set of instances that are associated with a set of labels, that is, an instance could be associated with multiple labels at the same time. This task occurs frequently in application areas like text categorization, multimedia classification, bioinformatics, protein function classification and semantic scene classification. Current multi-label classification methods could be divided into two categories. The first is called problem transformation methods, which transform multi-label classification problem into single label classification problem, and then apply any single label classifier to solve the problem. The second category is called algorithm adaptation methods, which adapt an existing single label classification algorithm to handle multi-label data. In this paper, we propose a multi-label classification approach based on correlations among labels that use both problem transformation methods and algorithm adaptation methods. The approach begins with transforming multi-label dataset into a single label dataset using least frequent label criteria, and then applies the PART algorithm on the transformed dataset. The output of the approach is multi-labels rules. The approach also tries to get benefit from positive correlations among labels using predictive Apriori algorithm. The proposed approach has been evaluated using two multi-label datasets named (Emotions and Yeast) and three evaluation measures (Accuracy, Hamming Loss, and Harmonic Mean). The experiments showed that the proposed approach has a fair accuracy in comparison to other related methods.},
author = {Alazaidah, Raed and Thabtah, Fadi and Al-Radaideh, Qasem},
file = {:home/jan/Documents/Mendeley Desktop/Alazaidah, Thabtah, Al-Radaideh/IJACSA) International Journal of Advanced Computer Science and Applications/Alazaidah, Thabtah, Al-Radaideh - 2015 - A Multi-Label Classification Approach Based on Correlations Among Labels.pdf:pdf},
journal = {IJACSA) International Journal of Advanced Computer Science and Applications},
keywords = {Data mining,Multi-label Classification,—Classification},
number = {2},
title = {{A Multi-Label Classification Approach Based on Correlations Among Labels}},
url = {www.ijacsa.thesai.org},
volume = {6},
year = {2015}
}
@article{Al-Salemi2015,
author = {Al-Salemi, B and Aziz, MJ Ab and Noah, SA},
doi = {10.1177/0165551515590079},
file = {:home/jan/Documents/Mendeley Desktop/Al-Salemi, Aziz, Noah/Journal of Information Science/Al-Salemi, Aziz, Noah - 2015 - Boosting algorithms with topic modeling for multi-label text categorization A comparative empirical study.pdf:pdf},
issn = {17416485},
journal = {Journal of Information Science},
keywords = {Multi-Label,Text Categorisation,adaboost,boosting,mh,multi-label classification,text categorization,text representation,topic modeling},
mendeley-tags = {Multi-Label,Text Categorisation},
title = {{Boosting algorithms with topic modeling for multi-label text categorization: A comparative empirical study}},
url = {http://jis.sagepub.com/content/early/2015/06/30/0165551515590079.abstract},
year = {2015}
}
@article{DeComite2003,
abstract = {Multi-label decision procedures are the target of the supervised learning algorithm we propose in this paper. Multi-label decision procedures map examples to a finite set of labels. Our learning algorithm extends Schapire and Singer's Adaboost.MH and produces sets of rules that can be viewed as trees like Alternating Decision Trees (invented by Freund and Mason). Experiments show that we take advantage of both performance and readability using boosting techniques as well as tree representations of large set of rules. Moreover, a key feature of our algorithm is the ability to handle heterogenous input data: discrete and continuous values and text data.},
author = {Comite, Francesco De and Gilleron, Remi and Tommasi, Marc},
file = {:home/jan/Documents/Mendeley Desktop/Comite, Gilleron, Tommasi/Third International Conference Machine Learning and Data Mining in Pattern Recognition/De Comite, Gilleron, Tommasi - 2003 - Learning Multi-label Alternating Decision Trees from Texts and Data.pdf:pdf},
isbn = {978-3-540-40504-7},
issn = {03029743},
journal = {Third International Conference Machine Learning and Data Mining in Pattern Recognition},
keywords = {alternating decision trees,boosting,multi-label,text mining},
pages = {35--49},
publisher = {Springer},
title = {{Learning multi-label alternating decision trees from texts and data}},
url = {https://hal.inria.fr/inria-00536733},
year = {2003}
}
@article{Xu2016,
abstract = {Tail labels in the multi-label learning problem undermine the low-rank assumption. Nevertheless, this problem has rarely been investigated. In addition to using the low-rank structure to depict label correlations, this paper explores and exploits an additional sparse component to handle tail labels behaving as outliers, in order to make the classical low-rank principle in multi-label learning valid. The divideand-conquer optimization technique is employed to increase the scalability of the proposed algorithm while theoretically guaranteeing its performance. A theoretical analysis of the generalizability of the proposed algorithm suggests that it can be improved by the low-rank and sparse decomposition given tail labels. Experimental results on real-world data demonstrate the significance of investigating tail labels and the effectiveness of the proposed algorithm.},
archivePrefix = {arXiv},
arxivId = {arXiv:1602.05561v1},
author = {Xu, Chang and Tao, Dacheng and Xu, Chao},
doi = {10.475/123},
eprint = {arXiv:1602.05561v1},
file = {:home/jan/Documents/Mendeley Desktop/Xu, Tao, Xu/KDD/Xu, Tao, Xu - 2016 - Robust Extreme Multi-Label Learning.pdf:pdf},
isbn = {9781450335423},
issn = {0146-4833},
journal = {KDD},
keywords = {low-rank algorithm,multi-label learning},
pages = {421--434},
title = {{Robust Extreme Multi-Label Learning}},
year = {2016}
}
@article{Tawiah2013,
abstract = {Multi-label classifications exist in many real world applications. This paper empirically studies the performance of a variety of multi-label classification algorithms. Some of them are developed based on problem transformation. Some of them are developed based on adaption. Our experimental results show that the adaptive Multi-Label K-Nearest Neighbor performs the best, followed by Random k-Label Set, followed by Classifier Chain and Binary Relevance. Adaboost.MH performs the worst, followed by Pruned Problem Transformation. Our experimental results also provide us the confidence of the correlations among multilabels. These insights shed light for future research directions on multi-label classifications.},
author = {Tawiah, Ca and Sheng, Vs},
isbn = {9781424427659},
journal = {Proceedings of the 27th AAAI Conference on {\ldots}},
keywords = {Student Abstract and Poster Program},
pages = {1645--1646},
title = {{Empirical Comparison of Multi-Label Classification Algorithms}},
url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI13/paper/download/6170/6833},
year = {2013}
}
@article{Sucar2013,
abstract = {a b s t r a c t In multi-label classification the goal is to assign an instance to a set of different classes. This task is normally addressed either by defining a compound class variable with all the possible combinations of labels (label power-set methods) or by building independent classifiers for each class (binary relevance methods). The first approach suffers from high computationally complexity, while the second approach ignores possible dependencies among classes. Chain classifiers have been recently proposed to address these problems, where each classifier in the chain learns and predicts the label of one class given the attributes and all the predictions of the previous classifiers in the chain. In this paper we introduce a method for chaining Bayesian classifiers that combines the strengths of classifier chains and Bayesian networks for multi-label classification. A Bayesian network is induced from data to: (i) represent the probabilistic dependency relationships between classes, (ii) constrain the number of class variables used in the chain classifier by considering conditional independence conditions, and (iii) reduce the number of possible chain orders. The effects in the Bayesian chain classifier performance of considering different chain orders, training strategies, number of class variables added in the base classifiers, and different base classifiers, are experimentally assessed. In particular, it is shown that a random chain order considering the constraints imposed by a Bayesian network with a simple tree-based structure can have very competitive results in terms of predictive performance and time complexity against related state-of-the-art approaches.},
author = {Sucar, L Enrique and Bielza, Concha and Morales, Eduardo F and Hernandez-Leal, Pablo and Zaragoza, Julio H and Larra{\~{n}}aga, Pedro},
doi = {10.1016/j.patrec.2013.11.007},
file = {:home/jan/Documents/Mendeley Desktop/Sucar et al/Unknown/Sucar et al. - 2013 - Author's personal copy Multi-label classification with Bayesian network-based chain classifiers.pdf:pdf},
keywords = {Bayesian networks,Chain classifier,Multi-label classification},
title = {{Author's personal copy Multi-label classification with Bayesian network-based chain classifiers}},
year = {2013}
}
@article{Wu2014,
author = {Wu, Yunong and Kita, Kenji and Matsumoto, Kazuyuki},
doi = {10.1002/tee.22020},
file = {:home/jan/Documents/Mendeley Desktop/Wu, Kita, Matsumoto/IEEJ Transactions on Electrical and Electronic Engineering/Wu, Kita, Matsumoto - 2014 - Three predictions are better than one Sentence multi-emotion analysis from different perspectives.pdf:pdf},
issn = {19314973},
journal = {IEEJ Transactions on Electrical and Electronic Engineering},
keywords = {Emotion Analysis,Multi-Label,conditional random fields,crf,integrated prediction,l-lda,labeled latent dirichlet allocation,lgr,logistic,multi-emotion,received 28 june 2013,regression,revised 4 january 2014,sentence emotion analysis},
mendeley-tags = {Emotion Analysis,Multi-Label},
number = {6},
pages = {642--649},
title = {{Three predictions are better than one: Sentence multi-emotion analysis from different perspectives}},
url = {http://doi.wiley.com/10.1002/tee.22020},
volume = {9},
year = {2014}
}
@article{Tsoumakasd,
abstract = {This paper contributes a novel algorithm for effective and computationally efficient multilabel classification in domains with large label sets L. The HOMER algorithm constructs a Hierarchy Of Multilabel classifiERs, each one dealing with a much smaller set of labels compared to L and a more balanced example distribution. This leads to improved predictive performance along with linear training and logarithmic testing complexities with respect to |L|. Label distribution from parent to children nodes is achieved via a new balanced clustering algorithm, called balanced k means.},
author = {Tsoumakas, Grigorios and Katakis, Ioannis and Vlahavas, Ioannis},
file = {:home/jan/Documents/Mendeley Desktop/Tsoumakas, Katakis, Vlahavas/Proc. ECMLPKDD 2008 Workshop on Mining Multidimensional Data (MMD'08)/Tsoumakas, Katakis, Vlahavas - 2008 - Effective and efficient multilabel classification in domains with large number of labels.pdf:pdf},
journal = {Proc. ECML/PKDD 2008 Workshop on Mining Multidimensional Data (MMD'08)},
pages = {30--44},
title = {{Effective and efficient multilabel classification in domains with large number of labels}},
url = {http://lpis.csd.auth.gr/publications/tsoumakas-mmd08.pdf},
year = {2008}
}
@article{Huanga,
abstract = {It is well known that exploiting label correlations is important for multi-label learning. Existing approaches typically exploit label correlations globally, by assuming that the label correlations are shared by all the instances. In real-world tasks, however, different instances may share different label correlations, and few correlations are globally applicable. In this paper, we propose the ML-LOC approach which allows label correlations to be exploited locally. To encode the local influence of label correlations, we derive a LOC code to enhance the feature representation of each instance. The global discrimination fitting and local correlation sensitivity are incorporated into a unified framework, and an alternating solution is developed for the optimization. Experimental results on a number of image, text and gene data sets validate the effectiveness of our approach.},
author = {Huang, Sheng-Jun and Zhou, Zhi-Hua},
doi = {10.1145/1835804.1835930},
file = {:home/jan/Documents/Mendeley Desktop/Huang, Zhou/AAAI Conference on Artificial Intelligence/Huang, Zhou - 2012 - Multi-Label Learning by Exploiting Label Correlations Locally.pdf:pdf},
isbn = {9781577355687},
issn = {9781577355687},
journal = {AAAI Conference on Artificial Intelligence},
keywords = {Machine Learning (Main Track)},
pages = {949--955},
title = {{Multi-Label Learning by Exploiting Label Correlations Locally}},
year = {2012}
}
@article{Katakis2008,
abstract = {The increased popularity of tagging during the last few years can be mainly attributed to its embracing by most of the recently thriving user-centric content publishing and management Web 2.0 applications. However, tagging systems have some limitations that have led researchers to develop methods that assist users in the tagging process, by automat- ically suggesting an appropriate set of tags. We have tried to model the automated tag suggestion problem as a multilabel text classification task in order to participate in the ECML/PKDD 2008 Discovery Challenge.},
author = {Katakis, Ioannis and Tsoumakas, Grigorios and Vlahavas, Ioannis},
file = {:home/jan/Documents/Mendeley Desktop/Katakis, Tsoumakas, Vlahavas/Proceedings of the ECMLPKDD 2008 Discovery Challenge (2008)/Katakis, Tsoumakas, Vlahavas - 2008 - Multilabel Text Classification for Automated Tag Suggestion.pdf:pdf},
issn = {10675027},
journal = {Proceedings of the ECMLPKDD 2008 Discovery Challenge (2008)},
number = {3},
pages = {1--9},
title = {{Multilabel Text Classification for Automated Tag Suggestion}},
volume = {9},
year = {2008}
}
@article{Rokach,
abstract = {Ensemble methods have been shown to be an effective tool for solving multi-label classification tasks. In the RAndom k-labELsets (RAKEL) algorithm, each member of the ensemble is associated with a small randomly-selected subset of k labels. Then, a single label classifier is trained according to each combi-nation of elements in the subset. In this paper we adopt a similar approach, however, instead of randomly choosing subsets, we select the minimum required subsets of k labels that cover all labels and meet additional constraints such as coverage of inter-label correlations. Construction of the cover is achieved by formulating the subset selection as a minimum set covering problem (SCP) and solving it by using approximation algo-rithms. Every cover needs only to be prepared once by offline algorithms. Once prepared, a cover may be applied to the classification of any given multi-label dataset whose properties conform with those of the cov-er. The contribution of this paper is two-fold. First, we introduce SCP as a general framework for construct-ing label covers while allowing the user to incorporate cover construction constraints. We demonstrate the effectiveness of this framework by proposing two construction constraints whose enforcement produces co-vers that improve the prediction performance of random selection. Second, we provide theoretical bounds that quantify the probabilities of random selection to produce covers that meet the proposed construction cri-teria. The experimental results indicate that the proposed methods improve multi-label classification accura-cy and stability compared with the RAKEL algorithm and to other state-of-the-art algorithms.},
author = {Systems, Expert and Aviv-yafo, Tel},
doi = {10.1016/j.eswa.2014.06.015},
file = {:home/jan/Documents/Mendeley Desktop/Systems, Aviv-yafo/Unknown/Rokach, Schclar, Itach - Unknown - Ensemble Methods for Multi-label Classification.pdf:pdf},
number = {July 2013},
title = {{Ensemble Methods for Multi-label Classification Ensemble Methods for Multi-label Classification}},
year = {2014}
}
@article{Dembcz2012,
abstract = {Most of the multi-label classification (MLC) methods proposed in recent years intended to exploit, in one way or the other, dependencies between the class labels. Compar-ing to simple binary relevance learning as a baseline, any gain in performance is normally explained by the fact that this method is ignoring such dependencies. Without questioning the correctness of such studies, one has to admit that a blanket explanation of that kind is hiding many subtle details, and indeed, the underlying mechanisms and true reasons for the improvements reported in experimental studies are rarely laid bare. Rather than propos-ing yet another MLC algorithm, the aim of this paper is to elaborate more closely on the idea of exploiting label dependence, thereby contributing to a better understanding of MLC. Adopting a statistical perspective, we claim that two types of label dependence should be distinguished, namely conditional and marginal dependence. Subsequently, we present three scenarios in which the exploitation of one of these types of dependence may boost the pre-dictive performance of a classifier. In this regard, a close connection with loss minimization is established, showing that the benefit of exploiting label dependence does also depend on the type of loss to be minimized. Concrete theoretical results are presented for two repre-Editors: Mach Learn (2012) 88:5–45 sentative loss functions, namely the Hamming loss and the subset 0/1 loss. In addition, we give an overview of state-of-the-art decomposition algorithms for MLC and we try to re-veal the reasons for their effectiveness. Our conclusions are supported by carefully designed experiments on synthetic and benchmark data.},
author = {Dembcz, Krzysztof and Waegeman, Willem and Cheng, Weiwei and H{\"{u}}llermeier, Eyke and Tsoumakas, Grigorios and Zhang, Min-Ling and Zhou, Zhi-Hua and Dembczyski, K and Waegeman, W and Cheng, W and H{\"{u}}llermeier, E},
doi = {10.1007/s10994-012-5285-8},
file = {:home/jan/Documents/Mendeley Desktop/Dembcz et al/Mach Learn/Dembcz et al. - 2012 - On label dependence and loss minimization in multi-label classification.pdf:pdf},
journal = {Mach Learn},
keywords = {Label dependence {\textperiodcentered},Loss functions,Multi-label classification {\textperiodcentered}},
pages = {5--45},
title = {{On label dependence and loss minimization in multi-label classification}},
volume = {88},
year = {2012}
}
@article{Tsoumakasb,
abstract = {Binary relevance (BR) learns a single binary model for each different label of multi-label data. It has linear complexity with respect to the number of labels, but does not take into account label correlations and may fail to accurately predict label combinations and rank labels according to relevance with a new instance. Stacking the models of BR in order to learn a model that associates their output to the true value of each label is a way to alleviate this problem. In this paper we propose the pruning of the models participating in the stacking process, by explicitly measuring the degree of label correlation using the phi coefficient. Exploratory analysis of phi shows that the correlations detected are meaningful and useful. Empirical evaluation of the pruning approach shows that it leads to substantial reduction of the computational cost of stacking and occasional improvements in predictive performance.},
author = {Tsoumakas, Grigorios and Dimou, Anastasios and Spyromitros, Eleftherios and Mezaris, Vasileios and Kompatsiaris, Ioannis and Vlahavas, Ioannis},
file = {:home/jan/Documents/Mendeley Desktop/Tsoumakas et al/Proceedings of the Workshop on Learning from Multi-Label Data (MLD'09)/Tsoumakas et al. - 2009 - Correlation-based pruning of stacked binary relevance models for multi-label learning.pdf:pdf},
issn = {1475-925X},
journal = {Proceedings of the Workshop on Learning from Multi-Label Data (MLD'09)},
pages = {101--116},
title = {{Correlation-based pruning of stacked binary relevance models for multi-label learning}},
url = {http://www.ecmlpkdd2009.net/wp-content/uploads/2008/09/learning-from-multi-label-data.pdf{\#}page=102},
year = {2009}
}
@article{Alazaidah2016,
abstract = {—Multi label classification has become a very important paradigm in the last few years because of the increasing domains that it can be applied to. Many researchers have developed many algorithms to solve the problem of multi label classification. Nerveless, there are still some stuck problems that need to be investigated in depth. The aim of this paper is to provide researchers with a brief introduction to the problem of multi label classification, and introduce some of the most trending challenges.},
author = {Alazaidah, Raed and Ahmad, Farzana Kabir},
file = {:home/jan/Documents/Mendeley Desktop/Alazaidah, Ahmad/IJACSA) International Journal of Advanced Computer Science and Applications/Alazaidah, Ahmad - 2016 - Trending Challenges in Multi Label Classification.pdf:pdf},
journal = {IJACSA) International Journal of Advanced Computer Science and Applications},
keywords = {Correlations among labels,Multi Label Classification,—Challenges},
number = {10},
title = {{Trending Challenges in Multi Label Classification}},
url = {www.ijacsa.thesai.org},
volume = {7},
year = {2016}
}
@article{Madjarov2012a,
abstract = {This article appeared in a journal published by Elsevier. The attached copy is furnished to the author for internal non-commercial research and education use, including for instruction at the authors institution and sharing with colleagues. Other uses, including reproduction and distribution, or selling or licensing copies, or posting to personal, institutional or third party websites are prohibited. In most cases authors are permitted to post their version of the article (e.g. in Word or Tex form) to their personal website or institutional repository. Authors requiring further information regarding Elsevier's archiving and manuscript policies are encouraged to visit: a b s t r a c t Multi-label learning has received significant attention in the research community over the past few years: this has resulted in the development of a variety of multi-label learning methods. In this paper, we present an extensive experimental comparison of 12 multi-label learning methods using 16 evaluation measures over 11 benchmark datasets. We selected the competing methods based on their previous usage by the community, the representation of different groups of methods and the variety of basic underlying machine learning methods. Similarly, we selected the evaluation measures to be able to assess the behavior of the methods from a variety of view-points. In order to make conclusions independent from the application domain, we use 11 datasets from different domains. Furthermore, we compare the methods by their efficiency in terms of time needed to learn a classifier and time needed to produce a prediction for an unseen example. We analyze the results from the experiments using Friedman and Nemenyi tests for assessing the statistical significance of differences in performance. The results of the analysis show that for multi-label classification the best performing methods overall are random forests of predictive clustering trees (RF-PCT) and hierarchy of multi-label classifiers (HOMER), followed by binary relevance (BR) and classifier chains (CC). Furthermore, RF-PCT exhibited the best performance according to all measures for multi-label ranking. The recommendation from this study is that when new methods for multi-label learning are proposed, they should be compared to RF-PCT and HOMER using multiple evaluation measures.},
author = {Madjarov, Gjorgji and Kocev, Dragi and Gjorgjevikj, Dejan and D{\v{z}}eroski, Sa{\v{s}}o},
file = {:home/jan/Documents/Mendeley Desktop/Madjarov et al/Unknown/Madjarov et al. - 2012 - Author's personal copy An extensive experimental comparison of methods for multi-label learning.pdf:pdf},
keywords = {Comparison of multi-label learning methods,Multi-label classification,Multi-label ranking},
title = {{Author's personal copy An extensive experimental comparison of methods for multi-label learning}},
url = {http://www.elsevier.com/copyright},
year = {2012}
}
@article{Pachet2009,
abstract = {—This paper addresses the problem of automatically ex-tracting perceptive information from acoustic signals, in a super-vised classification context. Global labels, i.e., atomic information describing a music title in its entirety, such as its genre, mood, main instruments, or type of vocals, are entered by humans. Classifiers are trained to map audio features to these labels. However, the per-formances of these classifiers on individual labels are rarely satis-factory. In the case we have to predict several labels simultane-ously, we introduce a correction scheme to improve these perfor-mances. In this scheme—an instance of the classifier fusion par-adigm—an extra layer of classifiers is built to exploit redundan-cies between labels and correct some of the errors coming from the individual acoustic classifiers. We describe a series of experi-ments aiming at validating this approach on a large-scale database of music and metadata (about 30 000 titles and 600 labels per title). The experiments show that the approach brings statistically signif-icant improvements.},
author = {Pachet, Fran{\c{c}}ois and Roy, Pierre},
doi = {10.1109/TASL.2008.2008734},
file = {:home/jan/Documents/Mendeley Desktop/Pachet, Roy/IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING/Pachet, Roy - 2009 - Improving Multilabel Analysis of Music Titles A Large-Scale Validation of the Correction Approach.pdf:pdf},
journal = {IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING},
keywords = {Index Terms—Feature extraction,learning systems,music,pat-tern classification},
number = {2},
title = {{Improving Multilabel Analysis of Music Titles: A Large-Scale Validation of the Correction Approach}},
volume = {17},
year = {2009}
}
@article{Tsoumakas,
abstract = {Nowadays, multi-label classification methods are increasingly required by modern applications, such as protein function classification, music categorization and semantic scene classification. This paper introduces the task of multi-label classification, organizes the sparse related literature into a structured presentation and performs comparative experimental results of certain multi-label classification methods. It also contributes the definition of concepts for the quantification of the multi-label nature of a data set.},
author = {Tsoumakas, Grigorios and Katakis, Ioannis},
doi = {10.4018/jdwm.2007070101},
file = {:home/jan/Documents/Mendeley Desktop/Tsoumakas, Katakis/Unknown/Tsoumakas, Katakis - 2007 - Multi-Label Classification An Overview.pdf:pdf},
isbn = {9781424410651},
issn = {1548-3924},
title = {{Multi-Label Classification : An Overview}}
}
@article{Madjarov2011,
abstract = {See, stats, and : https : / / www . researchgate . net / publication / 232630058 Two - label Article DOI : 10 . 1016 / j . patcog . 2011 . 08 . 011 : DBLP CITATIONS 13 READS 99 3 , including : Gjorgji Ss . Cyril 31 SEE Dejan Ss . Cyril 57 SEE All . The . All - text and , letting . Abstract A common approach to solving multi - label learning problems is to use problem transformation methods and dichotomizing classifiers as in the pair - wise decomposition strategy . One of the problems with this strategy is the need for querying a quadratic number of binary classifiers for making a prediction that can be quite time consuming , especially in learning problems with a large number of labels . To tackle this problem , we propose a Two Stage Architecture (TSA) for efficient multi - label learning . We analyze three implementations of this architecture the Two Stage Voting Method (TSVM) , the Two Stage Classifier Chain Method (TSCCM) and the Two Stage Pruned Classifier Chain Method (TSPCCM) . Eight different real - world datasets are used to evaluate the performance of the proposed methods . The performance of our approaches is compared with the performance of two algorithm adaptation methods (Multi - Label k - NN and Multi - Label C4 . 5) and five problem transformation methods (Binary Relevance , Classifier Chain , Calibrated Label Ranking with majority voting , the Quick Weighted method for pair - wise multi - label learning and the Label Powerset method) . The results suggest that TSCCM and TSPCCM outperform the competing algorithms in terms of predictive accuracy , while TSVM has comparable predictive performance . In terms of testing speed , all three methods show better performance as compared to the pair - wise methods for multi - label learning .},
author = {Madjarov, Gjorgji and Gjorgjevikj, Dejan and D{\v{z}}eroski, Sa{\v{s}}o},
doi = {10.1016/j.patcog.2011.08.011},
file = {:home/jan/Documents/Mendeley Desktop/Madjarov, Gjorgjevikj, D{\v{z}}eroski/Unknown/Madjarov, Gjorgjevikj, D{\v{z}}eroski - 2011 - Two Stage Architecture for Multi - label Learning.pdf:pdf},
keywords = {classifier chain,multi - label classification,multi - label learning,multi - label ranking,two stage architecture},
pages = {1--24},
title = {{Two Stage Architecture for Multi - label Learning}},
volume = {00},
year = {2011}
}
@article{Huang2013,
abstract = {Both sentiment analysis and topic classification are frequently used in customer care and marketing. They can help people understand the brand perception and customer opinions from social media, such as online posts, tweets, forums, and blogs. As such, in recent years, many solutions have been proposed for both tasks. However, we believe that the following two problems have not been addressed adequately: (1) Conventional solutions usually treat the two tasks in isolation. When the two tasks are closely related (e.g., posts about "customer care" often have a "negative" tone), exploring their correlation may yield a better accuracy; (2) Each post is usually assigned with only one sentiment label and one topic label. Since social media is, compared to traditional document corpus, more noisy, ambiguous, and sparser, single label classification may not be able to capture the post classes accurately. To address these two problems, in this paper, we propose a multi-task multi-label (MTML) classification model that performs classification of both sentiments and topics concurrently. It incorporates results of each task from prior steps to promote and reinforce the other iteratively. For each task, the model is trained with multiple labels so that they can help address class ambiguity. In the empirical validation, we compare the accuracy of MTML model against four competing methods in two different settings. Results show that MTML produces a much higher accuracy of both sentiment and topic classifications.},
author = {Huang, Shu and Peng, Wei and Li, Jingxuan and Lee, Dongwon},
doi = {10.1145/2464464.2464512},
file = {:home/jan/Documents/Mendeley Desktop/Huang et al/Proceedings of the 5th Annual ACM Web Science Conference/Huang et al. - 2013 - Sentiment and Topic Analysis on Social Media A Multi-Task Multi-Label Classification Approach.pdf:pdf},
isbn = {9781450318891},
journal = {Proceedings of the 5th Annual ACM Web Science Conference},
keywords = {Multi-Label,Sentiment Analysis,Topic Classification},
mendeley-tags = {Multi-Label,Sentiment Analysis,Topic Classification},
pages = {172--181},
title = {{Sentiment and Topic Analysis on Social Media : A Multi-Task Multi-Label Classification Approach}},
year = {2013}
}
@article{Readb,
abstract = {The widely known binary relevance method for multi-label classification, which considers each label as an independent binary problem, has often been overlooked in the literature due to the perceived inadequacy of not directly modelling label correlations. Most current methods invest considerable complexity to model interdependencies between labels. This paper shows that binary relevance-based methods have much to offer, and that high predictive performance can be obtained without impeding scalability to large datasets. We exemplify this with a novel classifier chains method that can model label correlations while maintaining acceptable computational complexity. We extend this approach further in an ensemble framework. An extensive empirical evaluation covers a broad range of multi-label datasets with a variety of evaluation metrics. The results illustrate the competitiveness of the chaining method against related and state-of-the-art methods, both in terms of predictive performance and time complexity.},
archivePrefix = {arXiv},
arxivId = {arXiv:1207.6324},
author = {Read, Jesse and Pfahringer, Bernhard and Holmes, Geoff and Frank, Eibe},
doi = {10.1007/s10994-011-5256-5},
eprint = {arXiv:1207.6324},
file = {:home/jan/Documents/Mendeley Desktop/Read et al/Machine Learning/Read et al. - 2011 - Classifier chains for multi-label classification(2).pdf:pdf},
isbn = {9783642041730},
issn = {08856125},
journal = {Machine Learning},
keywords = {Ensemble methods,Multi-label classification,Problem transformation,Scalable methods},
number = {3},
pages = {333--359},
pmid = {22183238},
title = {{Classifier chains for multi-label classification}},
volume = {85},
year = {2011}
}
@article{Trohidis2008,
abstract = {In this paper, the automated detection of emotion in music is modeled as a multilabel classification task, where a piece of music may belong to more than one class. Four algorithms are evaluated and compared in this task. Furthermore, the predictive power of several audio features is evaluated using a new multilabel feature selection method. Experiments are conducted on a set of 593 songs with 6 clusters of music emotions based on the Tellegen-Watson-Clark model. Results provide interesting insights into the quality of the discussed algorithms and features.},
author = {Trohidis, Konstantinos and Kalliris, George},
file = {:home/jan/Documents/Mendeley Desktop/Trohidis, Kalliris/Proc. ISMIR/Trohidis, Kalliris - 2008 - Multi-Label Classification of Music Into Emotions.pdf:pdf},
isbn = {0615248497},
journal = {Proc. ISMIR},
pages = {325--330},
title = {{Multi-Label Classification of Music Into Emotions}},
url = {http://ismir2008.ismir.net/papers/ISMIR2008{\_}275.pdf},
volume = {2008},
year = {2008}
}
@inproceedings{McCallum1999a,
author = {McCallum, Andrew Kachites},
booktitle = {Notes AAAI Workshop Text Learn.},
file = {:home/jan/Documents/Mendeley Desktop/McCallum/AAAI'99 Workshop on Text Learning/McCallum - 1999 - “Multi-label text classification with a mixture model trained by EM.pdf:pdf},
title = {{Multi-label text classification with a mixture model trained by EM}},
url = {http://www.eecs.yorku.ca/course{\_}archive/2005-06/F/6002B/Readings/multilabel.pdf},
year = {1999}
}
@article{Tsoumakasa,
abstract = {Nowadays, multi-label classification methods are increasingly required by modern applications, such as protein function classification, music categorization and semantic scene classification. This paper intro-duces the task of multi-label classification, organizes the sparse related literature into a structured presentation and performs comparative ex-perimental results of certain multi-label classification methods. It also contributes the presentation of an undocumented method and the defi-nition of a concept for the quantification of the multi-label nature of a data set.},
author = {Tsoumakas, Grigorios and Katakis, Ioannis and Vlahavas, Ioannis},
file = {:home/jan/Documents/Mendeley Desktop/Tsoumakas, Katakis, Vlahavas/Proceedings of the 2nd ADBIS Workshop on Data Mining and Knowledge Discovery (ADMKD 2006)/Tsoumakas, Katakis, Vlahavas - Unknown - A Review of Multi-Label Classification Methods.pdf:pdf},
journal = {Proceedings of the 2nd ADBIS Workshop on Data Mining and Knowledge Discovery (ADMKD 2006)},
pages = {99--109},
title = {{A Review of Multi-Label Classification Methods}},
year = {2006}
}
@article{Sucar2013,
abstract = {a b s t r a c t In multi-label classification the goal is to assign an instance to a set of different classes. This task is normally addressed either by defining a compound class variable with all the possible combinations of labels (label power-set methods) or by building independent classifiers for each class (binary relevance methods). The first approach suffers from high computationally complexity, while the second approach ignores possible dependencies among classes. Chain classifiers have been recently proposed to address these problems, where each classifier in the chain learns and predicts the label of one class given the attributes and all the predictions of the previous classifiers in the chain. In this paper we introduce a method for chaining Bayesian classifiers that combines the strengths of classifier chains and Bayesian networks for multi-label classification. A Bayesian network is induced from data to: (i) represent the probabilistic dependency relationships between classes, (ii) constrain the number of class variables used in the chain classifier by considering conditional independence conditions, and (iii) reduce the number of possible chain orders. The effects in the Bayesian chain classifier performance of considering different chain orders, training strategies, number of class variables added in the base classifiers, and different base classifiers, are experimentally assessed. In particular, it is shown that a random chain order considering the constraints imposed by a Bayesian network with a simple tree-based structure can have very competitive results in terms of predictive performance and time complexity against related state-of-the-art approaches.},
author = {Sucar, L Enrique and Bielza, Concha and Morales, Eduardo F and Hernandez-Leal, Pablo and Zaragoza, Julio H and Larra{\~{n}}aga, Pedro},
doi = {10.1016/j.patrec.2013.11.007},
file = {:home/jan/Documents/Mendeley Desktop/Sucar et al/Unknown/Sucar et al. - 2013 - Author's personal copy Multi-label classification with Bayesian network-based chain classifiers.pdf:pdf},
keywords = {Bayesian networks,Chain classifier,Multi-label classification},
title = {{Author's personal copy Multi-label classification with Bayesian network-based chain classifiers}},
year = {2013}
}
@article{Readc,
abstract = {In multi-label classification, the main focus has been to develop ways of learning the underlying dependencies between labels, and to take advantage of this at classification time. Developing better feature-space representations has been predominantly employed to reduce complexity, e.g., by eliminating non-helpful feature attributes from the input space prior to (or during) training. This is an important task, since many multi-label methods typically create many different copies or views of the same input data as they transform it, and considerable memory can be saved by taking advantage of redundancy. In this paper, we show that a proper development of the feature space can make labels less interdependent and easier to model and predict at inference time. For this task we use a deep learning approach with restricted Boltzmann machines. We present a deep network that, in an empirical evaluation, outperforms a number of competitive methods from the literature},
archivePrefix = {arXiv},
arxivId = {1502.05988},
author = {Read, Jesse and Perez-Cruz, Fernando},
eprint = {1502.05988},
file = {:home/jan/Documents/Mendeley Desktop/Read, Perez-Cruz/arXiv preprint arXiv1502.05988/Read, Perez-Cruz - Unknown - Deep Learning for Multi-label Classification.pdf:pdf},
journal = {arXiv preprint arXiv:1502.05988},
pages = {1--8},
title = {{Deep Learning for Multi-label Classification}},
url = {https://arxiv.org/pdf/1502.05988.pdf http://arxiv.org/abs/1502.05988},
year = {2014}
}
@article{Lim2017,
abstract = {a b s t r a c t Nowadays, many data sources that include multi-label learning and multi-label classification have emerged in recent application areas. To achieve high classification accuracy, the multi-label feature selec-tion method has received much attention because its accuracy can be significantly improved by selecting important features. In previous multi-label feature selection studies, a score function was designed based on the measure of the dependency between features and labels. However, identifying the optimal feature subset is an impractical task because all possible feature subsets are 2 N , where N is the number of total features in a given dataset. Thus, the conventional methods utilized a greedy search approach that can be stuck in local optima. To circumvent the drawback of the greedy approaches, we design a score function based on mutual information and present a numerical optimization approach to avoid being stuck in the local optima. The experimental results demonstrate the superiority of the proposed multi-label feature selection method.},
author = {Lim, Hyunki and Lee, Jaesung and Kim, Dae-Won},
doi = {10.1016/j.patrec.2017.02.004},
file = {:home/jan/Documents/Mendeley Desktop/Lim, Lee, Kim/Pattern Recognition Letters/Lim, Lee, Kim - 2017 - Optimization approach for feature selection in multi-label classification.pdf:pdf},
journal = {Pattern Recognition Letters},
keywords = {Multi-label feature selection,Mutual information,Numerical optimization},
pages = {25--30},
title = {{Optimization approach for feature selection in multi-label classification}},
url = {http://ac.els-cdn.com.ez.sun.ac.za/S016786551730034X/1-s2.0-S016786551730034X-main.pdf?{\_}tid=24f68d72-1528-11e7-9527-00000aacb35d{\&}acdnat=1490864962{\_}d69b92b5449c6c0acc46c9a4c4c63940},
volume = {89},
year = {2017}
}
@article{Tsoumakas,
abstract = {Nowadays, multi-label classification methods are increasingly required by modern applications, such as protein function classification, music categorization and semantic scene classification. This paper introduces the task of multi-label classification, organizes the sparse related literature into a structured presentation and performs comparative experimental results of certain multi-label classification methods. It also contributes the definition of concepts for the quantification of the multi-label nature of a data set.},
author = {Tsoumakas, Grigorios and Katakis, Ioannis},
doi = {10.4018/jdwm.2007070101},
file = {:home/jan/Documents/Mendeley Desktop/Tsoumakas, Katakis/Unknown/Tsoumakas, Katakis - 2007 - Multi-Label Classification An Overview.pdf:pdf},
isbn = {9781424410651},
issn = {1548-3924},
title = {{Multi-Label Classification : An Overview}},
year = {2007}
}
@article{Elisseeff2001,
abstract = {ML-learning; SVM;Ranking based system},
author = {Elisseeff, a. and Weston, Jason},
file = {:home/jan/Documents/Mendeley Desktop/Elisseeff, Weston/Advances in neural information processing systems/Elisseeff, Weston - 2001 - A kernel method for multi-labelled classification.pdf:pdf},
isbn = {0262042088},
issn = {10495258},
journal = {Advances in neural information processing systems},
pages = {681--687},
title = {{A kernel method for multi-labelled classification}},
year = {2001}
}
@inproceedings{Qi,
abstract = {Automatically annotating concepts for video is a key to semantic-level video browsing, search and navigation. The research on this topic evolved through two paradigms. The first paradigm used binary classification to detect each in-dividual concept in a concept set. It achieved only limited success, as it did not model the inherent correlation between concepts, e.g., urban and building. The second paradigm added a second step on top of the individual-concept de-tectors to fuse multiple concepts. However, its performance varies because the errors incurred in the first detection step can propagate to the second fusion step and therefore de-grade the overall performance. To address the above issues, we propose a third paradigm which simultaneously classi-fies concepts and models correlations between them in a single step by using a novel Correlative Multi-Label (CML) framework. We compare the performance between our pro-posed approach and the state-of-the-art approaches in the first and second paradigms on the widely used TRECVID data set. We report superior performance from the proposed approach.},
annote = {NULL},
author = {Qi, Guo-Jun and Hua, Xian-Sheng and Rui, Yong and Tang, Jinhui and Mei, Tao and Zhang, Hong-Jiang},
booktitle = {Proceedings of the 15th international conference on Multimedia - MULTIMEDIA '07},
doi = {10.1145/1291233.1291245},
file = {:home/jan/Documents/Mendeley Desktop/Qi et al/Proceedings of the 15th international conference on Multimedia - MULTIMEDIA '07/Qi et al. - Unknown - Correlative Multi-Label Video Annotation.pdf:pdf},
isbn = {9781595937025},
issn = {15516857},
keywords = {Concept Correlation,Content Analysis and Indexing-indexing methods,Experimentation Keywords Video Annotation,I210 [Artificial Intelligence],Multi-Labeling,Theory},
pages = {17},
title = {{Correlative multi-label video annotation}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.88.4803{\&}rep=rep1{\&}type=pdf http://portal.acm.org/citation.cfm?doid=1291233.1291245},
year = {2007}
}
@article{Pang2002,
abstract = {We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging.},
archivePrefix = {arXiv},
arxivId = {cs/0205070},
author = {Pang, Bo and Lee, Lillian and Vaithyanathan, Shivakumar},
doi = {10.3115/1118693.1118704},
eprint = {0205070},
file = {:home/jan/Documents/Mendeley Desktop/Pang, Lee, Vaithyanathan/Proceedings of the ACL-02 conference on Empirical methods in natural language processing - EMNLP '02/Pang, Lee, Vaithyanathan - 2002 - Thumbs up sentiment classification using machine learning techniques.pdf:pdf},
issn = {1554-0669},
journal = {Proceedings of the ACL-02 conference on Empirical methods in natural language processing - EMNLP '02},
keywords = {Sentiment Analysis},
mendeley-tags = {Sentiment Analysis},
number = {July},
pages = {79--86},
primaryClass = {cs},
title = {{Thumbs up?: sentiment classification using machine learning techniques}},
url = {http://dl.acm.org/citation.cfm?id=1118693.1118704},
volume = {10},
year = {2002}
}
@article{Dembcz,
abstract = {The F-measure, originally introduced in information retrieval, is nowadays rou-tinely used as a performance metric for problems such as binary classification, multi-label classification, and structured output prediction. Optimizing this mea-sure remains a statistically and computationally challenging problem, since no closed-form maximizer exists. Current algorithms are approximate and typically rely on additional assumptions regarding the statistical distribution of the binary response variables. In this paper, we present an algorithm which is not only com-putationally efficient but also exact, regardless of the underlying distribution. The algorithm requires only a quadratic number of parameters of the joint distribu-tion (with respect to the number of binary responses). We illustrate its practical performance by means of experimental results for multi-label classification.},
annote = {NULL},
author = {Dembcz, Krzysztof and Waegeman, Willem and Cheng, Weiwei},
file = {:home/jan/Documents/Mendeley Desktop/Dembcz, Waegeman, Cheng/Unknown/Dembcz, Waegeman, Cheng - Unknown - An Exact Algorithm for F-Measure Maximization.pdf:pdf},
isbn = {978-1-61839-599-3},
title = {{An Exact Algorithm for F-Measure Maximization}}
}
@article{Abu-El-Haija2016,
abstract = {Many recent advancements in Computer Vision are attributed to large datasets. Open-source software packages for Machine Learning and inexpensive commodity hardware have reduced the barrier of entry for exploring novel approaches at scale. It is possible to train models over millions of examples within a few days. Although large-scale datasets exist for image understanding, such as ImageNet, there are no comparable size video classification datasets. In this paper, we introduce YouTube-8M, the largest multi-label video classification dataset, composed of {\~{}}8 million videos (500K hours of video), annotated with a vocabulary of 4800 visual entities. To get the videos and their labels, we used a YouTube video annotation system, which labels videos with their main topics. While the labels are machine-generated, they have high-precision and are derived from a variety of human-based signals including metadata and query click signals. We filtered the video labels (Knowledge Graph entities) using both automated and manual curation strategies, including asking human raters if the labels are visually recognizable. Then, we decoded each video at one-frame-per-second, and used a Deep CNN pre-trained on ImageNet to extract the hidden representation immediately prior to the classification layer. Finally, we compressed the frame features and make both the features and video-level labels available for download. We trained various (modest) classification models on the dataset, evaluated them using popular evaluation metrics, and report them as baselines. Despite the size of the dataset, some of our models train to convergence in less than a day on a single machine using TensorFlow. We plan to release code for training a TensorFlow model and for computing metrics.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1609.08675},
author = {Abu-El-Haija, Sami and Kothari, Nisarg and Lee, Joonseok and Natsev, Paul and Toderici, George and Varadarajan, Balakrishnan and Vijayanarasimhan, Sudheendra},
eprint = {1609.08675},
file = {:home/jan/Documents/Mendeley Desktop/Abu-El-Haija et al/arXiv/Abu-El-Haija et al. - Unknown - YouTube-8M A Large-Scale Video Classification Benchmark.pdf:pdf},
journal = {arXiv},
title = {{YouTube-8M: A Large-Scale Video Classification Benchmark}},
url = {https://arxiv.org/pdf/1609.08675.pdf http://arxiv.org/abs/1609.08675},
year = {2016}
}
@article{TorresTomas2014,
abstract = {A controlled environment based on known properties of the dataset used by a learning algorithm is useful to empirically evaluate machine learning algorithms. Synthetic (artificial) datasets are used for this purpose. Although there are publicly available frameworks to generate synthetic single-label datasets, this is not the case for multi-label datasets, in which each instance is associated with a set of labels usually correlated. This work presents Mldatagen, a multi-label dataset generator framework we have implemented, which is publicly available to the community. Currently, two strategies have been implemented in Mldatagen: hypersphere and hypercube. For each label in the multi-label dataset, these strategies randomly generate a geometric shape (hypersphere or hypercube), which is populated with points (instances) randomly generated. Afterwards, each instance is labeled according to the shapes it belongs to, which defines its multi-label. Experiments with a multi-label classification algorithm in six synthetic datasets illustrate the use of Mldatagen. {\textcopyright} 2014 Elsevier B.V.},
author = {Tom{\'{a}}s, Jimena Torres and Spola{\^{o}}r, Newton and Cherman, Everton Alvares and Monard, Maria Carolina},
doi = {10.1016/j.entcs.2014.01.025},
file = {:home/jan/Documents/Mendeley Desktop/Torres Tom{\'{a}}s et al/Electronic Notes in Theoretical Computer Science/Torres Tom{\'{a}}s et al. - 2014 - A Framework to Generate Synthetic Multi-label Datasets.pdf:pdf},
issn = {15710661},
journal = {Electronic Notes in Theoretical Computer Science},
keywords = {Java,PHP,artificial datasets,data generator,multi-label learning,publicly available framework},
pages = {155--176},
title = {{A framework to generate synthetic multi-label datasets}},
url = {http://ac.els-cdn.com/S1571066114000267/1-s2.0-S1571066114000267-main.pdf?{\_}tid=207a475a-25c4-11e7-9d47-00000aacb35d{\&}acdnat=1492691174{\_}037266698571d8a927f3feb0eb432995},
volume = {302},
year = {2014}
}
@article{Lee2011,
abstract = {With the increasing popularity of microblogging sites, we are in the era of information explosion. As of June 2011, about 200 million tweets are being generated everyday. Although Twitter provides a list of most popular topics people tweet about known as Trending Topics in real time, it is often hard to understand what these trending topics are about. Therefore, it is important and necessary to classify these topics into general categories with high accuracy for better information retrieval. To address this problem, we classify Twitter Trending Topics into 18 general categories such as sports, politics, technology, etc. We experiment with 2 approaches for topic classification, (i) the well-known Bag-of-Words approach for text classification and (ii) network-based classification. In text-based classification method, we construct word vectors with trending topic definition and tweets, and the commonly used tf-idf weights are used to classify the topics using a Naive Bayes Multinomial classifier. In network-based classification method, we identify top 5 similar topics for a given topic based on the number of common influential users. The categories of the similar topics and the number of common influential users between the given topic and its similar topics are used to classify the given topic using a C5.0 decision tree learner. Experiments on a database of randomly selected 768 trending topics (over 18 classes) show that classification accuracy of up to 65{\&}{\#}x025; and 70{\&}{\#}x025; can be achieved using text-based and network-based classification modeling respectively.},
author = {Lee, Kathy and Palsetia, Diana and Narayanan, Ramanathan and Patwary, Md Mostofa Ali and Agrawal, Ankit and Choudhary, Alok},
doi = {10.1109/ICDMW.2011.171},
file = {:home/jan/Documents/Mendeley Desktop/Lee et al/Proceedings - IEEE International Conference on Data Mining, ICDM/Lee et al. - 2011 - Twitter trending topic classification.pdf:pdf},
isbn = {9780769544090},
issn = {15504786},
journal = {Proceedings - IEEE International Conference on Data Mining, ICDM},
keywords = {Social networks,Topic classification,Twitter},
pages = {251--258},
title = {{Twitter trending topic classification}},
year = {2011}
}
@article{Li2013,
abstract = {Multi-label classification has attracted an increasing amount of attention in recent years. To this end, many algorithms have been developed to classify multi-label data in an effective manner. However, they usually do not consider the pairwise relations indicated by sample labels, which actually play important roles in multi-label classification. Inspired by this, we naturally extend the traditional pairwise constraints to the multi-label scenario via a flexible thresholding scheme. Moreover, to improve the generalization ability of the classifier, we adopt a boosting-like strategy to construct a multi-label ensemble from a group of base classifiers. To achieve these goals, this paper presents a novel multi-label classification framework named Variable Pairwise Constraint projection for Multi-label Ensemble (VPCME). Specifically, we take advantage of the variable pairwise constraint projection to learn a lower-dimensional data representation, which preserves the correlations between samples and labels. Thereafter, the base classifiers are trained in the new data space. For the boosting-like strategy, we employ both the variable pairwise constraints and the bootstrap steps to diversify the base classifiers. Empirical studies have shown the superiority of the proposed method in comparison with other approaches. ?? 2012 Elsevier Inc. All rights reserved.},
author = {Li, Ping and Li, Hong and Wu, Min},
doi = {10.1016/j.ins.2012.07.066},
file = {:home/jan/Documents/Mendeley Desktop/Li, Li, Wu/Information Sciences/Li, Li, Wu - 2013 - Multi-label ensemble based on variable pairwise constraint projection.pdf:pdf},
issn = {00200255},
journal = {Information Sciences},
keywords = {Boosting,Constraint projection,Ensemble learning,Multi-Label,Multi-label classification,Variable pairwise constraints},
mendeley-tags = {Multi-Label},
pages = {269--281},
title = {{Multi-label ensemble based on variable pairwise constraint projection}},
volume = {222},
year = {2013}
}
@article{Dembczynski2010,
abstract = {In multi-label classification (MLC), each instance is associated with$\backslash$na subset of labels instead of a single class, as in conventional$\backslash$nclassification, and this generalization enables the definition of$\backslash$na multitude of loss functions. Indeed, a large number of losses has$\backslash$nalready been proposed and is commonly applied as performance metrics$\backslash$nin experimental studies. However, even though these loss functions$\backslash$nare of a quite different nature, a concrete connection between the$\backslash$ntype of multi-label classifier used and the loss to be minimized$\backslash$nis rarely established, implicitly giving the misleading impression$\backslash$nthat the same method can be optimal for different loss functions.$\backslash$nIn this paper, we elaborate on risk minimization and the connection$\backslash$nbetween loss functions in MLC, both theoretically and empirically.$\backslash$nIn particular, we compare two important loss functions, namely the$\backslash$nHamming loss and the subset 0/1 loss. We perform a regret analysis,$\backslash$nshowing how poor a classifier intended to minimize the subset 0/1$\backslash$nloss can become in terms of Hamming loss and vice versa. The theoretical$\backslash$nresults are corroborated by experimental studies, and their implications$\backslash$nfor MLC methods are discussed in a broader context. 漏 2010 Springer-Verlag$\backslash$nBerlin Heidelberg.},
author = {Dembczy{\'{n}}ski, Krzysztof and Waegeman, Willem and Cheng, Weiwei and H{\"{u}}llermeier, Eyke},
doi = {10.1007/978-3-642-15880-3_24},
isbn = {364215879X},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 1},
pages = {280--295},
title = {{Regret analysis for performance metrics in multi-label classification: The case of hamming and subset zero-one loss}},
volume = {6321 LNAI},
year = {2010}
}
@article{Madjarov2012a,
abstract = {This article appeared in a journal published by Elsevier. The attached copy is furnished to the author for internal non-commercial research and education use, including for instruction at the authors institution and sharing with colleagues. Other uses, including reproduction and distribution, or selling or licensing copies, or posting to personal, institutional or third party websites are prohibited. In most cases authors are permitted to post their version of the article (e.g. in Word or Tex form) to their personal website or institutional repository. Authors requiring further information regarding Elsevier's archiving and manuscript policies are encouraged to visit: a b s t r a c t Multi-label learning has received significant attention in the research community over the past few years: this has resulted in the development of a variety of multi-label learning methods. In this paper, we present an extensive experimental comparison of 12 multi-label learning methods using 16 evaluation measures over 11 benchmark datasets. We selected the competing methods based on their previous usage by the community, the representation of different groups of methods and the variety of basic underlying machine learning methods. Similarly, we selected the evaluation measures to be able to assess the behavior of the methods from a variety of view-points. In order to make conclusions independent from the application domain, we use 11 datasets from different domains. Furthermore, we compare the methods by their efficiency in terms of time needed to learn a classifier and time needed to produce a prediction for an unseen example. We analyze the results from the experiments using Friedman and Nemenyi tests for assessing the statistical significance of differences in performance. The results of the analysis show that for multi-label classification the best performing methods overall are random forests of predictive clustering trees (RF-PCT) and hierarchy of multi-label classifiers (HOMER), followed by binary relevance (BR) and classifier chains (CC). Furthermore, RF-PCT exhibited the best performance according to all measures for multi-label ranking. The recommendation from this study is that when new methods for multi-label learning are proposed, they should be compared to RF-PCT and HOMER using multiple evaluation measures.},
author = {Madjarov, Gjorgji and Kocev, Dragi and Gjorgjevikj, Dejan and D{\v{z}}eroski, Sa{\v{s}}o},
file = {:home/jan/Documents/Mendeley Desktop/Madjarov et al/Unknown/Madjarov et al. - 2012 - Author's personal copy An extensive experimental comparison of methods for multi-label learning.pdf:pdf},
keywords = {Comparison of multi-label learning methods,Multi-label classification,Multi-label ranking},
title = {{Author's personal copy An extensive experimental comparison of methods for multi-label learning}},
url = {http://www.elsevier.com/copyright},
year = {2012}
}
@article{Chen2007,
abstract = {Feature selection on multi-label documents for automatic text categorization$\backslash$nis an under-explored research area. This paper presents a systematic$\backslash$ndocument transformation framework, whereby the multi-label documents$\backslash$nare transformed into single-label documents before applying standard$\backslash$nfeature selection algorithms, to solve the multi-label feature selection$\backslash$nproblem. Under this framework, we undertake a comparative study on$\backslash$nfour intuitive document transformation approaches and propose a novel$\backslash$napproach called entropy-based label assignment (ELA), which assigns$\backslash$nthe labels weights to a multi-label document based on label entropy.$\backslash$nThree standard feature selection algorithms are utilized for evaluating$\backslash$nthe document transformation approaches in order to verify its impact$\backslash$non multi-class text categorization problems. Using a SVM classifier$\backslash$nand two multi-label evaluation benchmark text collections, we show$\backslash$nthat the choice of document transformation approaches can significantly$\backslash$ninfluence the performance of multi-class categorization and that$\backslash$nour proposed document transformation approach ELA can achieve better$\backslash$nperformance than all other approaches.},
author = {Chen, Weizhu and Yan, Jun and Zhang, Benyu and Chen, Zheng and Yang, Qiang},
doi = {10.1109/ICDM.2007.18},
file = {:home/jan/Documents/Mendeley Desktop/Chen et al/Proceedings - IEEE International Conference on Data Mining, ICDM/04470272.pdf:pdf},
isbn = {0769530184},
issn = {15504786},
journal = {Proceedings - IEEE International Conference on Data Mining, ICDM},
pages = {451--456},
title = {{Document transformation for multi-label feature selection in text categorization}},
year = {2007}
}
@article{CarvalhoAndreCPLFde2009,
abstract = {Most classification problems associate a single class to each example or instance. However, there are many classification tasks where each instance can be associated with one or more classes. This group of problems represents an area known as multi-label classification. One typical example of multi-label classification problems is the classification of documents, where each document can be assigned to more than one class. This tutorial presents the most frequently used techniques to deal with these problems in a pedagogical manner, with examples illustrating the main techniques and proposing a taxonomy of multi-label techniques that highlights the similarities and differences between these techniques.},
author = {{Carvalho, Andr{\'{e}} C P L F de}, Alex A. Freitas},
doi = {10.1007/978-3-642-01536-6_8},
file = {:home/jan/Documents/Mendeley Desktop/Carvalho, Andr{\'{e}} C P L F de/Foundations of Computational Intelligence/C, De Carvalho, Freitas - Unknown - A Tutorial on Multi-Label Classification Techniques.pdf:pdf},
isbn = {978-3-642-01535-9},
journal = {Foundations of Computational Intelligence},
pages = {177--195},
title = {{A Tutoria on Multi-Label Classification Techniques}},
url = {http://www.icmc.usp.br/{~}andre http://www.cs.kent.ac.uk/{~}aaf http://www.cs.kent.ac.uk/people/staff/aaf/pub{\_}papers.dir/Found-Comp-Intel-bk-ch-2009-Carvalho.pdf},
volume = {5},
year = {2009}
}
@article{Kwok2013,
abstract = {Although the social medium Twitter grants users freedom of speech, its instantaneous nature and retweeting features also amplify hate speech. Because Twitter has a sizeable black constituency, racist tweets against blacks are especially det- rimental in the Twitter community, though this effect may not be obvious against a backdrop of half a billion tweets a day.1 We apply a supervised machine learning approach, employing inexpensively acquired labeled data from diverse Twitter accounts to learn a binary classifier for the labels “racist” and “nonracist.” The classifier has a 76{\%} average accuracy on individual tweets, suggesting that with further improvements, our work can contribute data on the sources of anti-black hate speech.},
author = {Kwok, Irene and Wang, Yuzhou},
file = {:home/jan/Documents/Mendeley Desktop/Kwok, Wang/Twenty-Seventh AAAI Conference on Artificial Intelligence/Kwok, Wang - 2013 - Locate the Hate Detecting Tweets against Blacks.pdf:pdf},
isbn = {9781577356158},
journal = {Twenty-Seventh AAAI Conference on Artificial Intelligence},
keywords = {Student Abstract and Poster Program},
pages = {1621--1622},
title = {{Locate the Hate: Detecting Tweets against Blacks}},
url = {http://www.google.com/url?sa=t{\&}rct=j{\&}q={\&}esrc=s{\&}source=web{\&}cd=1{\&}ved=0CC0QFjAA{\&}url=http://www.aaai.org/ocs/index.php/AAAI/AAAI13/paper/download/6419/6821{\&}ei=e7hJUq2EAtKq4AOB04HoDg{\&}usg=AFQjCNEi9mX0w71lUCo8tdxTnQJkR74MLg{\&}am},
year = {2013}
}
@article{Snoek2006,
abstract = {We introduce the challenge problem for generic video indexing to gain insight in intermediate steps that affect performance of multimedia analysis methods, while at the same time fostering repeatability of experiments. To arrive at a challenge problem, we provide a general scheme for the systematic examination of automated concept detection methods, by decomposing the generic video indexing problem into 2 unimodal analysis experiments, 2 multimodal analysis experiments, and 1 combined analysis experiment. For each experiment, we evaluate generic video indexing performance on 85 hours of international broadcast news data, from the TRECVID 2005/2006 benchmark, using a lexicon of 101 semantic concepts. By establishing a minimum performance on each experiment, the challenge problem allows for component-based optimization of the generic indexing issue, while simultaneously offering other researchers a reference for comparison during indexing methodology development. To stimulate further investigations in intermediate analysis steps that inuence video indexing performance, the challenge offers to the research community a manually annotated concept lexicon, pre-computed low-level multimedia features, trained classifier models, and five experiments together with baseline performance, which are all available at http://www.mediamill.nl/challenge/.},
annote = {NULL},
author = {Snoek, Cees G M and Worring, Marcel and {Van Gemert}, Jan C and Geusebroek, Jan-Mark and Smeulders, Arnold W M},
doi = {10.1145/1180639.1180727},
file = {:home/jan/Documents/Mendeley Desktop/Snoek et al/Proceedings of the 14th annual ACM international conference on Multimedia MULTIMEDIA 06/Snoek et al. - 2006 - The Challenge Problem for Automated Detection of 101 Semantic Concepts in Multimedia.pdf:pdf},
isbn = {1595934472},
journal = {Proceedings of the 14th annual ACM international conference on Multimedia MULTIMEDIA 06},
keywords = {baseline,capture,field multimedia,fueled ever increasing,generic concept detection,growth recent years,indexing has witnessed a,rapid,video analysis},
pages = {421--430},
title = {{The challenge problem for automated detection of 101 semantic concepts in multimedia}},
url = {http://www.mediamill.nl/challenge/. http://portal.acm.org/citation.cfm?doid=1180639.1180727},
year = {2006}
}
@article{Xu2016,
abstract = {Tail labels in the multi-label learning problem undermine the low-rank assumption. Nevertheless, this problem has rarely been investigated. In addition to using the low-rank structure to depict label correlations, this paper explores and exploits an additional sparse component to handle tail labels behaving as outliers, in order to make the classical low-rank principle in multi-label learning valid. The divideand-conquer optimization technique is employed to increase the scalability of the proposed algorithm while theoretically guaranteeing its performance. A theoretical analysis of the generalizability of the proposed algorithm suggests that it can be improved by the low-rank and sparse decomposition given tail labels. Experimental results on real-world data demonstrate the significance of investigating tail labels and the effectiveness of the proposed algorithm.},
archivePrefix = {arXiv},
arxivId = {arXiv:1602.05561v1},
author = {Xu, Chang and Tao, Dacheng and Xu, Chao},
doi = {10.475/123},
eprint = {arXiv:1602.05561v1},
file = {:home/jan/Documents/Mendeley Desktop/Xu, Tao, Xu/KDD/Xu, Tao, Xu - 2016 - Robust Extreme Multi-Label Learning.pdf:pdf},
isbn = {9781450335423},
issn = {0146-4833},
journal = {KDD},
keywords = {low-rank algorithm,multi-label learning},
pages = {421--434},
title = {{Robust Extreme Multi-Label Learning}},
year = {2016}
}
@article{Boutell2004a,
abstract = {In classic pattern recognition problems, classes are mutually exclusive by de{\"{y}}nition. Classi{\"{y}}cation errors occur when the classes overlap in the feature space. We examine a diierent situation, occurring when the classes are, by de{\"{y}}nition, not mutually exclusive. Such problems arise in semantic scene and document classi{\"{y}}cation and in medical diagnosis. We present a framework to handle such problems and apply it to the problem of semantic scene classi{\"{y}}cation, where a natural scene may contain multiple objects such that the scene can be described by multiple class labels (e.g., a {\"{y}}eld scene with a mountain in the background). Such a problem poses challenges to the classic pattern recognition paradigm and demands a diierent treatment. We discuss approaches for training and testing in this scenario and introduce new metrics for evaluating individual examples, class recall and precision, and overall accuracy. Experiments show that our methods are suitable for scene classi{\"{y}}cation; furthermore, our work appears to generalize to other classi{\"{y}}cation problems of the same nature.},
author = {Boutell, Matthew R and Luo, Jiebo and Shen, Xipeng and Brown, Christopher M},
doi = {10.1016/j.patcog.2004.03.009},
file = {:home/jan/Documents/Mendeley Desktop/Boutell et al/Pattern Recognition/Boutell et al. - 2004 - Learning multi-label scene classi{\"{y}}cation.pdf:pdf},
journal = {Pattern Recognition},
keywords = {Cross-training,Image organization,Image understanding,Jaccard similarity,Multi-label classi{\"{y}}cation,Multi-label evaluation,Multi-label training,Semantic scene classi{\"{y}}cation},
pages = {1757--1771},
title = {{Learning multi-label scene classi{\"{y}}cation}},
url = {www.elsevier.com/locate/patcog},
volume = {37},
year = {2004}
}
@article{Berger,
abstract = {Multi-label text classification has been applied to a multitude of tasks, including document indexing, tag suggestion, and sentiment classification. However, many of these methods disregard word order, opting to use bag-of-words models or TF- IDF weighting to create document vectors. With the advent of powerful semantic embeddings, such as word2vec and GloVe, we explore how word embeddings and word order can be used to improve multi-label learning. Specifically, we explore how both a convolutional neural network (CNN) and a recurrent network with a gated recurrent unit (GRU) can independently be used with pre-trained word2vec embeddings to solve a large scale multi-label text classification problem. On a data set of over two million documents and 1000 potential labels, we demonstrate that both a CNN and a GRU provide substantial improvement over a Binary Relevance model with a bag-of-words representation.},
author = {Berger, Mark J},
file = {:home/jan/Documents/Mendeley Desktop/Berger/Technical Report/Berger - 2014 - Large Scale Multi-label Text Classification with Semantic Word Vectors.pdf:pdf},
journal = {Technical Report},
keywords = {Multi-Label,Text Classification},
mendeley-tags = {Multi-Label,Text Classification},
pages = {1--8},
title = {{Large Scale Multi-label Text Classification with Semantic Word Vectors}},
year = {2014}
}
@article{Sun,
abstract = {—Canonical Correlation Analysis (CCA) is a well-known technique for finding the correlations between two sets of multidimensional variables. It projects both sets of variables onto a lower-dimensional space in which they are maximally correlated. CCA is commonly applied for supervised dimensionality reduction in which the two sets of variables are derived from the data and the class labels, respectively. It is well-known that CCA can be formulated as a least-squares problem in the binary class case. However, the extension to the more general setting remains unclear. In this paper, we show that under a mild condition which tends to hold for high-dimensional data, CCA in the multilabel case can be formulated as a least-squares problem. Based on this equivalence relationship, efficient algorithms for solving least-squares problems can be applied to scale CCA to very large data sets. In addition, we propose several CCA extensions, including the sparse CCA formulation based on the 1-norm regularization. We further extend the least-squares formulation to partial least squares. In addition, we show that the CCA projection for one set of variables is independent of the regularization on the other set of multidimensional variables, providing new insights on the effect of regularization on CCA. We have conducted experiments using benchmark data sets. Experiments on multilabel data sets confirm the established equivalence relationships. Results also demonstrate the effectiveness and efficiency of the proposed CCA extensions.},
annote = {NULL},
author = {Sun, Liang and Ji, Shuiwang and Ye, Jieping},
file = {:home/jan/Documents/Mendeley Desktop/Sun, Ji, Ye/Unknown/Sun, Ji, Ye - Unknown - Canonical Correlation Analysis for Multilabel Classification A Least-Squares Formulation, Extensions, and Analys.pdf:pdf},
keywords = {Index Terms—Canonical correlation analysis,least squares,multilabel learning,partial least squares,regularization},
title = {{Canonical Correlation Analysis for Multilabel Classification: A Least-Squares Formulation, Extensions, and Analysis}}
}
@article{Zhang2003,
abstract = {The singer's information is essential in organizing, browsing and retrieving music collections. In this technical report, a system for automatic singer identification is developed which recognizes the singer of a song by analyzing the music signal. Meanwhile, songs which are similar in terms of singer's voice are clustered. The proposed scheme follows the framework of common speaker identification systems, but special efforts are made to distinguish the singing voice from instrumental sounds in a song. A statistical model is trained for each singer's voice with typical song(s) of the singer. Then, for a song to be identified, the starting point of singing voice is detected and a portion of the song is excerpted from that point. Audio features are extracted and matched with singers' voice models in the database. The song is assigned to the model having the best match. Promising results are obtained on a small set of samples, and accuracy rates of around 80{\%} are achieved.},
author = {Zhang, Tong},
file = {:home/jan/Documents/Mendeley Desktop/Zhang/IEEE International Conference on Multimedia and Expo/Zhang - 2003 - System and Method for Automatic Singer Identification System and Method for Automatic Singer Identification.pdf:pdf},
isbn = {2974619401},
journal = {IEEE International Conference on Multimedia and Expo},
number = {July},
pages = {6--9},
title = {{System and Method for Automatic Singer Identification System and Method for Automatic Singer Identification}},
year = {2003}
}
@article{Sorower,
abstract = {Multi-label Learning is a form of supervised learning where the classification al- gorithm is required to learn from a set of instances, each instance can belong to multiple classes and so after be able to predict a set of class labels for a new in- stance. This is a generalized version of most popular multi-class problems where each instances is restricted to have only one class label. There exists a wide range of applications for multi-labelled predictions, such as text categorization, seman- tic image labeling, gene functionality classification etc. and the scope and interest is increasing with modern applications. This survey paper introduces the task of multi-label prediction (classification), presents the sparse literature in this area in an organized manner, discusses different evaluation metrics and performs a com- parative analysis of the existing algorithms. This paper also relates multi-label problems with similar but different problems that are often reduced to multi-label problems to have access to wide range of multi-label algorithms.},
author = {Sorower, Ms},
file = {:home/jan/Documents/Mendeley Desktop/Sorower/Oregon State University, Corvallis/Sorower - Unknown - A Literature Survey on Algorithms for Multi-label Learning.pdf:pdf},
journal = {Oregon State University, Corvallis},
pages = {1--25},
title = {{A literature survey on algorithms for multi-label learning}},
url = {http://people.oregonstate.edu/{~}sorowerm/pdf/Qual-Multilabel-Shahed-CompleteVersion.pdf},
year = {2010}
}
@article{Ng2015,
abstract = {Beyond Short Snippets: Deep Networks for Video Classification Joe Yue-Hei Ng 1 yhng@umiacs.umd.edu Matthew Hausknecht 2 mhauskn@cs.utexas.edu Sudheendra Vijayanarasimhan 3 svnaras@google.com Oriol Vinyals 3 vinyals@google.com Rajat Monga 3 rajatmonga@google.com George Toderici 3 gtoderici@google.com 1 University of Maryland, College Park 2 University of Texas at Austin 3 Google, Inc. Abstract Convolutional neural networks (CNNs) have been exten- sively applied for image recognition problems giving state- of-the-art results on recognition, detection, segmentation and retrieval. In this work we propose and evaluate several deep neural network architectures to combine image infor- mation across a video over longer time periods than previ- ously attempted. We propose two methods capable of han- dling full length videos. The first method explores various convolutional temporal feature pooling architectures, ex- amining the various design choices which need to be made when adapting a CNN for this task. The second proposed method explicitly models the video as an ordered sequence of frames. For this purpose we employ a recurrent neural network that uses Long Short-Term Memory (LSTM) cells which are connected to the output of the underlying CNN. Our best networks exhibit significant performance improve- ments over previously published results on the Sports 1 mil- lion dataset (73.1{\%} vs. 60.9{\%}) and the UCF-101 datasets with (88.6{\%} vs. 88.0{\%}) and without additional optical flow information (82.6{\%} vs. 73.0{\%})},
archivePrefix = {arXiv},
arxivId = {arXiv:1503.08909v2},
author = {Ng, Joe Yue-Hei},
doi = {10.1109/CVPR.2015.7299101},
eprint = {arXiv:1503.08909v2},
file = {:home/jan/Documents/Mendeley Desktop/Ng/Unknown/Ng - Unknown - Beyond Short Snippets Deep Networks for Video Classification.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
title = {{Beyond Short Snippets : Deep Networks for Video Classification}},
url = {https://pdfs.semanticscholar.org/57ed/4de2c8ea9c865dcf4273f0576eb746263475.pdf?{\_}ga=1.106149697.379343330.1490351020},
year = {2015}
}
@misc{Schapire99improvedboosting,
author = {Schapire, Robert E and Singer, Yoram},
title = {{Improved Boosting Algorithms Using Confidence-rated Predictions}},
year = {1999}
}
@article{Godbole2007,
abstract = {Newspapers and blogs express opinion of news entities (peo- ple, places, things) while reporting on recent events. We present a system that assigns scores indicating positive or negative opinion to each distinct entity in the text corpus. Our system consists of a sentiment identication phase, which associates expressed opinions with each relevant entity, and a sentiment aggregation and scoring phase, which scores each entity relative to others in the same class. Finally, we evalu- ate the signicance of our scoring techniques over large corpus of news and blogs.},
archivePrefix = {arXiv},
arxivId = {cond-mat/0112110},
author = {Godbole, N and Srinivasaiah, M},
doi = {10.1177/01461079070370040501},
eprint = {0112110},
file = {:home/jan/Documents/Mendeley Desktop/Godbole, Srinivasaiah/Conference on Weblogs and Social Media (ICWSM 2007)/Godbole, Srinivasaiah - 2007 - Large-scale sentiment analysis for news and blogs.pdf:pdf},
issn = {0146-1079},
journal = {Conference on Weblogs and Social Media (ICWSM 2007)},
keywords = {News,Sentiment Analysis,ge-scale sentiment analysis for,news and blogs},
mendeley-tags = {News,Sentiment Analysis},
pages = {219--222},
pmid = {12060727},
primaryClass = {cond-mat},
title = {{Large-scale sentiment analysis for news and blogs}},
url = {http://student.bus.olemiss.edu/files/conlon/Others/{\_}{\_}BookChapter{\_}SocialMEsia{\_}EBusiness/Large-Scale Sentiment Analysis for News and Blogs.pdf},
year = {2007}
}
@article{Dembczyski2010,
abstract = {In the realm of multilabel classification (MLC), it has become an opinio communis that optimal predictive performance can only be achieved by learners that explicitly take label dependence into account. The goal of this paper is to elaborate on this postulate in a critical way. To this end, we formal- ize and analyze MLC within a probabilistic setting. Thus, it becomes possible to look at the problem from the point of view of risk minimization and Bayes optimal predic- tion. Moreover, inspired by our probabilistic setting, we propose a new method for MLC that generalizes and outperforms another ap- proach, called classifier chains, that was re- cently introduced in the literature.},
author = {Dembczy, Krzysztof},
file = {:home/jan/Documents/Mendeley Desktop/Dembczyski, Cheng, H{\"{u}}llermeier/Unknown/Dembczyski, Cheng, H{\"{u}}llermeier - 2010 - Bayes Optimal Multilabel Classification via Probabilistic Classifier Chains.pdf:pdf},
isbn = {9781605589077},
journal = {Proceedings of the 27th international conference on machine learning (ICML-10)},
keywords = {chain classifiers,label correlation,multilabel classification,risk minimization},
pages = {279--286},
title = {{Bayes Optimal Multilabel Classification via Probabilistic Classifier Chains}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/icml2010{\_}DembczynskiCH10.pdf http://www.uni-marburg.de/fb12/kebi/people/cheng/cheng-icml10c.pdf},
year = {2010}
}
@article{Wu2014,
author = {Wu, Yunong and Kita, Kenji and Matsumoto, Kazuyuki},
doi = {10.1002/tee.22020},
file = {:home/jan/Documents/Mendeley Desktop/Wu, Kita, Matsumoto/IEEJ Transactions on Electrical and Electronic Engineering/Wu, Kita, Matsumoto - 2014 - Three predictions are better than one Sentence multi-emotion analysis from different perspectives.pdf:pdf},
issn = {19314973},
journal = {IEEJ Transactions on Electrical and Electronic Engineering},
keywords = {Emotion Analysis,Multi-Label,conditional random fields,crf,integrated prediction,l-lda,labeled latent dirichlet allocation,lgr,logistic,multi-emotion,received 28 june 2013,regression,revised 4 january 2014,sentence emotion analysis},
mendeley-tags = {Emotion Analysis,Multi-Label},
number = {6},
pages = {642--649},
title = {{Three predictions are better than one: Sentence multi-emotion analysis from different perspectives}},
url = {http://doi.wiley.com/10.1002/tee.22020},
volume = {9},
year = {2014}
}
@article{Zhang2009a,
abstract = {In multi-label learning, the training set is made up of instances each associated with a set of labels, and the task is to predict the label sets of unseen instances. In this paper, this learning problem is addressed by using a method called Mlnb which adapts the traditional naive Bayes classifiers to deal with multi-label instances. Feature selection mechanisms are incorporated into Mlnb to improve its performance. Firstly, feature extraction techniques based on principal component analysis are applied to remove irrelevant and redundant features. After that, feature subset selection techniques based on genetic algorithms are used to choose the most appropriate subset of features for prediction. Experiments on synthetic and real-world data show that Mlnb achieves comparable performance to other well-established multi-label learning algorithms. {\textcopyright} 2009 Elsevier Inc. All rights reserved.},
author = {Zhang, Min Ling and Pe{\~{n}}a, Jos{\'{e}} M and Robles, Victor},
doi = {10.1016/j.ins.2009.06.010},
file = {:home/jan/Documents/Mendeley Desktop/Zhang, Pe{\~{n}}a, Robles/Information Sciences/Zhang, Pe{\~{n}}a, Robles - Unknown - Author's personal copy Feature selection for multi-label naive Bayes classification.pdf:pdf},
isbn = {0020-0255},
issn = {00200255},
journal = {Information Sciences},
keywords = {Feature selection,Genetic algorithm,Multi-label learning,Naive Bayes,Principal component analysis},
number = {19},
pages = {3218--3229},
title = {{Feature selection for multi-label naive Bayes classification}},
url = {http://www.elsevier.com/copyright},
volume = {179},
year = {2009}
}
@article{Turnbull2008,
abstract = {—We present a computer audition system that can both annotate novel audio tracks with semantically meaningful words and retrieve relevant tracks from a database of unlabeled audio content given a text-based query. We consider the related tasks of content-based audio annotation and retrieval as one supervised multiclass, multilabel problem in which we model the joint proba-bility of acoustic features and words. We collect a data set of 1700 human-generated annotations that describe 500 Western popular music tracks. For each word in a vocabulary, we use this data to train a Gaussian mixture model (GMM) over an audio feature space. We estimate the parameters of the model using the weighted mixture hierarchies expectation maximization algorithm. This algorithm is more scalable to large data sets and produces better density estimates than standard parameter estimation techniques. The quality of the music annotations produced by our system is comparable with the performance of humans on the same task. Our " query-by-text " system can retrieve appropriate songs for a large number of musically relevant words. We also show that our audition system is general by learning a model that can annotate and retrieve sound effects.},
author = {Turnbull, Douglas and Barrington, Luke and Torres, David and Lanckriet, Gert},
doi = {10.1109/TASL.2007.913750},
file = {:home/jan/Documents/Mendeley Desktop/Turnbull et al/IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING/Turnbull et al. - 2008 - Semantic Annotation and Retrieval of Music and Sound Effects.pdf:pdf},
journal = {IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING},
keywords = {Index Terms—Audio annotation and retrieval,music informa-tion retrieval,semantic music analysis},
number = {2},
title = {{Semantic Annotation and Retrieval of Music and Sound Effects}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.297.2154{\&}rep=rep1{\&}type=pdf},
volume = {16},
year = {2008}
}
@article{Gupta2014,
abstract = {During sudden onset crisis events, the presence of spam, rumors and fake content on Twitter reduces the value of information contained on its messages (or {\&}ldquo;tweets{\&}rdquo;). A possible solution to this problem is to use machine learning to automatically evaluate the credibility of a tweet, i.e. whether a person would deem the tweet believable or trustworthy. This has been often framed and studied as a supervised classification problem in an off-line (post-hoc) setting. In this paper, we present a semi-supervised ranking model for scoring tweets according to their credibility. This model is used in TweetCred, a real-time system that assigns a credibility score to tweets in a user{\&}rsquo;s timeline. TweetCred, available as a browser plug-in, was installed and used by 1,127 Twitter users within a span of three months. During this period, the credibility score for about 5.4 million tweets was computed, allowing us to evaluate TweetCred in terms of response time, effectiveness and usability. To the best of our knowledge, this is the first research work to develop a real-time system for credibility on Twitter, and to evaluate it on a user base of this size. {\&}copy; Springer International Publishing Switzerland 2014.},
archivePrefix = {arXiv},
arxivId = {1405.5490},
author = {Gupta, Aditi and Kumaraguru, Ponnurangam and Castillo, Carlos and Meier, Patrick},
doi = {10.1007/978-3-319-13734-6_16},
eprint = {1405.5490},
file = {:home/jan/Documents/Mendeley Desktop/Gupta et al/Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)/Gupta et al. - 2014 - TweetCred Real-time credibility assessment of content on Twitter.pdf:pdf},
isbn = {9783319137339},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {228--243},
title = {{TweetCred: Real-time credibility assessment of content on Twitter}},
url = {https://www.engineeringvillage.com/blog/document.url?mid=cpx{\_}5c6c95b714b5b731481M6bd71017816338{\&}database=cpx},
volume = {8851},
year = {2014}
}
@article{Papanikolaou,
abstract = {This paper documents the systems that we developed for our participation in the BioASQ 2014 large-scale bio-medical semantic indexing and question answering challenge. For the large-scale semantic indexing task, we employed a novel multi-label ensemble method con-sisting of support vector machines, labeled Latent Dirichlet Allocation models and meta-models predicting the number of relevant labels. This method proved successful in our experiments as well as during the compe-tition. For the question answering task we combined different techniques for scoring of candidate answers based on recent literature.},
author = {Papanikolaou, Yannis and Dimitriadis, Dimitrios and Tsoumakas, Grigorios and Laliotis, Manos and Markantonatos, Nikos and Vlahavas, Ioannis},
file = {:home/jan/Documents/Mendeley Desktop/Papanikolaou et al/CEUR Workshop Proceedings/Papanikolaou et al. - 2014 - Ensemble approaches for large-scale multi-label classification and question answering in biomedicine.pdf:pdf},
issn = {16130073},
journal = {CEUR Workshop Proceedings},
keywords = {BioASQ,Ensemble methods,Latent Dirichlet Allocation,Multi-label learning,Support vector machines},
pages = {1348--1360},
title = {{Ensemble approaches for large-scale multi-label classification and question answering in biomedicine}},
volume = {1180},
year = {2014}
}
@article{Tai2010,
abstract = {We propose a novel hypercube view that per-ceives the label space of multi-label classifi-cation problems geometrically. The view al-lows us to not only unify many existing multi-label classification approaches, but also de-sign a novel algorithm, Principle Label Space Transformation (PLST), which seeks impor-tant correlations between labels before learn-ing. The simple and efficient PLST relies on only singular value decomposition as the key step. Experimental results demonstrate that PLST is faster than the traditional Bi-nary Relevance approach and is superior to the modern Compressive Sensing approach in terms of both performance and efficiency.},
annote = {NULL},
author = {Tai, Farbound and Lin, H.T.},
file = {:home/jan/Documents/Mendeley Desktop/Tai, Lin/International Workshop on Learning from Multi-Label Data/Tai, Lin - Unknown - Multi-label Classification with Principle Label Space Transformation.pdf:pdf},
journal = {International Workshop on Learning from Multi-Label Data},
pages = {45},
title = {{Multi-label classification with principle label space transformation}},
url = {http://ntur.lib.ntu.edu.tw/retrieve/188514/18.pdf http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.170.1860{\&}rep=rep1{\&}type=pdf{\#}page=46},
year = {2010}
}
@article{Reada,
abstract = {Multi-label classification has gained significant interest in recent years, paralleled by the increasing use of manual multi-labelling, often known as applying "tags" to documents. Well known examples include Flickr 1, YouTube2, CiteULike3 and Google Bookmarks4. This paper focuses on Problem Transformation (PT) as an approach to multi-label classification and details these methods as well as their respective advantages and disadvantages. A Pruned Problem Transformation method (PPT) is presented, along with several extensions, designed to overcome such disadvantages. This new method is empirically compared with existing methods, both in terms of accuracy and training time, and the results are encouraging.},
author = {Read, J},
file = {:home/jan/Documents/Mendeley Desktop/Read/New Zealand Computer Science Research Student Conference, NZCSRSC 2008 - Proceedings/Read - 2008 - A pruned problem transformation method for multi-label classification.pdf:pdf},
journal = {New Zealand Computer Science Research Student Conference, NZCSRSC 2008 - Proceedings},
keywords = {Computer science,Information retrieval systems,Multi-label classifications; Problem transformatio},
number = {April},
pages = {143--150},
title = {{A pruned problem transformation method for multi-label classification}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84880106608{\&}partnerID=40{\&}md5=ccdd19f7d0f111a2fdda8df7af0a8075},
year = {2008}
}
@article{Weston2010,
abstract = {Image annotation datasets are becoming larger and larger, with tens of millions of images and tens of thousands of possible anno-tations. We propose a strongly performing method that scales to such datasets by simultaneously learning to optimize precision at k of the ranked list of annotations for a given image and learning a low-dimensional joint embedding space for both images and annotations. Our method both outperforms several baseline methods and, in comparison to them, is faster and consumes less memory. We also demonstrate how our method learns an interpretable model, where annotations with alternate spellings or even languages are close in the embedding space. Hence, even when our model does not predict the exact annotation given by a human la-beler, it often predicts similar annotations, a fact that we try to quantify by measuring the newly introduced " sibling " precision metric, where our method also obtains excellent results.},
annote = {NULL},
author = {Weston, Jason and Bengio, Samy and Usunier, Nicolas},
file = {:home/jan/Documents/Mendeley Desktop/Weston, Bengio, Usunier/EcmlPkdd/Weston, Bengio, Usunier - Unknown - Large Scale Image Annotation Learning to Rank with Joint Word-Image Embeddings.pdf:pdf},
issn = {0885-6125},
journal = {Ecml/Pkdd},
keywords = {ranking},
number = {June},
pages = {1--26},
title = {{Web Scale Image Annotation : Learning to Rank with Joint Word-Image Embeddings Image Annotation}},
year = {2010}
}
@article{Wang,
abstract = {While deep convolutional neural networks (CNNs) have shown a great success in single-label image classification, it is important to note that real world images generally contain multiple labels, which could correspond to different objects, scenes, actions and attributes in an image. Traditional approaches to multi-label image classification learn independent classifiers for each category and employ ranking or thresholding on the classification results. These techniques, although working well, fail to explicitly exploit the label dependencies in an image. In this paper, we utilize recurrent neural networks (RNNs) to address this problem. Combined with CNNs, the proposed CNN-RNN framework learns a joint image-label embedding to characterize the semantic label dependency as well as the image-label relevance, and it can be trained end-to-end from scratch to integrate both information in a unified framework. Experimental results on public benchmark datasets demonstrate that the proposed architecture achieves better performance than the state-of-the-art multi-label classification model},
archivePrefix = {arXiv},
arxivId = {1604.04573},
author = {Jiang, Wang},
doi = {10.1109/CVPR.2016.251},
eprint = {1604.04573},
file = {:home/jan/Documents/Mendeley Desktop/Wang et al/Unknown/Wang et al. - Unknown - CNN-RNN A Unified Framework for Multi-label Image Classification.pdf:pdf},
isbn = {9781467388511},
issn = {10636919},
journal = {Cvpr 2016},
pages = {2285--2294},
title = {{CNN-RNN : A Unified Framework for Multi-label Image Classification}},
url = {http://www.cv-foundation.org/openaccess/content{\_}cvpr{\_}2016/papers/Wang{\_}CNN-RNN{\_}A{\_}Unified{\_}CVPR{\_}2016{\_}paper.pdf},
year = {2016}
}
@article{DeComite2003,
abstract = {Multi-label decision procedures are the target of the supervised learning algorithm we propose in this paper. Multi-label decision procedures map examples to a finite set of labels. Our learning algorithm extends Schapire and Singer's Adaboost.MH and produces sets of rules that can be viewed as trees like Alternating Decision Trees (invented by Freund and Mason). Experiments show that we take advantage of both performance and readability using boosting techniques as well as tree representations of large set of rules. Moreover, a key feature of our algorithm is the ability to handle heterogenous input data: discrete and continuous values and text data.},
annote = {NULL},
author = {Comite, Francesco De and Gilleron, Remi and Tommasi, Marc},
file = {:home/jan/Documents/Mendeley Desktop/Comite, Gilleron, Tommasi/Third International Conference Machine Learning and Data Mining in Pattern Recognition/De Comite, Gilleron, Tommasi - 2003 - Learning Multi-label Alternating Decision Trees from Texts and Data.pdf:pdf},
isbn = {978-3-540-40504-7},
issn = {03029743},
journal = {Third International Conference Machine Learning and Data Mining in Pattern Recognition},
keywords = {alternating decision trees,boosting,multi-label,text mining},
pages = {35--49},
publisher = {Springer},
title = {{Learning multi-label alternating decision trees from texts and data}},
url = {https://hal.inria.fr/inria-00536733},
year = {2003}
}
@article{Tsoumakas2007b,
abstract = {This paper proposes an ensemble method for multilabel clas- sification. TheRAndomk-labELsets (RAKEL) algorithm constructs each member of the ensemble by considering a small randomsubset of labels and learning a single-label classifier for the prediction of each element in the powerset of this subset. In this way, the proposed algorithm aims to take into account label correlations using single-label classifiers that are applied on subtasks with manageable number of labels and adequate number of examples per label. Experimental results on common multilabel domains involving protein, document and scene classification showthat better per- formance can be achieved compared to popular multilabel classification approaches.},
annote = {NULL},
author = {Tsoumakas, Grigorios and Vlahavas, Ioannis},
doi = {10.1007/978-3-540-74958-5_38},
file = {:home/jan/Documents/Mendeley Desktop/Tsoumakas, Vlahavas/Unknown/Tsoumakas, Vlahavas - 2007 - Random k-labelsets An Ensemble Method for Multilabel Classification.pdf:pdf},
isbn = {6463501476},
issn = {01681605},
journal = {European Conference on Machine Learning},
pages = {406--417},
pmid = {11518430},
title = {{Random k-labelsets: An Ensemble Method for Multilabel Classification}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-74958-5{\_}38},
year = {2007}
}
@article{Lee2017,
abstract = {Multi-label feature selection involves the selection of relevant features from multi-labeled datasets, resulting in a potential improvement of multi-label learning accuracy. In conventional multi-label feature selection methods, the fi nal feature subset is obtained by identifying the features of high relevance with low redundancy. Thus, accurate score evaluation is a key factor for obtaining an e ff ective feature subset. However, conventional methods su ff er from inaccurate conditional relevance evaluation when a large number of labels are involved. As a result, irrelevant features can be a member of the fi nal feature subset, leading to low multi-label learning accuracy. In this paper, we propose a new multi-label feature selection method. Using a scalable relevance evaluation process that evaluates conditional relevance more accurately, the proposed method signi fi cantly improves multi-label learning accuracy compared with conventional multi-label feature selection methods.},
author = {Lee, Jaesung and Kim, Dae-Won},
doi = {10.1016/j.patcog.2017.01.014},
file = {:home/jan/Documents/Mendeley Desktop/Lee, Kim/Pattern Recognition/Lee, Kim - Unknown - SCLS Multi-label feature selection based on scalable criterion for large label set.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
title = {{SCLS: Multi-label feature selection based on scalable criterion for large label set}},
url = {http://ac.els-cdn.com.ez.sun.ac.za/S003132031730016X/1-s2.0-S003132031730016X-main.pdf?{\_}tid=72e6d1d6-1573-11e7-a49b-00000aacb361{\&}acdnat=1490897305{\_}4ff82d83a1296a46c537536304a7e929 http://linkinghub.elsevier.com/retrieve/pii/S003132031730016X},
year = {2017}
}
@article{Rokach,
abstract = {Ensemble methods have been shown to be an effective tool for solving multi-label classification tasks. In the RAndom k-labELsets (RAKEL) algorithm, each member of the ensemble is associated with a small randomly-selected subset of k labels. Then, a single label classifier is trained according to each combi-nation of elements in the subset. In this paper we adopt a similar approach, however, instead of randomly choosing subsets, we select the minimum required subsets of k labels that cover all labels and meet additional constraints such as coverage of inter-label correlations. Construction of the cover is achieved by formulating the subset selection as a minimum set covering problem (SCP) and solving it by using approximation algo-rithms. Every cover needs only to be prepared once by offline algorithms. Once prepared, a cover may be applied to the classification of any given multi-label dataset whose properties conform with those of the cov-er. The contribution of this paper is two-fold. First, we introduce SCP as a general framework for construct-ing label covers while allowing the user to incorporate cover construction constraints. We demonstrate the effectiveness of this framework by proposing two construction constraints whose enforcement produces co-vers that improve the prediction performance of random selection. Second, we provide theoretical bounds that quantify the probabilities of random selection to produce covers that meet the proposed construction cri-teria. The experimental results indicate that the proposed methods improve multi-label classification accura-cy and stability compared with the RAKEL algorithm and to other state-of-the-art algorithms.},
author = {Systems, Expert and Aviv-yafo, Tel},
doi = {10.1016/j.eswa.2014.06.015},
file = {:home/jan/Documents/Mendeley Desktop/Systems, Aviv-yafo/Unknown/Rokach, Schclar, Itach - Unknown - Ensemble Methods for Multi-label Classification.pdf:pdf},
number = {July 2013},
title = {{Ensemble Methods for Multi-label Classification Ensemble Methods for Multi-label Classification}},
year = {2014}
}
@article{Wang2008,
abstract = {Transductive video concept detection is an effective way to handle the lack of sufficient labeled videos. However, another issue, the multi-label interdependence, is not es-sentially addressed in the existing transductive methods. Most solutions only applied the transductive single-label ap-proach to detect each individual concept separately, but ig-noring the concept relation, or simply imposed the smooth-ness assumption over the multiple labels for each video, without indeed exploring the interdependence between the concepts. On the other hand, the semi-supervised exten-sion of supervised multi-label classifiers, such as correlative multi-label support vector machines, is usually intractable and hence impractical due to the quite expensive compu-tational cost. In this paper, we propose an effective trans-ductive multi-label classification approach, which simultane-ously models the labeling consistency between the visually similar videos and the multi-label interdependence for each video in an integrated framework. We compare the per-formance between the proposed approach and several rep-resentative transductive single-label and supervised multi-label classification approaches for the video concept detec-tion task over the widely-used TRECVID data set. The comparative results demonstrate the superiority of the pro-posed approach.},
annote = {NULL},
author = {Wang, Jingdong and Zhao, Yinghai and Wu, Xiuqing and Hua, Xian-Sheng},
doi = {10.1145/1460096.1460145},
file = {:home/jan/Documents/Mendeley Desktop/Wang et al/Proceeding of the 1st ACM international conference on Multimedia information retrieval - MIR '08/Wang et al. - Unknown - Transductive Multi-Label Learning for Video Concept Detection.pdf:pdf},
isbn = {9781605583129},
journal = {Proceeding of the 1st ACM international conference on Multimedia information retrieval - MIR '08},
keywords = {multi-label,transductive learning,video concept detection},
pages = {298},
title = {{Transductive multi-label learning for video concept detection}},
url = {https://pdfs.semanticscholar.org/c1d3/4d048074fa5b6f390715e37e822e0a5d9f25.pdf http://portal.acm.org/citation.cfm?doid=1460096.1460145},
year = {2008}
}
@article{Dimou2009,
abstract = {This paper presents an experimental comparison of dif- ferent approaches to learning from multi-labeled video data. We compare state-of-the-art multi-label learning methods on the Mediamill Challenge dataset. We employ MPEG-7 and SIFT-based global image descriptors independently and in conjunction using variations of the stacking approach for their fusion. We evaluate the results comparing the different classifiers using both MPEG-7 and SIFT-based descriptors and their fusion. A variety of multi-label evaluation mea- sures is used to explore advantages and disadvantages of the examined classifiers. Results give rise to interesting conclusions.},
annote = {NULL},
author = {Dimou, Anastasios and Tsoumakas, Grigorios and Mezaris, Vasileios and Kompatsiaris, Ioannis and Vlahavas, L},
file = {:home/jan/Documents/Mendeley Desktop/Dimou et al/Content-Based Multimedia Indexing, 2009. CBMI'09. Seventh International Workshop on/Dimou et al. - 2009 - An Empirical Study of Multi-Label Learning Methods for Video Annotation.pdf:pdf},
isbn = {1424442656},
journal = {Content-Based Multimedia Indexing, 2009. CBMI'09. Seventh International Workshop on},
number = {June},
pages = {19--24},
title = {{An empirical study of multi-label learning methods for video annotation}},
url = {http://lpis.csd.auth.gr/publications/tsoumakas-cbmi09.pdf},
year = {2009}
}
@article{Clare2001,
abstract = {The biological sciences are undergoing an explosion in the amount of available data. New data analysis methods are needed to deal with the data. We present work using KDD to analyse data from mutant phenotype growth experiments with the yeast S. cerevisiae to predict novel gene functions. The analysis of the data presented a number of challenges: multi-class labels, a large number of sparsely populated classes, the need to learn a set of accurate rules (not a complete classification), and a very large amount of missing values. We developed resampling strategies and modified the algorithm C4.5 to deal with these problems. Rules were learnt which are accurate and biologically meaningful. The rules predict function of 83 putative genes of currently unknown function at an estimated accuracy of {\textgreater} 80{\%}.},
author = {Clare, Amanda and King, Ross D.},
doi = {10.1007/3-540-44794-6_4},
file = {:home/jan/Documents/Mendeley Desktop/Clare, King/Pkdd/Clare, King - Unknown - Knowledge Discovery in Multi-Label Phenotype Data.pdf:pdf},
isbn = {3-540-42534-9},
issn = {16113349},
journal = {Pkdd},
pages = {42--53},
title = {{Knowledge Discovery in Multi-label Phenotype Data}},
url = {http://www.springerlink.com/index/10.1007/3-540-44794-6},
volume = {2168},
year = {2001}
}
@article{Tawiah2013,
abstract = {Multi-label classifications exist in many real world applications. This paper empirically studies the performance of a variety of multi-label classification algorithms. Some of them are developed based on problem transformation. Some of them are developed based on adaption. Our experimental results show that the adaptive Multi-Label K-Nearest Neighbor performs the best, followed by Random k-Label Set, followed by Classifier Chain and Binary Relevance. Adaboost.MH performs the worst, followed by Pruned Problem Transformation. Our experimental results also provide us the confidence of the correlations among multilabels. These insights shed light for future research directions on multi-label classifications.},
author = {Tawiah, Ca and Sheng, Vs},
isbn = {9781424427659},
journal = {Proceedings of the 27th AAAI Conference on {\ldots}},
keywords = {Student Abstract and Poster Program},
pages = {1645--1646},
title = {{Empirical Comparison of Multi-Label Classification Algorithms}},
url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI13/paper/download/6170/6833},
year = {2013}
}
@article{Zhang2007,
abstract = {Multi-label learning originated from the investigation of text categorization problem, where each document may belong to several predefined topics simultaneously. In multi-label learning, the training set is composed of instances each associated with a set of labels, and the task is to predict the label sets of unseen instances through analyzing training instances with known label sets. In this paper, a multi-label lazy learning approach named ML-KNN is presented, which is derived from the traditional K-nearest neighbor (KNN) algorithm. In detail, for each unseen instance, its K nearest neighbors in the training set are firstly identified. After that, based on statistical information gained from the label sets of these neighboring instances, i.e. the number of neighboring instances belonging to each possible class, maximum a posteriori (MAP) principle is utilized to determine the label set for the unseen instance. Experiments on three different real-world multi-label learning problems, i.e. Yeast gene functional analysis, natural scene classification and automatic web page categorization, show that ML-KNN achieves superior performance to some well-established multi-label learning algorithms.},
author = {Zhang, Min-Ling and Zhou, Zhi-Hua},
doi = {10.1016/j.patcog.2006.12.019},
file = {:home/jan/Documents/Mendeley Desktop/Zhang, Zhou/Pattern Recognition/Zhang, Zhou - 2007 - ML-KNN A lazy learning approach to multi-label learning.pdf:pdf},
journal = {Pattern Recognition},
keywords = {Functional genomics,K-nearest neighbor,Lazy learning,Machine learning,Multi-label learning,Natural scene classification,Text categorization},
pages = {2038--2048},
title = {{ML-KNN: A lazy learning approach to multi-label learning}},
url = {www.elsevier.com/locate/pr},
volume = {40},
year = {2007}
}
@article{Liu2015,
abstract = {A multi-label classification based approach for sentiment analysis is proposed in this paper. To the best of our knowledge, this work is the first to propose to use multi-label classification for sentiment classification of microblogs. The proposed prototype has three main components, text segmentation, feature extraction, and multi-label classification. Raw segmented words and sentiment features based on the three different sentiment dictionaries, Dalian University of Technology Sentiment Dictionary, National Taiwan University Sentiment Dictionary and HowNet Dictionary, are the features and the bag of words is the feature representation. A detailed empirical study of different multi-label classification methods on sentiment classification is conducted to compare their classification performances. Specifically, total 11 state of the art multi-label classification methods are compared on two microblog datasets and 8 evaluation metrics are used. The effects of the three sentiment dictionaries for multi-label classification are empirically studied and compared, which, to the best of our knowledge, have not been performed. The performed empirical comparisons show that Dalian University of Technology Sentiment Dictionary has the best performance among the three different sentiment dictionaries.},
author = {Liu, Shuhua Monica and Chen, Jiun-Hung},
doi = {10.1016/j.eswa.2014.08.036},
file = {:home/jan/Documents/Mendeley Desktop/Liu, Chen/Expert Systems with Applications/Liu, Chen - 2015 - A multi-label classification based approach for sentiment classification.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Microblogs,Multi-Label,Multi-label classification,Sentiment Classification,Sentiment analysis},
mendeley-tags = {Multi-Label,Sentiment Classification},
number = {3},
pages = {1083--1093},
publisher = {Elsevier Ltd},
title = {{A multi-label classification based approach for sentiment classification}},
url = {http://www.sciencedirect.com/science/article/pii/S0957417414005181},
volume = {42},
year = {2015}
}
@article{Tsoumakasd,
abstract = {This paper contributes a novel algorithm for effective and computationally efficient multilabel classification in domains with large label sets L. The HOMER algorithm constructs a Hierarchy Of Multilabel classifiERs, each one dealing with a much smaller set of labels compared to L and a more balanced example distribution. This leads to improved predictive performance along with linear training and logarithmic testing complexities with respect to |L|. Label distribution from parent to children nodes is achieved via a new balanced clustering algorithm, called balanced k means.},
author = {Tsoumakas, Grigorios and Katakis, Ioannis and Vlahavas, Ioannis},
file = {:home/jan/Documents/Mendeley Desktop/Tsoumakas, Katakis, Vlahavas/Proc. ECMLPKDD 2008 Workshop on Mining Multidimensional Data (MMD'08)/Tsoumakas, Katakis, Vlahavas - 2008 - Effective and efficient multilabel classification in domains with large number of labels.pdf:pdf},
journal = {Proc. ECML/PKDD 2008 Workshop on Mining Multidimensional Data (MMD'08)},
pages = {30--44},
title = {{Effective and efficient multilabel classification in domains with large number of labels}},
url = {http://lpis.csd.auth.gr/publications/tsoumakas-mmd08.pdf},
year = {2008}
}
@article{Zhou2011,
abstract = {Multi-label learning has attracted much attention during the past few years. Many multilabel learning approaches have been developed, mostly working with surrogate loss functions since multi-label loss functions are usually difficult to optimize directly owing to non-convexity and discontinuity. Though these approaches are effective, to the best of our knowledge, there is no theoretical result on the convergence of risk of the learned functions to the Bayes risk. In this paper, focusing on two well-known multi-label loss functions, i.e., ranking loss and hamming loss, we prove a necessary and sufficient condition for the consistency of multi-label learning based on surrogate loss functions. Our results disclose that, surprisingly, none convex surrogate loss is consistent with the ranking loss. Inspired by the finding, we introduce the partial ranking loss, with which some surrogate functions are consistent. For hamming loss, we show that some recent multi-label learning approaches are inconsistent even for deterministic multi-label classification, and give a surrogate loss function which is consistent for the deterministic case. Finally, we discuss on the consistency of learning approaches which address multi-label learning by decomposing into a set of binary classification problems},
archivePrefix = {arXiv},
arxivId = {1204.1688},
author = {Gao, Wei and Zhou, Zhi-Hua},
doi = {10.1214/13-AOS1142},
eprint = {1204.1688},
file = {:home/jan/Documents/Mendeley Desktop/Gao, Zhou/Annals of Statistics/Gao, Zhou - 2011 - On the Consistency of Multi-Label Learning.pdf:pdf},
isbn = {9781605589077},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Consistency,hamming loss,multi-label learning,ranking loss,surrogate loss},
pmid = {14121148},
title = {{On the Consistency of Multi-Label Learning}},
year = {2011}
}
@inproceedings{Dembczyski,
abstract = {In multi-label classification (MLC), each instance is associated with$\backslash$na subset of labels instead of a single class, as in conventional$\backslash$nclassification, and this generalization enables the definition of$\backslash$na multitude of loss functions. Indeed, a large number of losses has$\backslash$nalready been proposed and is commonly applied as performance metrics$\backslash$nin experimental studies. However, even though these loss functions$\backslash$nare of a quite different nature, a concrete connection between the$\backslash$ntype of multi-label classifier used and the loss to be minimized$\backslash$nis rarely established, implicitly giving the misleading impression$\backslash$nthat the same method can be optimal for different loss functions.$\backslash$nIn this paper, we elaborate on risk minimization and the connection$\backslash$nbetween loss functions in MLC, both theoretically and empirically.$\backslash$nIn particular, we compare two important loss functions, namely the$\backslash$nHamming loss and the subset 0/1 loss. We perform a regret analysis,$\backslash$nshowing how poor a classifier intended to minimize the subset 0/1$\backslash$nloss can become in terms of Hamming loss and vice versa. The theoretical$\backslash$nresults are corroborated by experimental studies, and their implications$\backslash$nfor MLC methods are discussed in a broader context. 漏 2010 Springer-Verlag$\backslash$nBerlin Heidelberg.},
author = {Dembczy{\'{n}}ski, Krzysztof and Waegeman, Willem and Cheng, Weiwei and H{\"{u}}llermeier, Eyke},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-15880-3_24},
file = {:home/jan/Documents/Mendeley Desktop/Dembczy{\'{n}}ski et al/Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)/Dembczyski et al. - Unknown - Regret Analysis for Performance Metrics in Multi-Label Classification The Case of Hamming and Subset Zero-.pdf:pdf},
isbn = {364215879X},
issn = {03029743},
number = {PART 1},
pages = {280--295},
title = {{Regret analysis for performance metrics in multi-label classification: The case of hamming and subset zero-one loss}},
url = {https://biblio.ugent.be/publication/1155381/file/1210780.pdf},
volume = {6321 LNAI},
year = {2010}
}
@article{Dembcz2012,
abstract = {Most of the multi-label classification (MLC) methods proposed in recent years intended to exploit, in one way or the other, dependencies between the class labels. Compar-ing to simple binary relevance learning as a baseline, any gain in performance is normally explained by the fact that this method is ignoring such dependencies. Without questioning the correctness of such studies, one has to admit that a blanket explanation of that kind is hiding many subtle details, and indeed, the underlying mechanisms and true reasons for the improvements reported in experimental studies are rarely laid bare. Rather than propos-ing yet another MLC algorithm, the aim of this paper is to elaborate more closely on the idea of exploiting label dependence, thereby contributing to a better understanding of MLC. Adopting a statistical perspective, we claim that two types of label dependence should be distinguished, namely conditional and marginal dependence. Subsequently, we present three scenarios in which the exploitation of one of these types of dependence may boost the pre-dictive performance of a classifier. In this regard, a close connection with loss minimization is established, showing that the benefit of exploiting label dependence does also depend on the type of loss to be minimized. Concrete theoretical results are presented for two repre-Editors: Mach Learn (2012) 88:5–45 sentative loss functions, namely the Hamming loss and the subset 0/1 loss. In addition, we give an overview of state-of-the-art decomposition algorithms for MLC and we try to re-veal the reasons for their effectiveness. Our conclusions are supported by carefully designed experiments on synthetic and benchmark data.},
author = {Dembcz, Krzysztof and Waegeman, Willem and Cheng, Weiwei and H{\"{u}}llermeier, Eyke and Tsoumakas, Grigorios and Zhang, Min-Ling and Zhou, Zhi-Hua and Dembczyski, K and Waegeman, W and Cheng, W and H{\"{u}}llermeier, E},
doi = {10.1007/s10994-012-5285-8},
file = {:home/jan/Documents/Mendeley Desktop/Dembcz et al/Mach Learn/Dembcz et al. - 2012 - On label dependence and loss minimization in multi-label classification.pdf:pdf},
journal = {Mach Learn},
keywords = {Label dependence {\textperiodcentered},Loss functions,Multi-label classification {\textperiodcentered}},
pages = {5--45},
title = {{On label dependence and loss minimization in multi-label classification}},
volume = {88},
year = {2012}
}
@article{Zhang2014,
abstract = {Multi-label learning studies the problem where each example is represented by a single instance while associated with a set of labels simultaneously. During the past decade, significant amount of progresses have been made toward this emerging machine learning paradigm. This paper aims to provide a timely review on this area with emphasis on state-of-the-art multi-label learning algorithms. Firstly, fundamentals on multi-label learning including formal definition and evaluation metrics are given. Secondly and primarily, eight representative multi-label learning algorithms are scrutinized under common notations with relevant analyses and discussions. Thirdly, several related learning settings are briefly summarized. As a conclusion, online resources and open research problems on multi-label learning are outlined for reference purposes.},
author = {Zhang, Min Ling and Zhou, Zhi Hua},
doi = {10.1109/TKDE.2013.39},
file = {:home/jan/Documents/Mendeley Desktop/Zhang, Zhou/IEEE Transactions on Knowledge and Data Engineering/Zhang, Zhou - 2014 - A review on multi-label learning algorithms.pdf:pdf},
isbn = {1041-4347},
issn = {10414347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Multi-label learning,algorithm adaptation,label correlations-problem transformation},
number = {8},
pages = {1819--1837},
title = {{A review on multi-label learning algorithms}},
volume = {26},
year = {2014}
}
@article{Yu2014,
abstract = {Nowadays, multi-label classification methods are of increasing interest in the areas such as text categorization, image annotation and protein function classification. Due to the correlation among the labels, traditional single-label classification methods are not directly applicable to the multi-label classification problem. This paper presents two novel multi-label classification algorithms based on the variable precision neighborhood rough sets, called multi-label classification using rough sets (MLRS) and MLRS using local correlation (MLRS-LC). The proposed algorithms consider two important factors that affect the accuracy of prediction, namely the correlation among the labels and the uncertainty that exists within the mapping between the feature space and the label space. MLRS provides a global view at the label correlation while MLRS-LC deals with the label correlation at the local level. Given a new instance, MLRS determines its location and then computes the probabilities of labels according to its location. The MLRS-LC first finds out its topic and then the probabilities of new instance belonging to each class is calculated in related topic. A series of experiments reported for seven multi-label datasets show that MLRS and MLRS-LC achieve promising performance when compared with some well-known multi-label learning algorithms. ?? 2013 Elsevier Ltd. All rights reserved.},
author = {Yu, Ying and Pedrycz, Witold and Miao, Duoqian},
doi = {10.1016/j.eswa.2013.10.030},
file = {:home/jan/Documents/Mendeley Desktop/Yu, Pedrycz, Miao/Expert Systems with Applications/Yu, Pedrycz, Miao - 2014 - Multi-label classification by exploiting label correlations.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Correlation,Multi-label classification,Rough sets,Uncertainty},
number = {6},
pages = {2989--3004},
title = {{Multi-label classification by exploiting label correlations}},
volume = {41},
year = {2014}
}
@article{Spolaor2013,
abstract = {Feature selection is an important task in machine learning, which can effectively reduce the dataset dimensionality by removing irrelevant and/or redundant features. Although a large body of research deals with feature selection in single-label data, in which measures have been proposed to filter out irrelevant features, this is not the case for multi-label data. This work proposes multi-label feature selection methods which use the filter approach. To this end, two standard multi-label feature selection approaches, which transform the multi-label data into single-label data, are used. Besides these two problem transformation approaches, we use ReliefF and Information Gain to measure the goodness of features. This gives rise to four multi-label feature selection methods. A thorough experimental evaluation of these methods was carried out on 10 benchmark datasets. Results show that ReliefF is able to select fewer features without diminishing the quality of the classifiers constructed using the features selected. {\textcopyright} 2013 Elsevier B.V.},
annote = {NULL},
author = {Spola{\^{o}}r, Newton and Cherman, Everton Alvares and Monard, Maria Carolina and Lee, Huei Diana},
doi = {10.1016/j.entcs.2013.02.010},
file = {:home/jan/Documents/Mendeley Desktop/Spola{\^{o}}r et al/Electronic Notes in Theoretical Computer Science/Spola{\^{o}}r et al. - 2013 - A Comparison of Multi-label Feature Selection Methods using the Problem Transformation Approach.pdf:pdf},
isbn = {9783319259383},
issn = {15710661},
journal = {Electronic Notes in Theoretical Computer Science},
keywords = {Feature Ranking,Information Gain,Multi-label learning,ReliefF},
pages = {135--151},
title = {{A comparison of multi-label feature selection methods using the problem transformation approach}},
volume = {292},
year = {2013}
}
@article{Schapire2000a,
abstract = {This work focuses on algorithms which learn from examples to perform multiclass text and speech categorization tasks. Our approach is based on a new and improved family of boosting algorithms. We describe in detail an implementation, called BoosTexter, of the new boosting algorithms for text categorization tasks. We present results comparing the performance of BoosTexter and a number of other text-categorization algorithms on a variety of tasks. We conclude by describing the application of our system to automatic call-type identification from unconstrained spoken customer responses.},
annote = {NULL},
author = {Schapire, Robert E and Singer, Y},
file = {:home/jan/Documents/Mendeley Desktop/Schapire, Singer/Machine learning/Schapire, Singer - 2000 - BoosTexter A boosting- based system for text categorization.pdf:pdf},
journal = {Machine learning},
keywords = {boosting algorithms,multiclass classification problems,text and speech categorization},
number = {23},
pages = {135--168},
title = {{BoosTexter: A boosting- based system for text categorization}},
url = {http://link.springer.com/article/10.1023/A:1007649029923},
volume = {39},
year = {2000}
}
@article{Charte2015,
abstract = {The purpose of this paper is to analyze the imbalanced learning task in the multilabel scenario, aiming to accomplish two different goals. The first one is to present specialized measures directed to assess the imbalance level in multilabel datasets (MLDs). Using these measures we will be able to conclude which MLDs are imbalanced, and therefore would need an appropriate treatment. The second objective is to propose several algorithms designed to reduce the imbalance in MLDs in a classifier-independent way, by means of resampling techniques. Two different approaches to divide the instances in minority and majority groups are studied. One of them considers each label combination as class identifier, whereas the other one performs an individual evaluation of each label imbalance level. A random undersampling and a random oversampling algorithm are proposed for each approach, giving as result four different algorithms. All of them are experimentally tested and their effectiveness is statistically evaluated. From the results obtained, a set of guidelines directed to show when these methods should be applied is also provided.},
author = {Charte, Francisco and Rivera, Antonio J and del Jesus, Mar{\'{i}}a J. and Herrera, Francisco},
doi = {10.1016/j.neucom.2014.08.091},
file = {:home/jan/Documents/Mendeley Desktop/Charte et al/Neurocomputing/Charte et al. - 2015 - Addressing imbalance in multilabel classification Measures and random resampling algorithms.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Imbalanced classification,Multilabel classification,Oversampling,Resampling algorithms,Undersampling,imbalanced classi fi cation,multilabel classi fi cation,resampling algorithms},
pages = {1--14},
title = {{Addressing imbalance in multilabel classification: Measures and random resampling algorithms}},
url = {http://sci2s.ugr.es/sites/default/files/ficherosPublicaciones/1790{\_}2015-Neuro-Charte-MultiLabel{\_}Imbalanced.pdf http://linkinghub.elsevier.com/retrieve/pii/S0925231215004269},
volume = {163},
year = {2015}
}
@article{Madjarov2011,
abstract = {See, stats, and : https : / / www . researchgate . net / publication / 232630058 Two - label Article DOI : 10 . 1016 / j . patcog . 2011 . 08 . 011 : DBLP CITATIONS 13 READS 99 3 , including : Gjorgji Ss . Cyril 31 SEE Dejan Ss . Cyril 57 SEE All . The . All - text and , letting . Abstract A common approach to solving multi - label learning problems is to use problem transformation methods and dichotomizing classifiers as in the pair - wise decomposition strategy . One of the problems with this strategy is the need for querying a quadratic number of binary classifiers for making a prediction that can be quite time consuming , especially in learning problems with a large number of labels . To tackle this problem , we propose a Two Stage Architecture (TSA) for efficient multi - label learning . We analyze three implementations of this architecture the Two Stage Voting Method (TSVM) , the Two Stage Classifier Chain Method (TSCCM) and the Two Stage Pruned Classifier Chain Method (TSPCCM) . Eight different real - world datasets are used to evaluate the performance of the proposed methods . The performance of our approaches is compared with the performance of two algorithm adaptation methods (Multi - Label k - NN and Multi - Label C4 . 5) and five problem transformation methods (Binary Relevance , Classifier Chain , Calibrated Label Ranking with majority voting , the Quick Weighted method for pair - wise multi - label learning and the Label Powerset method) . The results suggest that TSCCM and TSPCCM outperform the competing algorithms in terms of predictive accuracy , while TSVM has comparable predictive performance . In terms of testing speed , all three methods show better performance as compared to the pair - wise methods for multi - label learning .},
author = {Madjarov, Gjorgji and Gjorgjevikj, Dejan and D{\v{z}}eroski, Sa{\v{s}}o},
doi = {10.1016/j.patcog.2011.08.011},
file = {:home/jan/Documents/Mendeley Desktop/Madjarov, Gjorgjevikj, D{\v{z}}eroski/Unknown/Madjarov, Gjorgjevikj, D{\v{z}}eroski - 2011 - Two Stage Architecture for Multi - label Learning.pdf:pdf},
keywords = {classifier chain,multi - label classification,multi - label learning,multi - label ranking,two stage architecture},
pages = {1--24},
title = {{Two Stage Architecture for Multi - label Learning}},
volume = {00},
year = {2011}
}
@article{Alazaidah2015,
abstract = {—Multi label classification is concerned with learning from a set of instances that are associated with a set of labels, that is, an instance could be associated with multiple labels at the same time. This task occurs frequently in application areas like text categorization, multimedia classification, bioinformatics, protein function classification and semantic scene classification. Current multi-label classification methods could be divided into two categories. The first is called problem transformation methods, which transform multi-label classification problem into single label classification problem, and then apply any single label classifier to solve the problem. The second category is called algorithm adaptation methods, which adapt an existing single label classification algorithm to handle multi-label data. In this paper, we propose a multi-label classification approach based on correlations among labels that use both problem transformation methods and algorithm adaptation methods. The approach begins with transforming multi-label dataset into a single label dataset using least frequent label criteria, and then applies the PART algorithm on the transformed dataset. The output of the approach is multi-labels rules. The approach also tries to get benefit from positive correlations among labels using predictive Apriori algorithm. The proposed approach has been evaluated using two multi-label datasets named (Emotions and Yeast) and three evaluation measures (Accuracy, Hamming Loss, and Harmonic Mean). The experiments showed that the proposed approach has a fair accuracy in comparison to other related methods.},
author = {Alazaidah, Raed and Thabtah, Fadi and Al-Radaideh, Qasem},
file = {:home/jan/Documents/Mendeley Desktop/Alazaidah, Thabtah, Al-Radaideh/IJACSA) International Journal of Advanced Computer Science and Applications/Alazaidah, Thabtah, Al-Radaideh - 2015 - A Multi-Label Classification Approach Based on Correlations Among Labels.pdf:pdf},
journal = {IJACSA) International Journal of Advanced Computer Science and Applications},
keywords = {Data mining,Multi-label Classification,—Classification},
number = {2},
title = {{A Multi-Label Classification Approach Based on Correlations Among Labels}},
url = {www.ijacsa.thesai.org},
volume = {6},
year = {2015}
}
@article{Yang1999,
abstract = {This paper focuses on a comparative evaluation of a wide-range of text categorization methods, including previously published results on the Reuters corpus and new results of additional experiments. A controlled study using three classifiers, kNN, LLSF and WORD, was conducted to examine the impact of configuration variations in five versions of Reuters on the observed performance of classifiers. Analysis and empirical evidence suggest that the evaluation results on some versions of Reuters were significantly affected by the inclusion of a large portion of unlabelled documents, mading those results difficult to interpret and leading to considerable confusions in the literature. Using the results evaluated on the other versions of Reuters which exclude the unlabelled documents, the performance of twelve methods are compared directly or indirectly. For indirect compararions, kNN, LLSF and WORD were used as baselines, since they were evaluated on all versions of Reuters that exclude the unlabelled documents. As a global observation, kNN, LLSF and a neural network method had the best performance; except for a Naive Bayes approach, the other learning algorithms also performed relatively well.},
author = {Yang, Yiming},
doi = {10.1023/A:1009982220290},
file = {:home/jan/Documents/Mendeley Desktop/Yang/Information Retrieval/Yang - 1999 - An Evaluation of Statistical Approaches to Text Categorization.pdf:pdf},
isbn = {1386-4564},
issn = {1612-1880},
journal = {Information Retrieval},
keywords = {comparative study,evaluation,statistical learning algorithms,text categorization},
number = {1},
pages = {69--90},
title = {{An Evaluation of Statistical Approaches to Text Categorization}},
url = {http://www.cs.cmu.edu/{~}{\%}5Cnhttp://dx.doi.org/10.1023/A:1009982220290},
volume = {1},
year = {1999}
}
@article{Sechidis,
abstract = {Stratified sampling is a sampling method that takes into account the existence of disjoint groups within a population and pro-duces samples where the proportion of these groups is maintained. In single-label classification tasks, groups are differentiated based on the value of the target variable. In multi-label learning tasks, however, where there are multiple target variables, it is not clear how stratified sam-pling could/should be performed. This paper investigates stratification in the multi-label data context. It considers two stratification methods for multi-label data and empirically compares them along with random sampling on a number of datasets and based on a number of evaluation criteria. The results reveal some interesting conclusions with respect to the utility of each method for particular types of multi-label datasets.},
author = {Sechidis, Konstantinos and Tsoumakas, Grigorios and Vlahavas, Ioannis},
file = {:home/jan/Documents/Mendeley Desktop/Sechidis, Tsoumakas, Vlahavas/Unknown/Sechidis, Tsoumakas, Vlahavas - Unknown - On the Stratification of Multi-Label Data.pdf:pdf},
title = {{On the Stratification of Multi-label Data}},
url = {http://download.springer.com/static/pdf/229/chp{\%}253A10.1007{\%}252F978-3-642-23808-6{\_}10.pdf?originUrl=http{\%}3A{\%}2F{\%}2Flink.springer.com{\%}2Fchapter{\%}2F10.1007{\%}2F978-3-642-23808-6{\_}10{\&}token2=exp=1489589222{~}acl={\%}2Fstatic{\%}2Fpdf{\%}2F229{\%}2Fchp{\%}25253A10.1007{\%}25252F978-3-64},
year = {2011}
}
@article{Readb,
abstract = {The widely known binary relevance method for multi-label classification, which considers each label as an independent binary problem, has often been overlooked in the literature due to the perceived inadequacy of not directly modelling label correlations. Most current methods invest considerable complexity to model interdependencies between labels. This paper shows that binary relevance-based methods have much to offer, and that high predictive performance can be obtained without impeding scalability to large datasets. We exemplify this with a novel classifier chains method that can model label correlations while maintaining acceptable computational complexity. We extend this approach further in an ensemble framework. An extensive empirical evaluation covers a broad range of multi-label datasets with a variety of evaluation metrics. The results illustrate the competitiveness of the chaining method against related and state-of-the-art methods, both in terms of predictive performance and time complexity.},
archivePrefix = {arXiv},
arxivId = {arXiv:1207.6324},
author = {Read, Jesse and Pfahringer, Bernhard and Holmes, Geoff and Frank, Eibe},
doi = {10.1007/s10994-011-5256-5},
eprint = {arXiv:1207.6324},
file = {:home/jan/Documents/Mendeley Desktop/Read et al/Machine Learning/Read et al. - 2011 - Classifier chains for multi-label classification(2).pdf:pdf},
isbn = {9783642041730},
issn = {08856125},
journal = {Machine Learning},
keywords = {Ensemble methods,Multi-label classification,Problem transformation,Scalable methods},
number = {3},
pages = {333--359},
pmid = {22183238},
title = {{Classifier chains for multi-label classification}},
volume = {85},
year = {2011}
}
@article{Tang2012,
abstract = {Videos have become an integral part of our life, from watching mov-ies online to the use of videos in classroom teaching. Existing machine learning techniques are constrained with this scaled up activity because of this huge up-surge in online activity. A lot of research is now focused on reducing the time and accuracy of video classification. Content-Based Video Information Retriev-al CBVIR implementation (E.g. Columbia374) is one such approach. We pro-pose a fast Hamming Selection Pruned Sets (HSPS) algorithm that efficiently transforms multi-label video dataset into single-label representation. Thus, mul-ti-label relationship between the labels can be retained for later single label classifier learning stage. Hamming distance (HD) is used to detect similarity be-tween label-sets. HSPS captures new potential label-set relationships that were previously undetected by baseline approach. Experiments show a significant 22.9{\%} dataset building time reduction and consistent accuracy improvement over the baseline method. HSPS also works on general multi-label dataset. 1 Introduction Multimedia information indexing and retrieval is an integral part of modern video search engines to describe, store and organize multimedia information and to assist people in finding multimedia information conveniently. Many research papers have been published on the area of Content-Based Video Information Retrieval (CBVIR) [1, 2]. CBVIR is an information retrieval research problem which involves machine learning over the video content (e.g. colors, shapes, textures, texts, motions and etc.). In this paper, our focus is on Video Semantic Concept Classification (VSCC) which is a sub-theme of CBVIR for detecting the correct semantic concept of a video shot. For example, let us consider a video shot containing keyframes of a car moving on a road. The detected semantic concepts for this video shot are car, weather and road. In fact this example shows that the video semantic concept classification natu-rally is a multi-label research problem. 1 Relationship between these labels can be exploited for enhancing classification accuracy as demonstrated by these research 1 Throughout this paper, we use multi-label instead of multi-concept for our research context.},
author = {Tang, Tiong Yew and Alhashmi, Saadat M and Jaward, Mohamed Hisham},
file = {:home/jan/Documents/Mendeley Desktop/Tang, Alhashmi, Jaward/PRICAI/Tang, Alhashmi, Jaward - 2012 - Hamming Selection Pruned Sets (HSPS) for Efficient Multi-label Video Classification.pdf:pdf},
journal = {PRICAI},
keywords = {Classification,Index Terms— Semantic Concept,Index and Retrieval,Label Combinations,Multi-Label,Pruned Sets,TRECVID,Videos},
number = {1},
title = {{Hamming Selection Pruned Sets (HSPS) for Efficient Multi-label Video Classification}},
url = {https://pdfs.semanticscholar.org/6d49/8611e1d30c5d1928573c199db02fc538d8d4.pdf?{\_}ga=1.107724480.379343330.1490351020},
year = {2012}
}
@article{Spolaor,
abstract = {{\textcopyright} 2015 Elsevier B.V.Each example in a multi-label dataset is associated with multiple labels, which are often correlated. Learning from this data can be improved when dimensionality reduction tasks, such as feature selection, are applied. The standard approach for multi-label feature selection transforms the multi-label dataset into single-label datasets before using traditional feature selection algorithms. However, this approach often ignores label dependence. In this work, we propose an alternative method, LCFS, that constructs new labels based on relations between the original labels. By doing so, the label set from the data is augmented with second-order information before applying the standard approach. To assess LCFS, an experimental evaluation using Information Gain as a measure to estimate the importance of features was carried out on 10 benchmark multi-label datasets. This evaluation compared four LCFS settings with the standard approach, using random feature selection as a reference. For each dataset, the performance of a feature selection method is estimated by the quality of the classifiers built from the data described by the features selected by the method. The results show that a simple LCFS setting gave rise to classifiers similar to, or better than, the ones built using the standard approach. Furthermore, this work also pioneers the use of the systematic review method to survey the related work on multi-label feature selection. The summary of the 99 papers found promotes the idea that exploring label dependence during feature selection can lead to good results.},
author = {Spola{\^{o}}r, N. and Monard, M.C. and Tsoumakas, G. and Lee, H.D.},
doi = {10.1016/j.neucom.2015.07.118},
file = {:home/jan/Documents/Mendeley Desktop/Spolaor et al/Unknown/Spolaor et al. - Unknown - A systematic review of multi-label feature selection and a new method based on label construction.pdf:pdf},
issn = {18728286 09252312},
journal = {Neurocomputing},
keywords = {Feature ranking,Filter feature,[Binary relevance},
title = {{A systematic review of multi-label feature selection and a new method based on label construction}},
url = {http://ac.els-cdn.com.ez.sun.ac.za/S0925231215016197/1-s2.0-S0925231215016197-main.pdf?{\_}tid=d82dcdb8-160b-11e7-87ff-00000aab0f26{\&}acdnat=1490962758{\_}6fd0abd75e28b70f59f2db5863d099a7},
volume = {180},
year = {2016}
}
@article{Lee2017a,
author = {Lee, Jaesung and Kim, Dae-Won},
doi = {10.1016/j.patcog.2017.01.014},
file = {:home/jan/Documents/Mendeley Desktop/Lee, Kim/Pattern Recognition/1-s2.0-S003132031730016X-main.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Conditional relevance,Machine learning,Multi-label feature selection,Multi-label learning,Relevance evaluation},
number = {August 2016},
pages = {342--352},
publisher = {Elsevier Ltd},
title = {{SCLS: Multi-label feature selection based on scalable criterion for large label set}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S003132031730016X},
volume = {66},
year = {2017}
}
@article{McCallum1999,
abstract = {In many important document classication tasks, documents may each be associated with multiple class labels. This paper describes a Bayesian classication approach in which the multiple classes that comprise a document are represented by a mixture model. While the labeled training data indicates which classes were responsible for generating a document, it does not indicate which class was responsible for generating each word. Thus we use EM to ll in this missing value, learning both the...},
author = {McCallum, Andrew},
doi = {10.1.1.35.888},
file = {:home/jan/Documents/Mendeley Desktop/McCallum/AAAI'99 Workshop on Text Learning/McCallum - 1999 - “Multi-label text classification with a mixture model trained by EM.pdf:pdf},
journal = {AAAI'99 Workshop on Text Learning},
pages = {1--7},
title = {{Multi-label text classification with a mixture model trained by EM}},
url = {http://www.kyriakides.net/CBCL/references/Papers/mccallum99multilabel.pdf},
year = {1999}
}
@article{Bhowmick2009,
abstract = {Multiple emotions are often triggered in readers in response to text stimuli like news article. In this paper, we present a novel method for classifying news sentences into multiple emotion categories using an ensemble based multi-label classification technique called RAKEL. The emotion data consists of 1305 news sentences and the emotion classes considered are disgust, fear, happiness and sadness. Words are the most obvious choice as feature for emotion recognition. In addition to that we have introduced two novel feature sets: polarity of subject, verb and object of the sentences and semantic frames. Experiments concerning the comparison of features revealed that semantic frame feature combined with polarity based feature performs best in emotion classification. Experiments on feature selection over word and semantic frame features have been performed in order to handle feature sparseness problem. In both word and semantic frame feature, improvements in the overall performance have been observed after optimal feature selection.},
author = {Bhowmick, Plaban Kumar and Basu, Anupam and Mitra, Pabitra},
file = {:home/jan/Documents/Mendeley Desktop/Bhowmick, Basu, Mitra/Computer and Information Science/Bhowmick, Basu, Mitra - 2009 - Reader Perspective Emotion Analysis in Text through Ensemble based Multi-Label Classification Framework.pdf:pdf},
issn = {19138997},
journal = {Computer and Information Science},
keywords = {Emotion Analysis,Multi-Label,emotion classification,ensemble classifier,feature selection,multi-label classification},
mendeley-tags = {Emotion Analysis,Multi-Label},
number = {4},
pages = {64--74},
title = {{Reader Perspective Emotion Analysis in Text through Ensemble based Multi-Label Classification Framework}},
url = {http://ccsenet.org/journal/index.php/cis/article/view/3872/0},
volume = {2},
year = {2009}
}
@article{Diplaris2005,
abstract = {Nowadays, the number of protein sequences being stored in central protein databases from labs all over the world is constantly increasing. From these proteins only a fraction has been experimentally analyzed in order to de- tect their structure and hence their function in the corresponding organism. The reason is that experimental determination of structure is labor-intensive and quite time-consuming. Therefore there is the need for automated tools that can classify new proteins to structural families. This paper presents a comparative evaluation of several algorithms that learn such classification models from data concerning patterns of proteins with known structure. In addition, several ap- proaches that combine multiple learning algorithms to increase the accuracy of predictions are evaluated. The results of the experiments provide insights that can help biologists and computer scientists design high-performance protein classification systems of high quality.},
author = {Diplaris, Sotiris and Tsoumakas, Grigorios and Mitkas, Pericles A. and Vlahavas, Ioannis},
doi = {10.1007/11573036_42},
file = {:home/jan/Documents/Mendeley Desktop/Diplaris et al/Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)/Diplaris et al. - 2005 - Protein classification with multiple algorithms.pdf:pdf},
isbn = {3540296735},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {448--456},
title = {{Protein classification with multiple algorithms}},
volume = {3746 LNCS},
year = {2005}
}
@article{Hsu2009,
abstract = {We consider multi-label prediction problems with large output spaces under the assumption of output sparsity – that the target (label) vectors have small support. We develop a general theory for a variant of the popular error correcting output code scheme, using ideas from compressed sensing for exploiting this sparsity. The method can be regarded as a simple reduction from multi-label regression problems to binary regression problems. We show that the number of subproblems need only be logarithmic in the total number of possible labels, making this approach radically more efficient than others. We also state and prove robustness guarantees for this method in the form of regret transform bounds (in general), and also provide a more detailed analysis for the linear prediction setting.},
annote = {NULL},
author = {Hsu, Daniel and Kakade, Sham M and Langford, John and Zhang, Tong},
file = {:home/jan/Documents/Mendeley Desktop/Hsu et al/Unknown/Hsu et al. - 2009 - Multi-Label Prediction via Compressed Sensing.pdf:pdf},
title = {{Multi-Label Prediction via Compressed Sensing}},
url = {http://www.cs.columbia.edu/{~}djhsu/papers/mlcs.pdf},
year = {2009}
}
@article{Zhang2006,
author = {Zhang, Min-ling and Zhou, Zhi-hua and Member, Senior},
file = {:home/jan/Documents/Mendeley Desktop/Zhang, Zhou, Member/Unknown/Zhang, Zhou, Member - 2006 - Multilabel Neural Networks with Applications to Functional Genomics and Text Categorization.pdf:pdf},
number = {10},
pages = {1338--1351},
title = {{Multilabel Neural Networks with Applications to Functional Genomics and Text Categorization}},
volume = {18},
year = {2006}
}
@article{Wang2016,
author = {Wang, Lei and Ren, Fuji and Miao, Duoqian},
doi = {10.1002/tee.22204},
file = {:home/jan/Documents/Mendeley Desktop/Wang, Ren, Miao/IEEJ Transactions on Electrical and Electronic Engineering/Wang, Ren, Miao - 2016 - Multi-label emotion recognition of weblog sentence based on Bayesian networks.pdf:pdf},
issn = {19314973},
journal = {IEEJ Transactions on Electrical and Electronic Engineering},
keywords = {Emotion Recognition,Multi-Label,emotion recognition,gibbs sampling,latent dirichlet allocation,multi-label classification,received 27 july 2013,revised 14 december 2014},
mendeley-tags = {Emotion Recognition,Multi-Label},
number = {2},
pages = {178--184},
title = {{Multi-label emotion recognition of weblog sentence based on Bayesian networks}},
url = {http://doi.wiley.com/10.1002/tee.22204},
volume = {11},
year = {2016}
}
@article{Zhu,
abstract = {It is well-known that exploiting label correlations is important to multi-label learning. Existing approaches either assume that the label correlations are global and shared by all instances; or that the label correlations are local and shared only by a data subset. In fact, in the real-world applications, both cases may occur that some label correlations are globally applicable and some are shared only in a local group of instances. Moreover, it is also a usual case that only partial labels are observed, which makes the exploitation of the label correlations much more difficult. That is, it is hard to estimate the label correlations when many labels are absent. In this paper, we propose a new multi-label approach GLOCAL dealing with both the full-label and the missing-label cases, exploiting global and local label correlations simultaneously, through learning a latent label representation and optimizing label manifolds. The extensive experimental studies validate the effectiveness of our approach on both full-label and missing-label data.},
archivePrefix = {arXiv},
arxivId = {1704.01415},
author = {Zhu, Yue and Kwok, James T and Zhou, Zhi-Hua},
eprint = {1704.01415},
file = {:home/jan/Documents/Mendeley Desktop/Zhu, Kwok, Zhou/Unknown/Zhu, Kwok, Zhou - Unknown - Multi-Label Learning with Global and Local Label Correlation.pdf:pdf},
keywords = {Global and local label correlation,label manifold,missing labels,multi-label learning},
title = {{Multi-Label Learning with Global and Local Label Correlation}},
url = {https://arxiv.org/pdf/1704.01415.pdf http://arxiv.org/abs/1704.01415},
year = {2017}
}
@book{Hastie2009,
author = {Hastie, Trevor and Tibshiranie, Robert and Friedman, Jerome H.},
edition = {2nd},
publisher = {New York: Springer},
title = {{No Title}},
year = {2009}
}
@article{Spolaor2016,
abstract = {Each example in a multi-label dataset is associated with multiple labels, which are often correlated. Learning from this data can be improved when dimensionality reduction tasks, such as feature selection, are applied. The standard approach for multi-label feature selection transforms the multi-label dataset into single-label datasets before using traditional feature selection algorithms. However, this approach often ignores label dependence. In this work, we propose an alternative method, LCFS, that constructs new labels based on relations between the original labels. By doing so, the label set from the data is augmented with second-order information before applying the standard approach. To assess LCFS, an experimental evaluation using Information Gain as a measure to estimate the importance of features was carried out on 10 benchmark multi-label datasets. This evaluation compared four LCFS settings with the standard approach, using random feature selection as a reference. For each dataset, the performance of a feature selection method is estimated by the quality of the classifiers built from the data described by the features selected by the method. The results show that a simple LCFS setting gave rise to classifiers similar to, or better than, the ones built using the standard approach. Furthermore, this work also pioneers the use of the systematic review method to survey the related work on multi-label feature selection. The summary of the 99 papers found promotes the idea that exploring label dependence during feature selection can lead to good results.},
author = {Spola{\^{o}}r, Newton and Monard, Maria Carolina and Tsoumakas, Grigorios and Lee, Huei Diana},
doi = {10.1016/j.neucom.2015.07.118},
file = {:home/jan/Documents/Mendeley Desktop/Spola{\^{o}}r et al/Neurocomputing/1-s2.0-S0925231215016197-main.pdf:pdf},
isbn = {0925-2312},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Binary relevance,Feature ranking,Filter feature selection,Information gain,Systematic review},
pages = {3--15},
publisher = {Elsevier},
title = {{A systematic review of multi-label feature selection and a new method based on label construction}},
url = {http://dx.doi.org/10.1016/j.neucom.2015.07.118},
volume = {180},
year = {2016}
}
@article{Huang2013,
abstract = {Both sentiment analysis and topic classification are frequently used in customer care and marketing. They can help people understand the brand perception and customer opinions from social media, such as online posts, tweets, forums, and blogs. As such, in recent years, many solutions have been proposed for both tasks. However, we believe that the following two problems have not been addressed adequately: (1) Conventional solutions usually treat the two tasks in isolation. When the two tasks are closely related (e.g., posts about "customer care" often have a "negative" tone), exploring their correlation may yield a better accuracy; (2) Each post is usually assigned with only one sentiment label and one topic label. Since social media is, compared to traditional document corpus, more noisy, ambiguous, and sparser, single label classification may not be able to capture the post classes accurately. To address these two problems, in this paper, we propose a multi-task multi-label (MTML) classification model that performs classification of both sentiments and topics concurrently. It incorporates results of each task from prior steps to promote and reinforce the other iteratively. For each task, the model is trained with multiple labels so that they can help address class ambiguity. In the empirical validation, we compare the accuracy of MTML model against four competing methods in two different settings. Results show that MTML produces a much higher accuracy of both sentiment and topic classifications.},
author = {Huang, Shu and Peng, Wei and Li, Jingxuan and Lee, Dongwon},
doi = {10.1145/2464464.2464512},
file = {:home/jan/Documents/Mendeley Desktop/Huang et al/Proceedings of the 5th Annual ACM Web Science Conference/Huang et al. - 2013 - Sentiment and Topic Analysis on Social Media A Multi-Task Multi-Label Classification Approach.pdf:pdf},
isbn = {9781450318891},
journal = {Proceedings of the 5th Annual ACM Web Science Conference},
keywords = {Multi-Label,Sentiment Analysis,Topic Classification},
mendeley-tags = {Multi-Label,Sentiment Analysis,Topic Classification},
pages = {172--181},
title = {{Sentiment and Topic Analysis on Social Media : A Multi-Task Multi-Label Classification Approach}},
year = {2013}
}
@inproceedings{Dembczynski2012,
abstract = {The idea of classifier chains has recently been introduced as a promising technique for multi-label classification. However, de-spite being intuitively appealing and showing strong performance in empirical studies, still very little is known about the main prin-ciples underlying this type of method. In this paper, we provide a detailed probabilistic analysis of classifier chains from a risk mini-mization perspective, thereby helping to gain a better understanding of this approach. As a main result, we clarify that the original chain-ing method seeks to approximate the joint mode of the conditional distribution of label vectors in a greedy manner. As a result of a the-oretical regret analysis, we conclude that this approach can perform quite poorly in terms of subset 0/1 loss. Therefore, we present an en-hanced inference procedure for which the worst-case regret can be upper-bounded far more tightly. In addition, we show that a proba-bilistic variant of chaining, which can be utilized for any loss func-tion, becomes tractable by using Monte Carlo sampling. Finally, we present experimental results confirming the validity of our theoretical findings.},
author = {Dembczynski, Krzysztof and Waegeman, Willem and H{\"{u}}llermeier, Eyke},
booktitle = {Frontiers in Artificial Intelligence and Applications},
doi = {10.3233/978-1-61499-098-7-294},
file = {:home/jan/Documents/Mendeley Desktop/Dembczynski, Waegeman, H{\"{u}}llermeier/Frontiers in Artificial Intelligence and Applications/Dembcz Nski, Waegeman, Ullermeier - Unknown - An Analysis of Chaining in Multi-Label Classification.pdf:pdf},
isbn = {9781614990970},
issn = {09226389},
pages = {294--299},
title = {{An analysis of chaining in multi-label classification}},
url = {https://biblio.ugent.be/publication/3132158/file/3132170},
volume = {242},
year = {2012}
}
@article{Tsoumakase,
abstract = {{\textcopyright} Springer International Publishing Switzerland 2014. The WISE 2014 challenge was concerned with the task of multi-label classification of articles coming from Greek print media. Raw data comes from the scanning of print media, article segmentation, and optical character segmentation, and therefore is quite noisy. Each article is examined by a human annotator and categorized to one or more of the topics being monitored. Topics range from specific persons, products, and companies that can be easily categorized based on keywords, to more general semantic concepts, such as environment or economy. Building multi-label classifiers for the automated annotation of articles into topics can support the work of human annotators by suggesting a list of all topics by order of relevance, or even automate the annotation process for media and/or categories that are easier to predict. This saves valuable time and allows a media monitoring company to expand the portfolio of media being monitored. This paper summarizes the approaches of the top 4 among the 121 teams that participated in the competition.},
author = {Tsoumakas, Grigorios and Papadopoulos, Apostolos and Qian, Weining and Vologiannidis, Stavros and D'yakonov, A. and Puurula, Antti and Read, Jesse and {\v{S}}vec, J. and Semenov, Stanislav},
doi = {10.1007/978-3-319-11746-1_40},
file = {:home/jan/Documents/Mendeley Desktop/Tsoumakas et al/Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)/Tsoumakas et al. - 2014 - WISE 2014 challenge Multi-label classification of print media articles to topics.pdf:pdf},
isbn = {978-3-319-11745-4},
issn = {16113349 03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {541--548},
title = {{WISE 2014 challenge: Multi-label classification of print media articles to topics}},
volume = {8787},
year = {2014}
}
@article{Gasse2015,
abstract = {The benefit of exploiting label dependence in multi-label classification is known to be closely dependent on the type of loss to be minimized. In this paper, we show that the subsets of labels that appear as irreducible factors in the factor-ization of the conditional distribution of the la-bel set given the input features play a pivotal role for multi-label classification in the context of 0/1 loss minimization, as they divide the learning task into simpler independent multi-class prob-lems. We establish theoretical results to charac-terize and identify these irreducible label factors for any given probability distribution satisfying the Composition property. The analysis lays the foundation for generic multi-label classification and optimal feature subset selection procedures under this subclass of distributions. Our conclu-sions are supported by carefully designed exper-iments on synthetic and benchmark data.},
author = {Gasse, Maxime and Aussem, Alex and Elghazel, Haytham},
file = {:home/jan/Documents/Mendeley Desktop/Gasse, Aussem, Elghazel/ICML/Gasse, Aussem, Elghazel - 2015 - On the Optimality of Multi-Label Classification under Subset Zero-One Loss for Distributions Satisfying.pdf:pdf},
isbn = {9781510810587},
journal = {ICML},
keywords = {Markov bounda,Multi-label learning,Zero-one loss},
title = {{On the Optimality of Multi-Label Classification under Subset Zero-One Loss for Distributions Satisfying the Composition Property}},
volume = {37},
year = {2015}
}
@article{Koyejo2015,
abstract = {Multilabel classification is rapidly developing as an important aspect of modern predictive modeling, motivating study of its theoretical aspects. To this end, we propose a framework for constructing and analyzing multilabel classification met-rics which reveals novel results on a parametric form for population optimal clas-sifiers, and additional insight into the role of label correlations. In particular, we show that for multilabel metrics constructed as instance-, micro-and macro-averages, the population optimal classifier can be decomposed into binary classi-fiers based on the marginal instance-conditional distribution of each label, with a weak association between labels via the threshold. Thus, our analysis extends the state of the art from a few known multilabel classification metrics such as Ham-ming loss, to a general framework applicable to many of the classification metrics in common use. Based on the population-optimal classifier, we propose a compu-tationally efficient and general-purpose plug-in classification algorithm, and prove its consistency with respect to the metric of interest. Empirical results on synthetic and benchmark datasets are supportive of our theoretical findings.},
author = {Koyejo, Oluwasanmi O. and Natarajan, Nagarajan and Ravikumar, Pradeep K. and Dhillon, Inderjit S},
file = {:home/jan/Documents/Mendeley Desktop/Koyejo et al/Advances in Neural Information Processing Systems/Koyejo et al. - 2015 - Consistent Multilabel Classification.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {3303--3311},
title = {{Consistent Multilabel Classification}},
url = {http://papers.nips.cc/paper/5883-consistent-multilabel-classification},
year = {2015}
}
@article{Spolaor2013b,
abstract = {Feature selection is an important task in machine learning, which can effectively reduce the dataset dimensionality by removing irrelevant and/or redundant features. Although a large body of research deals with feature selection in single-label data, in which measures have been proposed to filter out irrelevant features, this is not the case for multi-label data. This work proposes multi-label feature selection methods which use the filter approach. To this end, two standard multi-label feature selection approaches, which transform the multi-label data into single-label data, are used. Besides these two problem transformation approaches, we use ReliefF and Information Gain to measure the goodness of features. This gives rise to four multi-label feature selection methods. A thorough experimental evaluation of these methods was carried out on 10 benchmark datasets. Results show that ReliefF is able to select fewer features without diminishing the quality of the classifiers constructed using the features selected. {\textcopyright} 2013 Elsevier B.V.},
author = {Spola{\^{o}}r, Newton and Cherman, Everton Alvares and Monard, Maria Carolina and Lee, Huei Diana},
doi = {10.1016/j.entcs.2013.02.010},
file = {:home/jan/Documents/Mendeley Desktop/Spola{\^{o}}r et al/Electronic Notes in Theoretical Computer Science/Spola{\^{o}}r et al. - 2013 - A Comparison of Multi-label Feature Selection Methods using the Problem Transformation Approach(2).pdf:pdf},
isbn = {9783319259383},
issn = {15710661},
journal = {Electronic Notes in Theoretical Computer Science},
keywords = {Feature Ranking,Information Gain,Multi-label learning,ReliefF},
pages = {135--151},
title = {{A comparison of multi-label feature selection methods using the problem transformation approach}},
url = {http://ac.els-cdn.com/S1571066113000121/1-s2.0-S1571066113000121-main.pdf?{\_}tid=20bd543e-15e1-11e7-b65c-00000aacb362{\&}acdnat=1490944412{\_}9864553197edbe2f350763a8098154b4},
volume = {292},
year = {2013}
}
@inproceedings{Chekina2011,
abstract = {Although various algorithms for multi-label classification have been developed in recent years, there is little, if any, information as to when each method is beneficial. The main goal of this paper is to compare the classification performance of several multi-label algorithms and to develop a set of rules or tools that will help in selecting the optimal algorithm according to a specific dataset and target evaluation measure. We utilize a meta-learning approach allowing fast automatic selection of the most appropriate algorithm for an unseen dataset based on its descriptive characteristics. We also define a list of characteristics specific for multi-label datasets. The experimental results indicate the applicability and usefulness of the meta-learning approach.},
author = {Chekina, Lena and Rokach, Lior and Shapira, Bracha},
booktitle = {Proceedings - IEEE International Conference on Data Mining, ICDM},
doi = {10.1109/ICDMW.2011.118},
file = {:home/jan/Documents/Mendeley Desktop/Chekina, Rokach, Shapira/Proceedings - IEEE International Conference on Data Mining, ICDM/06137383.pdf:pdf},
isbn = {9780769544090},
issn = {15504786},
keywords = {Dataset characteristics,Evaluation measures,Meta-learning,Multi-label classification},
pages = {220--227},
title = {{Meta-learning for selecting a multi-label classification algorithm}},
year = {2011}
}
@article{Gibaja2015a,
abstract = {Multilabel learning has become a relevant learning paradigm in the past years due to the increasing number of fields where it can be applied and also to the emerging number of techniques that are being developed. This article presents an up-to-date tutorial about multilabel learning that introduces the paradigm and describes the main contributions developed. Evaluation measures, fields of application, trending topics, and resources are also presented.},
author = {Gibaja, Eva and Ventura, Sebasti{\'{a}}n},
doi = {10.1145/2716262},
file = {:home/jan/Documents/Mendeley Desktop/Gibaja, Ventura/ACM Computing Surveys (CSUR)/Gibaja, Ventura - 2010 - A Tutorial on Multi-Label Learning.pdf:pdf},
isbn = {0360-0300},
issn = {0360-0300},
journal = {ACM Computing Surveys (CSUR)},
keywords = {Multilabel learning,classification,data mining,machine learning,ranking},
number = {3},
pages = {52:1----52:38},
title = {{A Tutorial on Multilabel Learning}},
url = {https://www.researchgate.net/profile/Sebastian{\_}Ventura/publication/270337594{\_}A{\_}Tutorial{\_}on{\_}Multi-Label{\_}Learning/links/54bcd8460cf253b50e2d697b.pdf http://doi.acm.org/10.1145/2716262},
volume = {47},
year = {2015}
}
@article{Spyromitros-Xioufis2016,
abstract = {In many practical applications of supervised learning the task involves the predic-tion of multiple target variables from a common set of input variables. When the prediction targets are binary the task is called multi-label classification, while when the targets are con-tinuous the task is called multi-target regression. In both tasks, target variables often exhibit statistical dependencies and exploiting them in order to improve predictive accuracy is a core challenge. A family of multi-label classification methods address this challenge by building a separate model for each target on an expanded input space where other targets are treated as additional input variables. Despite the success of these methods in the multi-label classi-fication domain, their applicability and effectiveness in multi-target regression has not been studied until now. In this paper, we introduce two new methods for multi-target regression, called Stacked Single-Target and Ensemble of Regressor Chains, by adapting two popu-lar multi-label classification methods of this family. Furthermore, we highlight an inherent problem of these methods -a discrepancy of the values of the additional input variables be-tween training and prediction -and develop extensions that use out-of-sample estimates of the target variables during training in order to tackle this problem. The results of an extensive experimental evaluation carried out on a large and diverse collection of datasets show that, when the discrepancy is appropriately mitigated, the proposed methods attain consistent im-provements over the independent regressions baseline. Moreover, two versions of Ensemble of Regression Chains perform significantly better than four state-of-the-art methods includ-ing regularization-based multi-task learning methods and a multi-objective random forest approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1211.6581v5},
author = {Spyromitros-Xioufis, Eleftherios and Tsoumakas, Grigorios and Groves, William and Vlahavas, Ioannis and Spyromitros-Xioufis, E and Tsoumakas, G and Vlahavas, I and Groves, W},
doi = {10.1007/s10994-016-5546-z},
eprint = {arXiv:1211.6581v5},
file = {:home/jan/Documents/Mendeley Desktop/Spyromitros-Xioufis et al/Unknown/Spyromitros-Xioufis et al. - 2016 - Multi-Target Regression via Input Space Expansion Treating Targets as Inputs.pdf:pdf},
keywords = {Chaining,Classification {\textperiodcentered},Multi-label,Multi-target,Regression {\textperiodcentered},Stacking {\textperiodcentered}},
title = {{Multi-Target Regression via Input Space Expansion: Treating Targets as Inputs}},
url = {http://dx.doi.org/10.1007/s10994-016-5546-z.},
year = {2016}
}
@article{Dembczynski2013,
abstract = {We compare the plug-in rule approach for optimizing the F $\beta$-measure in multi-label classification with an approach based on structured loss minimization, such as the structured support vector machine (SSVM). Whereas the former derives an optimal prediction from a probabilistic model in a separate inference step, the latter seeks to optimize the F $\beta$-mcasurc directly during the training phase. We introduce a novel plug-in rule algorithm that estimates all parameters required for a Bayes-optimal prediction via a set of multinomial regression models, and we compare this algorithm with SSVMs in terms of computational complexity and statistical consistency. As a main theoretical result, we show that our plug-in rule algorithm is consistent, whereas the SSVM approaches are not. Finally, we present results of a large experimental study showing the benefits of the introduced algorithm. Copyright 2013 by the author(s).},
annote = {NULL},
author = {Dembczy{\'{n}}ski, Krzysztof and Jachnik, Arkadiusz and Kot{\l}owski, Wojciech and Waegeman, Willem and H{\"{u}}llermeier, Eyke},
file = {:home/jan/Documents/Mendeley Desktop/Dembczy{\'{n}}ski et al/30th International Conference on Machine Learning, ICML 2013/Dembczyski et al. - Unknown - Optimizing the F-Measure in Multi-Label Classification Plug-in Rule Approach versus Structured Loss Minimi.pdf:pdf},
journal = {30th International Conference on Machine Learning, ICML 2013},
number = {PART 3},
pages = {2167--2175},
title = {{Optimizing the F-measure in multi-label classification: Plug-in rule approach versus structured loss minimization}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84897550056{\&}partnerID=tZOtx3y1},
volume = {28},
year = {2013}
}
@article{Readd,
abstract = {In the ``classifier chains{\{}''{\}} (CC) approach for multi-label$\backslash$nclassification, the predictions of binary classifiers are cascaded along$\backslash$na chain as additional features. This method has attained high predictive$\backslash$nperformance, and is receiving increasing analysis and attention in the$\backslash$nrecent multi-label literature, although a deep understanding of its$\backslash$nperformance is still taking shape. In this paper, we show that CC gets$\backslash$npredictive power from leveraging labels as additional stochastic$\backslash$nfeatures, contrasting with many other methods, such as stacking and$\backslash$nerror correcting output codes, which use label dependence only as kind$\backslash$nof regularization. CC methods can learn a concept which these cannot,$\backslash$neven supposing the same base classifier and hypothesis space. This leads$\backslash$nus to connections with deep learning (indeed, we show that CC is$\backslash$ncompetitive precisely because it is a deep learner), and we employ deep$\backslash$nlearning methods -showing that they can supplement or even replace a$\backslash$nclassifier chain. Results are convincing, and throw new insight into$\backslash$npromising future directions.},
author = {Read, Jesse and Hollmen, Jaakko},
file = {:home/jan/Documents/Mendeley Desktop/Read, Hollmen/Advances in Intelligent Data Analysis Xiii/Read, Hollm{\'{e}}n - Unknown - A Deep Interpretation of Classifier Chains.pdf:pdf},
isbn = {978-3-319-12571-8; 978-3-319-12570-1},
issn = {0302-9743},
journal = {Advances in Intelligent Data Analysis Xiii},
pages = {251--262},
title = {{A Deep Interpretation of Classifier Chains}},
url = {http://jmread.github.io/papers/Read, Holmen - A Deep Interpretation of Classifier Chains.pdf},
volume = {8819},
year = {2014}
}
@article{Lee2015,
abstract = {a b s t r a c t Multi-label feature selection involves selecting important features from multi-label data sets. This can be achieved by ranking features based on their importance and then selecting the top-ranked features. Many multi-label feature selection methods for finding a feature subset that can improve multi-label learning accuracy have been proposed. In contrast, computationally efficient multi-label feature selection methods have not been studied extensively. In this study, we propose a fast multi-label feature selection method based on information-theoretic feature ranking. Experimental results demon-strate that the proposed method generates a feature subset significantly faster than several other multi-label feature selection methods for large multi-label data sets.},
author = {Lee, Jaesung and Kim, Dae-Won},
doi = {10.1016/j.patcog.2015.04.009},
file = {:home/jan/Documents/Mendeley Desktop/Lee, Kim/Pattern Recognition/Lee, Kim - 2015 - Fast multi-label feature selection based on information-theoretic feature ranking.pdf:pdf},
journal = {Pattern Recognition},
keywords = {Entropy,Interaction information,Multi-label feature selection,Mutual information},
pages = {2761--2771},
title = {{Fast multi-label feature selection based on information-theoretic feature ranking}},
url = {http://ac.els-cdn.com.ez.sun.ac.za/S0031320315001338/1-s2.0-S0031320315001338-main.pdf?{\_}tid=1581c368-15e9-11e7-964a-00000aab0f01{\&}acdnat=1490947829{\_}7fc74eb90a545073fe28ca290654e346},
volume = {48},
year = {2015}
}
@article{Godbole,
abstract = {In this paper we present methods of enhancing existing discriminative classifiers for multi-labeled predictions. Discriminative methods like support vector machines perform very well for uni-labeled text classification tasks. Multi-labeled classification is a harder task subject to relatively less attention. In the multi-labeled setting, classes are often related to each other or part of a is-a hierarchy. We present a new technique for combining text features and features indicating relationships between classes, which can be used with any discriminative algorithm. We also present two enhancements to the margin of SVMs for building better models in the presence of overlapping classes. We present results of experiments on real world text benchmark datasets. Our new methods beat accuracy of existing methods with statistically significant improvements.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {10.1007/978-3-540-24775-3{\_}5},
author = {Godbole, Shantanu and Sarawagi, Sunita},
doi = {10.1007/978-3-540-24775-3_5},
eprint = {978-3-540-24775-3{\_}5},
file = {:home/jan/Documents/Mendeley Desktop/Godbole, Sarawagi/Lecture Notes in Computer Science/Godbole, Sarawagi - 2004 - Discriminative Methods for Multi-labeled Classification.pdf:pdf},
isbn = {978-3-540-22064-0},
issn = {03029743},
journal = {Lecture Notes in Computer Science},
pages = {22--30},
primaryClass = {10.1007},
title = {{Discriminative Methods for Multi-labeled Classification}},
url = {http://link.springer.com/10.1007/978-3-540-24775-3{\_}5},
volume = {3056},
year = {2004}
}
@article{Turney2002,
abstract = {This paper presents a simple unsupervised learning algorithm for classifying reviews as recommended (thumbs up) or not recommended (thumbs down). The classification of a review is predicted by the average semantic orientation of the phrases in the review that contain adjectives or adverbs. A phrase has a positive semantic orientation when it has good associations (e.g., “subtle nuances”) and a negative semantic orientation when it has bad associations (e.g., “very cavalier”). In this paper, the semantic orientation of a phrase is calculated as the mutual information between the given phrase and the word “excellent” minus the mutual information between the given phrase and the word “poor”. A review is classified as recommended if the average semantic orientation of its phrases is positive. The algorithm achieves an average accuracy of 74{\%} when evaluated on 410 reviews from Epinions, sampled from four different domains (reviews of automobiles, banks, movies, and travel destinations). The accuracy ranges from 84{\%} for automobile reviews to 66{\%} for movie reviews.},
archivePrefix = {arXiv},
arxivId = {cs/0212032},
author = {Turney, Peter D},
doi = {10.3115/1073083.1073153},
eprint = {0212032},
file = {:home/jan/Documents/Mendeley Desktop/Turney/Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL)/Turney - 2002 - Thumbs up or thumbs down Semantic Orientation applied to Unsupervised Classification of Reviews.pdf:pdf},
issn = {0738467X},
journal = {Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL)},
keywords = {Sentiment Analysis},
mendeley-tags = {Sentiment Analysis},
number = {July},
pages = {417--424},
pmid = {16715730},
primaryClass = {cs},
title = {{Thumbs up or thumbs down? Semantic Orientation applied to Unsupervised Classification of Reviews}},
year = {2002}
}
@article{Zhanga,
abstract = {—Multi-label learning deals with the problem where each example is represented by a single instance (feature vector) while associated with a set of class labels. Existing approaches learn from multi-label data by manipulating with identical feature set, i.e. the very instance representation of each example is employed in the discrimination processes of all class labels. However, this popular strategy might be suboptimal as each label is supposed to possess specific characteristics of its own. In this paper, another strategy to learn from multi-label data is studied, where label-specific features are exploited to benefit the discrimination of different class labels. Accordingly, an intuitive yet effective algorithm named LIFT, i.e. multi-label learning with Label specIfic FeaTures, is proposed. LIFT firstly constructs features specific to each label by conducting clustering analysis on its positive and negative instances, and then performs training and testing by querying the clustering results. Comprehensive experiments on a total of seventeen benchmark data sets clearly validate the superiority of LIFT against other well-established multi-label learning algorithms as well as the effectiveness of label-specific features.},
author = {Zhang, Min-Ling and Wu, Lei},
file = {:home/jan/Documents/Mendeley Desktop/Zhang, Wu/Unknown/Zhang, Wu - Unknown - LIFT Multi-Label Learning with Label-Specific Features.pdf:pdf},
keywords = {Index Terms—machine learning,label correlations,label-specific features,multi-label learning},
title = {{LIFT: Multi-Label Learning with Label-Specific Features}}
}
@article{Tan2013,
archivePrefix = {arXiv},
arxivId = {arXiv:1109.6018v1},
author = {Tan, C and Lee, L and Tang, J},
eprint = {arXiv:1109.6018v1},
file = {:home/jan/Documents/Mendeley Desktop/Tan, Lee, Tang/Unknown/Tan, Lee, Tang - 2013 - User-Level Sentiment Analysis Incorporating Social Networks Background Sentiment Analysis ( SA ).pdf:pdf},
isbn = {9781450308137},
keywords = {opinion mining,sentiment analysis,social networks,twitter},
number = {March},
pages = {1397--1405},
title = {{User-Level Sentiment Analysis Incorporating Social Networks Background : Sentiment Analysis ( SA )}},
year = {2013}
}
@article{Huang2010,
abstract = {Extreme learning machine (ELM) as an emergent technology has shown its good performance in regression applications as well as in large dataset (and/or multi-label) classification applications. The ELM theory shows that the hidden nodes of the ‘‘generalized'' single-hidden layer feedforward networks (SLFNs), which need not be neuron alike, can be randomly generated and the universal approximation capability of such SLFNs can be guaranteed. This paper further studies ELM for classification in the aspect of the standard optimization method and extends ELM to a specific type of ‘‘generalized'' SLFNs—support vector network. This paper shows that: (1) under the ELM learning framework, SVM's maximal margin property and the minimal norm of weights theory of feedforward neural networks are actually consistent; (2) from the standard optimization method point of view ELM for classification and SVM are equivalent but ELM has less optimization constraints due to its special separability feature; (3) as analyzed in theory and further verified by the simulation results, ELM for classification tends to achieve better generalization performance than traditional SVM. ELM for classification is less sensitive to user specified parameters and can be implemented easily.},
author = {Huang, Guang-Bin and Ding, Xiaojian and Zhou, Hongming},
doi = {10.1016/j.neucom.2010.02.019},
file = {:home/jan/Documents/Mendeley Desktop/Huang, Ding, Zhou/Neurocomputing/Optimization method based extreme learning machine for classification.pdf:pdf},
isbn = {0925-2312},
issn = {09252312},
journal = {Neurocomputing},
keywords = {ELM feature space,ELM kernel,Equivalence between ELM and SVM,Extreme learning machine,Maximal margin,Minimal norm of weights,Primal and dual ELM networks,Support vector machine,Support vector network,extreme learning machine,support vector machine,support vector network},
number = {1-3},
pages = {155--163},
publisher = {Elsevier},
title = {{Optimization method based extreme learning machine for classification}},
url = {http://dx.doi.org/10.1016/j.neucom.2010.02.019},
volume = {74},
year = {2010}
}
@article{Zhang2005,
abstract = {— In multi-label learning, each instance in the training set is associated with a set of labels, and the task is to output a label set whose size is unknown a priori for each unseen instance. In this paper, a multi-label lazy learning approach named ML-kNN is presented, which is derived from the traditional k-nearest neighbor (kNN) algorithm. In detail, for each new instance, its k nearest neighbors are firstly identified. After that, according to the label sets of these neighboring instances, maximum a posteriori (MAP) principle is utilized to determine the label set for the new instance. Experiments on a real-world multi-label bioinformatic data show that ML-kNN is highly comparable to existing multi-label learning algorithms.},
author = {Zhang, ML},
file = {:home/jan/Documents/Mendeley Desktop/Zhang/Granular Computing, 2005 IEEE/Zhang - 2005 - A k-nearest neighbor based algorithm for multi-label classification.pdf:pdf},
isbn = {0780390172},
journal = {Granular Computing, 2005 IEEE},
pages = {718--721},
title = {{A k-nearest neighbor based algorithm for multi-label classification}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1547385},
year = {2005}
}
@article{DeCarvalho2009,
abstract = {Industries have to design and produce performing and reliable systems. Nevertheless, designers suffer from the diversity of methods, which are not really adequate to their needs. Authors highlight the need of close interactions between product and project design, often treated either independently or sequentially, necessary to improve system design, and logistics in this context. Strengthening the links between product design and project management processes is an ongoing challenge, and this situation relies on perfect control of methods, tools and know-how, both on the technical side as well as on the organizational side. The aim of our work is to facilitate the project manager's decision making, thus allowing him to define, follow and adapt a working plan, while still considering various organizational options. From these options, the project manager chooses the scheme that best encompasses the project's objectives with respect to costs, delay and risks, without neglecting performance and safety. To encourage the project manager to explore various possibilities, we developed and tested a heuristic based on ant colony optimization and evolutionary algorithm adapted for multi-objective problems. Its hybridization with a tabu search and a greedy algorithm were performed in order to accelerate convergence of the research study and to reduce the cost engendered by the evaluation process. The experiments carried out reveals that it was possible to offer the decision maker a reduced number of solutions that he can evaluate more accurately in order to choose one according to technical, economic and financial criteria. {\textcopyright} 2009 Springer-Verlag Berlin Heidelberg.},
author = {de Carvalho, Andr{\'{e}} C. P. L. F. and Freitas, Alex A.},
doi = {10.1007/978-3-642-01536-6_8},
isbn = {9783642015359},
issn = {1860949X},
journal = {Studies in Computational Intelligence},
pages = {177--195},
title = {{A tutorial on multi-label classification techniques}},
url = {http://link.springer.com/10.1007/978-3-642-01536-6{\_}8},
volume = {205},
year = {2009}
}
@inproceedings{Monta??es2014,
abstract = {Several meta-learning techniques for multi-label classification (MLC), such as chaining and stacking, have already been proposed in the literature, mostly aimed at improving predictive accuracy through the exploitation of label dependencies. In this paper, we propose another technique of that kind, called dependent binary relevance (DBR) learning. DBR combines properties of both, chaining and stacking. We provide a careful analysis of the relationship between these and other techniques, specifically focusing on the underlying dependency structure and the type of training data used for model construction. Moreover, we offer an extensive empirical evaluation, in which we compare different techniques on MLC benchmark data. Our experiments provide evidence for the good performance of DBR in terms of several evaluation measures that are commonly used in MLC. ?? 2013 Elsevier Ltd. All rights reserved.},
author = {Monta??es, Elena and Senge, Robin and Barranquero, Jose and {Ram??n Quevedo}, Jos?? and {Jos?? Del Coz}, Juan and H??llermeier, Eyke},
booktitle = {Pattern Recognition},
doi = {10.1016/j.patcog.2013.09.029},
file = {:home/jan/Documents/Mendeley Desktop/Montaes et al/Pattern Recognition/Unknown - Unknown - Dependent binary relevance models for multi-label classification.pdf:pdf},
issn = {00313203},
keywords = {Chaining,Label dependence,Multi-label classification,Stacking},
number = {3},
pages = {1494--1508},
title = {{Dependent binary relevance models for multi-label classification}},
url = {http://ac.els-cdn.com.ez.sun.ac.za/S0031320313004019/1-s2.0-S0031320313004019-main.pdf?{\_}tid=37459642-16bf-11e7-b2fb-00000aab0f6b{\&}acdnat=1491039798{\_}e130263764b6a9af87e0d2e25ee5285e},
volume = {47},
year = {2014}
}
@article{Cherman2011a,
abstract = {Traditional classification algorithms consider learning problems that contain only one label, i.e., each example is associated with one single nominal target variable characterizing its property. However, the number of practical applications involving data with multiple target variables has increased. To learn from this sort of data, multi-label classification algorithms should be used. The task of learning from multi-label data can be addressed by methods that transform the multi-label classification problem into several single-label classification problems. In this work, two well known methods based on this approach are used, as well as a third method we propose to overcome some deficiencies of one of them, in a case study using textual data related to medical findings, which were structured using the bag-of-words approach. The experimental study using these three methods shows an improvement on the results obtained by our proposed multi-label classification method.},
annote = {NULL},
author = {Cherman, Everton Alvares and Monard, Maria Carolina and Metz, Jean},
file = {:home/jan/Documents/Mendeley Desktop/Cherman, Monard, Metz/CLEI ELECTRONIC JOURNAL/Cherman, Monard, Metz - 2011 - Multi-label Problem Transformation Methods a Case Study.pdf:pdf},
issn = {0717-5000},
journal = {CLEI ELECTRONIC JOURNAL},
keywords = {binary relevance,label dependency,machine learning,multi-label classification},
number = {1},
pages = {4--4},
publisher = {Centro Latinoamericano de Estudios en Inform{\'{a}}tica},
title = {{Multi-label Problem Transformation Methods: a Case Study}},
volume = {14},
year = {2011}
}
@article{Jiang2011,
abstract = {Sentiment analysis on Twitter data has attracted much attention recently. In this paper, we focus on target-dependent Twitter sentiment classification; namely, given a query, we classify the sentiments of the tweets as positive, negative or neutral according to whether they contain positive, negative or neutral sentiments about that query. Here the query serves as the target of the sentiments. The state-ofthe- art approaches for solving this problem always adopt the target-independent strategy, which may assign irrelevant sentiments to the given target. Moreover, the state-of-the-art approaches only take the tweet to be classified into consideration when classifying the sentiment; they ignore its context (i.e., related tweets). However, because tweets are usually short and more ambiguous, sometimes it is not enough to consider only the current tweet for sentiment classification. In this paper, we propose to improve target-dependent Twitter sentiment classification by 1) incorporating target-dependent features; and 2) taking related tweets into consideration. According to the experimental results, our approach greatly improves the performance of target-dependent sentiment classification.},
annote = {NULL},
author = {Jiang, Long and Yu, Mo and Zhou, Ming and Liu, Xiaohua and Zhao, Tiejun},
file = {:home/jan/Documents/Mendeley Desktop/Jiang et al/Computational Linguistics/Jiang et al. - 2011 - Target-dependent Twitter Sentiment Classification.pdf:pdf},
isbn = {9781932432879},
journal = {Computational Linguistics},
pages = {151--160},
title = {{Target-dependent Twitter Sentiment Classification}},
url = {http://www.aclweb.org/anthology/P11-1016},
year = {2011}
}
@article{Yu,
abstract = {Abstract The multi-label classification problem has generated significant interest in recent years. However, existing approaches do not adequately address two key challenges:(a) scaling up to problems with a large number (say millions) of labels, and (b) handling data ...},
author = {Yu, H F and Jain, P and Kar, P and Dhillon, I S},
file = {:home/jan/Documents/Mendeley Desktop/Yu et al/Icml/Yu et al. - 2014 - Large-scale Multi-label Learning with Missing Labels.pdf:pdf},
journal = {Icml},
title = {{Large-scale Multi-label Learning with Missing Labels.}},
url = {http://www.jmlr.org/proceedings/papers/v32/yu14.pdf},
year = {2014}
}
@article{Chen2012,
abstract = {Label space dimension reduction (LSDR) is an efficient and effective paradigm for multi-label classification with many classes. Existing approaches to LSDR, such as compressive sensing and principal label space transformation, exploit only the label part of the dataset, but not the feature part. In this paper, we propose a novel approach to LSDR that considers both the label and the feature parts. The approach, called conditional principal label space transformation, is based on minimizing an upper bound of the popular Hamming loss. The minimization step of the approach can be carried out efficiently by a simple use of singular value decomposition. In addition, the approach can be extended to a kernelized version that allows the use of sophisticated feature combinations to assist LSDR. The experimental results verify that the proposed approach is more effective than existing ones to LSDR across many real-world datasets. 1},
annote = {NULL},
author = {Chen, Yn and Lin, Ht},
file = {:home/jan/Documents/Mendeley Desktop/Chen, Lin/Advances in Neural Information Processing Systems/Chen, Lin - 2012 - Feature-aware Label Space Dimension Reduction for Multi-label Classification.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {1538--1546},
title = {{Feature-aware Label Space Dimension Reduction for Multi-label Classification}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/NIPS2012{\_}0728.pdf},
year = {2012}
}
@article{Grimmer2013,
abstract = {Politics and political conflict often occur in the written and spoken word. Scholars have long recognized this, but the massive costs of analyzing even moderately sized collections of texts have hindered their use in political science research. Here lies the promise of automated text analysis: it substantially reduces the costs of analyzing large collections of text. We provide a guide to this exciting new area of research and show how, in many instances, the methods have already obtained part of their promise. But there are pitfalls to using automated methods—they are no substitute for careful thought and close reading and require extensive and problem-specific validation. We survey a wide range of new methods, provide guidance on how to validate the output of the models, and clarify misconceptions and errors in the literature. To conclude, we argue that for automated text methods to become a standard tool for political scientists, methodologists must contribute new methods and new methods of validation.},
archivePrefix = {arXiv},
arxivId = {cs/9605103},
author = {Grimmer, Justin and Stewart, Brandon M.},
doi = {10.1093/pan/mps028},
eprint = {9605103},
file = {:home/jan/Documents/Mendeley Desktop/Grimmer, Stewart/Political Analysis/Grimmer, Stewart - 2013 - Text as data The promise and pitfalls of automatic content analysis methods for political texts.pdf:pdf},
isbn = {1047-1987},
issn = {10471987},
journal = {Political Analysis},
number = {3},
pages = {267--297},
primaryClass = {cs},
title = {{Text as data: The promise and pitfalls of automatic content analysis methods for political texts}},
volume = {21},
year = {2013}
}
@article{Tsoumakas2012,
author = {Tsoumakas, Grigorios and Zhang, Min-Ling and Zhou, Zhi-Hua},
doi = {10.1007/s10994-012-5292-9},
issn = {0885-6125},
journal = {Machine Learning},
month = {jul},
number = {1-2},
pages = {1--4},
title = {{Introduction to the special issue on learning from multi-label data}},
url = {http://link.springer.com/10.1007/s10994-012-5292-9},
volume = {88},
year = {2012}
}
@article{Chua2012,
abstract = {Many event monitoring systems rely on counting known keywords in streaming text data to detect sudden spikes in frequency. But the dynamic and conversational nature of Twitter makes it hard to select known keywords for monitoring. Here we consider a method of automatically finding noun phrases (NPs) as keywords for event monitoring in Twitter. Finding NPs has two aspects, identifying the boundaries for the subsequence of words which represent the NP, and classifying the NP to a specific broad category such as politics, sports, etc. To classify an NP, we define the feature vector for the NP using not just the words but also the author's behavior and social activities. Our results show that we can classify many NPs by using a sample of training data from a knowledge-base. {\textcopyright} 2012 ACM.},
author = {Chua, Freddy C. T. and Cohen, William W. and Betteridge, Justin and Lim, Ee-Peng},
doi = {10.1145/2396761.2398501},
file = {:home/jan/Documents/Mendeley Desktop/Chua et al/Proceedings of the 21st ACM International Conference on Information and Knowledge Management (CIKM'12)/Chua et al. - 2012 - Community-based classification of noun phrases in Twitter.pdf:pdf},
isbn = {9781450311564 (ISBN)},
journal = {Proceedings of the 21st ACM International Conference on Information and Knowledge Management (CIKM'12)},
keywords = {Classification (of information),Event monitoring,Feature vectors,Knowledge base,Knowledge management,Noun phrase,Social activities,Social networking (online),Streaming texts,Training data,named entities,noun phrases,social media,twitter},
pages = {1702--1706},
title = {{Community-based classification of noun phrases in Twitter}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84871048382{\&}partnerID=40{\&}md5=43be17d5ea6e03c997654e4100ae68d6},
volume = {1},
year = {2012}
}
@article{Luaces,
abstract = {The goal of multilabel (ML) classification is to induce models able to tag objects with the labels that better describe them. The main baseline for ML classi-fication is Binary Relevance (BR), which is commonly criticized in the literature because of its label inde-pendence assumption. Despite this fact, this paper dis-cusses some interesting properties of BR, mainly that it produces optimal models for several ML loss functions. Additionally, we present an analytical study about ML benchmarks datasets, pointing out some shortcomings. As a result, this paper proposes the use of synthetic datasets to better analyze the behavior of ML meth-ods in domains with different characteristics. To sup-port this claim, we perform some experiments using synthetic data proving the competitive performance of BR with respect to a more complex method in difficult problems with many labels, a conclusion which was not stated by previous studies.},
author = {Luaces, Oscar and D{\'{i}}ez, Jorge and Barranquero, Jos{\'{e}} and {Del Coz}, Juan Jos{\'{e}} and Bahamonde, Antonio},
file = {:home/jan/Documents/Mendeley Desktop/Luaces et al/Unknown/Luaces et al. - Unknown - Binary Relevance Efficacy for Multilabel Classification.pdf:pdf},
keywords = {Binary,Classification {\textperiodcentered},Label dependency,Multilabel,Rele-vance {\textperiodcentered},Synthetic datasets {\textperiodcentered}},
title = {{Binary Relevance Efficacy for Multilabel Classification}}
}
@article{Trohidis2008,
abstract = {In this paper, the automated detection of emotion in music is modeled as a multilabel classification task, where a piece of music may belong to more than one class. Four algorithms are evaluated and compared in this task. Furthermore, the predictive power of several audio features is evaluated using a new multilabel feature selection method. Experiments are conducted on a set of 593 songs with 6 clusters of music emotions based on the Tellegen-Watson-Clark model. Results provide interesting insights into the quality of the discussed algorithms and features.},
author = {Trohidis, Konstantinos and Kalliris, George},
file = {:home/jan/Documents/Mendeley Desktop/Trohidis, Kalliris/Proc. ISMIR/Trohidis, Kalliris - 2008 - Multi-Label Classification of Music Into Emotions.pdf:pdf},
isbn = {0615248497},
journal = {Proc. ISMIR},
pages = {325--330},
title = {{Multi-Label Classification of Music Into Emotions}},
url = {http://ismir2008.ismir.net/papers/ISMIR2008{\_}275.pdf},
volume = {2008},
year = {2008}
}
@article{Gibaja2015,
abstract = {Multilabel learning has become a relevant learning paradigm in the past years due to the increasing number of fields where it can be applied and also to the emerging number of techniques that are being developed. This article presents an up-to-date tutorial about multilabel learning that introduces the paradigm and describes the main contributions developed. Evaluation measures, fields of application, trending topics, and resources are also presented.},
author = {Gibaja, Eva and Ventura, Sebasti{\'{a}}n},
doi = {10.1145/2716262},
file = {:home/jan/Documents/Mendeley Desktop/Gibaja, Ventura/ACM Computing Surveys (CSUR)/Gibaja - 2013 - 39 A Tutorial on Multi - Label Learning.pdf:pdf},
isbn = {0360-0300},
issn = {0360-0300},
journal = {ACM Computing Surveys (CSUR)},
keywords = {Multilabel learning,classification,data mining,machine learning,ranking},
number = {3},
pages = {52:1----52:38},
title = {{A Tutorial on Multilabel Learning}},
url = {https://www.researchgate.net/profile/Sebastian{\_}Ventura/publication/270337594{\_}A{\_}Tutorial{\_}on{\_}Multi-Label{\_}Learning/links/54bcd8460cf253b50e2d697b.pdf http://doi.acm.org/10.1145/2716262},
volume = {47},
year = {2015}
}
@article{Goncalves,
abstract = {Multi-label classification (MLC) is the task of assigning multiple class labels to an object based on the features that describe the object. One of the most effective MLC methods is known as Classifier Chains (CC). This approach consists in training q binary classifiers linked in a chain, y1 → y2 → ... → yq, with each responsible for classifying a specific label in {\{}l1, l2, ..., lq{\}}. The chaining mechanism allows each individual classifier to incorporate the predictions of the previous ones as additional information at classification time. Thus, possible correlations among labels can be automatically exploited. Nevertheless, CC suffers from two important drawbacks: (i) the label ordering is decided at random, although it usually has a strong effect on predictive accuracy; (ii) all labels are inserted into the chain, although some of them might carry irrelevant information to discriminate the others. In this paper we tackle both problems at once, by proposing a novel genetic algorithm capable of searching for a single optimized label ordering, while at the same time taking into consideration the utilization of partial chains. Experiments on benchmark datasets demonstrate that our approach is able to produce models that are both simpler and more accurate.},
author = {Gon{\c{c}}alves, Eduardo C and Plastino, Alexandre and Freitas, Alex A},
doi = {10.1145/2739480.2754650},
file = {:home/jan/Documents/Mendeley Desktop/Gon{\c{c}}alves, Plastino, Freitas/Unknown/Gon{\c{c}}alves, Plastino, Freitas - Unknown - Simpler is Better a Novel Genetic Algorithm to Induce Compact Multi - label Chain Classifiers.pdf:pdf},
isbn = {9781450334723},
journal = {Proceedings of the 2015 on Genetic and Evolutionary Computation Conference (GECCO 2015)},
keywords = {classifier chains,genetic algorithms,multi-label classification},
pages = {559--566},
title = {{Simpler is Better : a Novel Genetic Algorithm to Induce Compact Multi-label Chain Classifiers}},
url = {https://www.researchgate.net/profile/Eduardo{\_}Goncalves17/publication/301290182{\_}Simpler{\_}is{\_}Better{\_}a{\_}Novel{\_}Genetic{\_}Algorithm{\_}to{\_}Induce{\_}Compact{\_}Multi-label{\_}Chain{\_}Classifiers/links/570fb19a08aec95f06158862.pdf},
year = {2015}
}
@article{Gibaja2014,
abstract = {Multi-label learning is quite a recent supervised learning paradigm. Owing to its capabilities to improve performance in problems where a pattern may have more than one associated class, it has attracted the attention of researchers, producing an increasing number of publications. This study presents an up-to-date overview about multi-label learning with the aim of sorting and describing the main approaches developed till now. The formal definition of the paradigm, the analysis of its impact on the literature, its main applications, works developed, pitfalls and guidelines, and ongoing research are presented.},
author = {Gibaja, Eva and Ventura, Sebasti{\'{a}}n},
doi = {10.1002/widm.1139},
file = {:home/jan/Documents/Mendeley Desktop/Gibaja, Ventura/Wiley Interdisciplinary Reviews Data Mining and Knowledge Discovery/Gibaja, Ventura - 2014 - Multi-label learning A review of the state of the art and ongoing research.pdf:pdf},
issn = {19424795},
journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
keywords = {multi - label learning {\textperiodcentered} review},
number = {6},
pages = {411--444},
title = {{Multi-label learning: A review of the state of the art and ongoing research}},
volume = {4},
year = {2014}
}
@article{Huanga,
abstract = {It is well known that exploiting label correlations is important for multi-label learning. Existing approaches typically exploit label correlations globally, by assuming that the label correlations are shared by all the instances. In real-world tasks, however, different instances may share different label correlations, and few correlations are globally applicable. In this paper, we propose the ML-LOC approach which allows label correlations to be exploited locally. To encode the local influence of label correlations, we derive a LOC code to enhance the feature representation of each instance. The global discrimination fitting and local correlation sensitivity are incorporated into a unified framework, and an alternating solution is developed for the optimization. Experimental results on a number of image, text and gene data sets validate the effectiveness of our approach.},
author = {Huang, Sheng-Jun and Zhou, Zhi-Hua},
doi = {10.1145/1835804.1835930},
file = {:home/jan/Documents/Mendeley Desktop/Huang, Zhou/AAAI Conference on Artificial Intelligence/Huang, Zhou - 2012 - Multi-Label Learning by Exploiting Label Correlations Locally.pdf:pdf},
isbn = {9781577355687},
issn = {9781577355687},
journal = {AAAI Conference on Artificial Intelligence},
keywords = {Machine Learning (Main Track)},
pages = {949--955},
title = {{Multi-Label Learning by Exploiting Label Correlations Locally}},
year = {2012}
}
@article{Tsoumakas2011,
abstract = {MULAN is a Java library for learning from multi-label data. It offers a variety of classification, rank-ing, thresholding and dimensionality reduction algorithms, as well as algorithms for learning from hierarchically structured labels. In addition, it contains an evaluation framework that calculates a rich variety of performance measures.},
author = {Tsoumakas, Grigorios and Gr, Greg@csd Auth and Gr, Espyromi@csd Auth and Vilcek, Jozef and Gr, Vlahavas@csd Auth},
file = {:home/jan/Documents/Mendeley Desktop/Tsoumakas et al/Journal of Machine Learning Research/Tsoumakas et al. - 2011 - MULAN A Java Library for Multi-Label Learning Eleftherios Spyromitros-Xioufis Ioannis Vlahavas.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {classification,dimensionality reduction,evaluation,hier-archical classification,multi-label data,ranking,thresholding},
pages = {2411--2414},
title = {{MULAN: A Java Library for Multi-Label Learning Eleftherios Spyromitros-Xioufis Ioannis Vlahavas}},
url = {http://www.jmlr.org/papers/volume12/tsoumakas11a/tsoumakas11a.pdf},
volume = {12},
year = {2011}
}
@article{Al-Salemi2015,
author = {Al-Salemi, B and Aziz, MJ Ab and Noah, SA},
doi = {10.1177/0165551515590079},
file = {:home/jan/Documents/Mendeley Desktop/Al-Salemi, Aziz, Noah/Journal of Information Science/Al-Salemi, Aziz, Noah - 2015 - Boosting algorithms with topic modeling for multi-label text categorization A comparative empirical study.pdf:pdf},
issn = {17416485},
journal = {Journal of Information Science},
keywords = {Multi-Label,Text Categorisation,adaboost,boosting,mh,multi-label classification,text categorization,text representation,topic modeling},
mendeley-tags = {Multi-Label,Text Categorisation},
title = {{Boosting algorithms with topic modeling for multi-label text categorization: A comparative empirical study}},
url = {http://jis.sagepub.com/content/early/2015/06/30/0165551515590079.abstract},
year = {2015}
}
@article{Huangb,
abstract = {Label embedding (LE) is an important family of multi-label classification algorithms that digest the label information jointly for better perfor-mance. Different real-world applications evalu-ate performance by different cost functions of in-terest. Current LE algorithms often aim to opti-mize one specific cost function, but they can suf-fer from bad performance with respect to other cost functions. In this paper, we resolve the performance issue by proposing a novel cost-sensitive LE algorithm that takes the cost func-tion of interest into account. The proposed algo-rithm, cost-sensitive label embedding with multi-dimensional scaling (CLEMS), approximates the cost information with the distances of the embed-ded vectors using the classic multidimensional scaling approach for manifold learning. CLEMS is able to deal with both symmetric and asym-metric cost functions, and effectively makes cost-sensitive decisions by nearest-neighbor decoding within the embedded vectors. Theoretical results justify that CLEMS achieves the cost-sensitivity and extensive experimental results demonstrate that CLEMS is significantly better than a wide spectrum of existing LE algorithms and state-of-the-art cost-sensitive algorithms across different cost functions.},
author = {Huang, Kuan-Hao and Lin, Hsuan-Tien},
file = {:home/jan/Documents/Mendeley Desktop/Huang, Lin/Unknown/Huang, Lin - Unknown - Cost-sensitive Label Embedding for Multi-label Classification.pdf:pdf},
title = {{Cost-sensitive Label Embedding for Multi-label Classification}}
}
@article{Alazaidah2016,
abstract = {—Multi label classification has become a very important paradigm in the last few years because of the increasing domains that it can be applied to. Many researchers have developed many algorithms to solve the problem of multi label classification. Nerveless, there are still some stuck problems that need to be investigated in depth. The aim of this paper is to provide researchers with a brief introduction to the problem of multi label classification, and introduce some of the most trending challenges.},
author = {Alazaidah, Raed and Ahmad, Farzana Kabir},
file = {:home/jan/Documents/Mendeley Desktop/Alazaidah, Ahmad/IJACSA) International Journal of Advanced Computer Science and Applications/Alazaidah, Ahmad - 2016 - Trending Challenges in Multi Label Classification.pdf:pdf},
journal = {IJACSA) International Journal of Advanced Computer Science and Applications},
keywords = {Correlations among labels,Multi Label Classification,—Challenges},
number = {10},
title = {{Trending Challenges in Multi Label Classification}},
url = {www.ijacsa.thesai.org},
volume = {7},
year = {2016}
}
@article{Feng2006,
abstract = { Many computer vision applications, such as scene analysis and medical image interpretation, are ill-suited for traditional classification where each image can only be associated with a single class. This has stimulated recent work in multi-label learning where a given image can be tagged with multiple class labels. A serious problem with existing approaches is that they are unable to exploit correlations between class labels. This paper presents a novel framework for multi-label learning termed Correlated Label Propagation (CLP) that explicitly models interactions between labels in an efficient manner. As in standard label propagation, labels attached to training data points are propagated to test data points; however, unlike standard algorithms that treat each label independently, CLP simultaneously co-propagates multiple labels. Existing work eschews such an approach since naive algorithms for label co-propagation are intractable. We present an algorithm based on properties of submodular functions that efficiently finds an optimal solution. Our experiments demonstrate that CLP leads to significant gains in precision/recall against standard techniques on two real-world computer vision tasks involving several hundred labels.},
author = {Feng, Kang and Rong, Jin and Sukthankar, Rahul},
doi = {10.1109/CVPR.2006.90},
file = {:home/jan/Documents/Mendeley Desktop/Feng, Rong, Sukthankar/Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition/Feng, Rong, Sukthankar - 2006 - Correlated label propagation with application to multi-label learning.pdf:pdf},
isbn = {0769525970},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
number = {c},
pages = {1719--1726},
title = {{Correlated label propagation with application to multi-label learning}},
volume = {2},
year = {2006}
}
@article{Katakis2008,
abstract = {The increased popularity of tagging during the last few years can be mainly attributed to its embracing by most of the recently thriving user-centric content publishing and management Web 2.0 applications. However, tagging systems have some limitations that have led researchers to develop methods that assist users in the tagging process, by automat- ically suggesting an appropriate set of tags. We have tried to model the automated tag suggestion problem as a multilabel text classification task in order to participate in the ECML/PKDD 2008 Discovery Challenge.},
author = {Katakis, Ioannis and Tsoumakas, Grigorios and Vlahavas, Ioannis},
file = {:home/jan/Documents/Mendeley Desktop/Katakis, Tsoumakas, Vlahavas/Proceedings of the ECMLPKDD 2008 Discovery Challenge (2008)/Katakis, Tsoumakas, Vlahavas - 2008 - Multilabel Text Classification for Automated Tag Suggestion.pdf:pdf},
issn = {10675027},
journal = {Proceedings of the ECMLPKDD 2008 Discovery Challenge (2008)},
number = {3},
pages = {1--9},
title = {{Multilabel Text Classification for Automated Tag Suggestion}},
volume = {9},
year = {2008}
}
@article{Vens2008,
abstract = {Hierarchical multi-label classification (HMC) is a variant of classification where instances may belong to multiple classes at the same time and these classes are organized in a hierarchy. This article presents several approaches to the induction of decision trees for HMC, as well as an empirical study of their use in functional genomics. We compare learning a single HMC tree (which makes predictions for all classes together) to two approaches that learn a set of regular classification trees (one for each class). The first approach defines an independent single-label classification task for each class (SC). Obviously, the hierarchy introduces dependencies between the classes. While they are ignored by the first approach, they are exploited by the second approach, named hierarchical single-label classification (HSC). Depending on the application at hand, the hierarchy of classes can be such that each class has at most one parent (tree structure) or such that classes may have multiple parents (DAG structure). The latter case has not been considered before and we show how the HMC and HSC approaches can be modified to support this setting. We compare the three approaches on 24 yeast data sets using as classification schemes MIPS's FunCat (tree structure) and the Gene Ontology (DAG structure). We show that HMC trees outperform HSC and SC trees along three dimensions: predictive accuracy, model size, and induction time. We conclude that HMC trees should definitely be considered in HMC tasks where interpretable models are desired.},
author = {Vens, Celine and Struyf, Jan and Schietgat, Leander and D{\v{z}}eroski, Sa{\v{s}}o and Blockeel, Hendrik},
doi = {10.1007/s10994-008-5077-3},
file = {:home/jan/Documents/Mendeley Desktop/Vens et al/Machine Learning/Vens et al. - Unknown - Decision Trees for Hierarchical Multi-label Classification.pdf:pdf},
isbn = {0885-6125$\backslash$r1573-0565},
issn = {08856125},
journal = {Machine Learning},
keywords = {Decision trees,Functional genomics,Hierarchical classification,Multi-label classification,Precision-recall analysis},
number = {2},
pages = {185--214},
title = {{Decision trees for hierarchical multi-label classification}},
url = {https://lirias.kuleuven.be/bitstream/123456789/186698/4/hmc.pdf},
volume = {73},
year = {2008}
}
@article{Huang,
abstract = {Multi-label learning arises in many real-world tasks where an object is naturally associated with multiple concepts. It is well-accepted that, in order to achieve a good performance, the relationship among labels should be exploited. Most existing approaches require the label relationship as prior knowledge, or exploit by counting the label co-occurrence. In this paper, we propose the MAHR approach, which is able to automatically discover and exploit label relationship. Our basic idea is that, if two labels are related, the hypothesis generated for one label can be helpful for the other label. MAHR implements the idea as a boosting approach with a hypothesis reuse mechanism. In each boosting round, the base learner for a label is generated by not only learning on its own task but also reusing the hypotheses from other labels, and the amount of reuse across labels provides an estimate of the label relationship. Extensive experimental results validate that MAHR is able to achieve superior performance and discover reasonable label relationship. Moreover, we disclose that the label relationship is usually asymmetric.},
author = {Huang, Sheng-jun and Yu, Yang and Zhou, Zhi-hua},
doi = {10.1145/2339530.2339615},
file = {:home/jan/Documents/Mendeley Desktop/Huang, Yu, Zhou/Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '12/Huang, Yu, Zhou - 2012 - Multi-label hypothesis reuse.pdf:pdf},
isbn = {9781450314626},
journal = {Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '12},
keywords = {hypothesis reuse,label relationship,multi-label learning},
pages = {525},
title = {{Multi-label hypothesis reuse}},
url = {http://dl.acm.org/citation.cfm?id=2339530.2339615},
year = {2012}
}
@article{Zhang,
abstract = {In multi-label learning, each training example is associated with a set of labels and the task is to predict the proper label set for the unseen example. Due to the tremendous (exponential) number of possible label sets, the task of learning from multi-label examples is rather challenging. Therefore, the key to successful multi-label learning is how to effectively exploit correlations between different labels to facilitate the learning process. In this paper, we propose to use a Bayesian network structure to efficiently encode the condi- tional dependencies of the labels as well as the feature set, with the feature set as the common parent of all labels. To make it practical, we give an approximate yet efficient procedure to find such a network structure. With the help of this network, multi-label learning is decomposed into a series of single-label classification problems, where a classifier is constructed for each label by incorporating its parental labels as additional features. Label sets of unseen examples are predicted recursively according to the label ordering given by the network. Extensive experiments on a broad range of data sets validate the effectiveness of our approach against other well-established methods.},
annote = {NULL},
author = {Zhang, Min-Ling and Zhang, Kun},
doi = {10.1145/1835804.1835930},
file = {:home/jan/Documents/Mendeley Desktop/Zhang, Zhang/Kdd/Zhang, Zhang - 2010 - Multi-label learning by exploiting label dependency.pdf:pdf},
isbn = {9781450300551},
issn = {9781577355687},
journal = {Kdd},
keywords = {Learning—concept learn-ing,induction General Terms Algorithms},
pages = {999--1007},
title = {{Multi-label learning by exploiting label dependency}},
url = {http://dl.acm.org/citation.cfm?doid=1835804.1835930},
year = {2010}
}
@article{Reade,
abstract = {Competitive methods for multi-label data typically invest in learning labels together. To do so in a beneficial way, analysis of label dependence is often seen as a fundamental step, separate and prior to constructing a classifier. Some methods invest up to hundreds of times more computational effort in building dependency models, than training the final classifier itself. We extend some recent discussion in the literature and provide a deeper analysis, namely, developing the view that label dependence is often introduced by an inadequate base classifier, rather than being inherent to the data or underlying concept; showing how even an exhaustive analysis of label dependence may not lead to an optimal classification structure. Viewing labels as additional features (a transformation of the input), we create neural-network inspired novel methods that remove the emphasis of a prior dependency structure. Our methods take an important advantage particular to multi-label data: they leverage labels to create effective units in middle layers, rather than learning these units from scratch in an unsupervised fashion with gradient-based methods. Results are promising. The methods we propose perform competitively, and also have very important qualities of scalability.},
archivePrefix = {arXiv},
arxivId = {1503.09022},
author = {Read, Jesse and Hollm{\'{e}}n, Jaakko},
eprint = {1503.09022},
file = {:home/jan/Documents/Mendeley Desktop/Read, Hollm{\'{e}}n/Unknown/Read, Hollm{\'{e}}n - Unknown - Multi-label Classification using Labels as Hidden Nodes.pdf:pdf},
keywords = {meta-labels,multi-label classification,neural net-,problem transformation},
pages = {1--23},
title = {{Multi-label Classification using Labels as Hidden Nodes}},
url = {https://arxiv.org/pdf/1503.09022.pdf http://arxiv.org/abs/1503.09022},
year = {2015}
}
@article{Roth2007,
abstract = {BACKGROUND: We develop a probabilistic model for combining kernel matrices to predict the function of proteins. It extends previous approaches in that it can handle multiple labels which naturally appear in the context of protein function.$\backslash$n$\backslash$nRESULTS: Explicit modeling of multilabels significantly improves the capability of learning protein function from multiple kernels. The performance and the interpretability of the inference model are further improved by simultaneously predicting the subcellular localization of proteins and by combining pairwise classifiers to consistent class membership estimates.$\backslash$n$\backslash$nCONCLUSION: For the purpose of functional prediction of proteins, multilabels provide valuable information that should be included adequately in the training process of classifiers. Learning of functional categories gains from co-prediction of subcellular localization. Pairwise separation rules allow very detailed insights into the relevance of different measurements like sequence, structure, interaction data, or expression data. A preliminary version of the software can be downloaded from http://www.inf.ethz.ch/personal/vroth/KernelHMM/.},
author = {Roth, Volker and Fischer, Bernd},
doi = {10.1186/1471-2105-8-S2-S12},
file = {:home/jan/Documents/Mendeley Desktop/Roth, Fischer/BMC bioinformatics/Roth, Fischer - 2007 - Improved functional prediction of proteins by learning kernel combinations in multilabel settings.pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Algorithms,Artificial Intelligence,Computer Simulation,Fungal Proteins,Fungal Proteins: chemistry,Fungal Proteins: classification,Fungal Proteins: metabolism,Models, Biological,Sequence Analysis, Protein,Sequence Analysis, Protein: methods,Signal Transduction,Signal Transduction: physiology,Structure-Activity Relationship,Yeasts,Yeasts: metabolism},
pages = {S12},
pmid = {17493250},
title = {{Improved functional prediction of proteins by learning kernel combinations in multilabel settings.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1892070{\&}tool=pmcentrez{\&}rendertype=abstract},
volume = {8 Suppl 2},
year = {2007}
}
@article{Dembczynski,
abstract = {The aim of this paper is to elaborate on the important issue of label dependence in multi-label classification (MLC). Looking at the problem from a statistical perspective, we claim that two different types of label depen-dence should be distinguished, namely con-ditional and unconditional. We formally ex-plain the differences and connections between both types of dependence and illustrate them by means of simple examples. Moreover, we given an overview of state-of-the-art algo-rithms for MLC and categorize them accord-ing to the type of label dependence they seek to capture.},
author = {Dembczynski, Krzysztof and Waegeman, Willem and Cheng, Weiwei and H{\"{u}}llermeier, Eyke},
file = {:home/jan/Documents/Mendeley Desktop/Dembczynski et al/Unknown/Dembczynski - 2010 - On Label Dependence in Multi-Label Classification.pdf:pdf},
title = {{On Label Dependence in Multi-Label Classification}}
}
@inproceedings{Gu2011,
abstract = {Multi-label learning studies the problem where each instance is associated with a set of labels. There are two challenges in multi-label learning: (1) the labels are interdependent and correlated, and (2) the data are of high dimensionality. In this paper, we aim to tackle these challenges in one shot. In particular, we propose to learn the label correlation and do feature selection simultaneously. We introduce a matrix-variate Normal prior distribution on the weight vectors of the classifier to model the label correlation. Our goal is to find a subset of features, based on which the label correlation regularized loss of label ranking is minimized. The resulting multi-label feature selection problem is a mixed integer programming, which is reformulated as quadratically constrained linear programming (QCLP). It can be solved by cutting plane algorithm, in each iteration of which a minimax optimization problem is solved by dual coordinate descent and projected sub-gradient descent alternatively. Experiments on benchmark data sets illustrate that the proposed methods outperform single-label feature selection method and many other state-of-the-art multi-label learning methods. {\textcopyright} 2011 ACM.},
author = {Gu, Quanquan and Li, Zhenhui and Han, Jiawei},
booktitle = {Proceedings of the 20th ACM international conference on Information and knowledge management - CIKM '11},
doi = {10.1145/2063576.2063734},
file = {:home/jan/Documents/Mendeley Desktop/Gu, Li, Han/Proceedings of the 20th ACM international conference on Information and knowledge management - CIKM '11/Gu, Li, Han - Unknown - Correlated Multi-Label Feature Selection.pdf:pdf},
isbn = {9781450307178},
keywords = {cutting plane,dual coordinate descent,feature selection,label correlation,multi-label learning},
pages = {1087},
title = {{Correlated multi-label feature selection}},
url = {http://delivery.acm.org.ez.sun.ac.za/10.1145/2070000/2063734/p1087-gu.pdf?ip=146.232.129.75{\&}id=2063734{\&}acc=ACTIVE SERVICE{\&}key=646D7B17E601A2A5.C011CE1E941E2524.4D4702B0C3E38B35.4D4702B0C3E38B35{\&}CFID=744899709{\&}CFTOKEN=20823743{\&}{\_}{\_}acm{\_}{\_}=1490876009{\_}2f58a46477},
year = {2011}
}
@article{Lewis2004,
abstract = {Reuters Corpus Volume I (RCV1) is an archive of over 800,000 manually categorized newswire stories recently made available by Reuters, Ltd. for research purposes. Use of this data for research on text categorization requires a detailed understanding of the real world constraints under which the data was produced. Drawing on interviews with Reuters personnel and access to Reuters doc-umentation, we describe the coding policy and quality control procedures used in producing the RCV1 data, the intended semantics of the hierarchical category taxonomies, and the corrections necessary to remove errorful data. We refer to the original data as RCV1-v1, and the corrected data as RCV1-v2. We benchmark several widely used supervised learning methods on RCV1-v2, illus-trating the collection's properties, suggesting new directions for research, and providing baseline results for future studies. We make available detailed, per-category experimental results, as well as corrected versions of the category assignments and taxonomy structures, via online appendices.},
author = {Lewis, David D and Yang, Yiming and Rose, Tony G and Li, Fan and Lewis, Fan Li},
file = {:home/jan/Documents/Mendeley Desktop/Lewis et al/Journal of Machine Learning Research/Lewis et al. - 2004 - RCV1 A New Benchmark Collection for Text Categorization Research.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {Rocchio,SCut,SCutFBR,SVMs,applications,automated indexing,controlled vocabulary indexing,effectiveness mea-sures,evaluation,feature selection,k-NN,methodology,multiclass,multilabel,nearest neighbor,news articles,operational systems,support vector machines,term weighting,test collection,text classification,thresholding},
pages = {361--397},
title = {{RCV1: A New Benchmark Collection for Text Categorization Research}},
url = {http://www.jmlr.org/papers/volume5/lewis04a/lewis04a.pdf},
volume = {5},
year = {2004}
}
@article{Lo2013,
abstract = {—Label powerset (LP) method is one category of multi-label learning algorithm. This paper presents a basis expansions model for multi-label classification, where a basis function is a LP classifier trained on a random k-labelset. The expansion coefficients are learned to minimize the global error between the prediction and the ground truth. We derive an analytic solution to learn the coefficients efficiently. We further extend this model to handle the cost-sensitive multi-label classification problem, and apply it in social tagging to handle the issue of the noisy training set by treating the tag counts as the misclassification costs. We have conducted experiments on several benchmark datasets and compared our method with other state-of-the-art multi-label learning methods. Experimental results on both multi-label classification and cost-sensitive social tagging demonstrate that our method has better performance than other methods.},
author = {Lo, Hung-Yi and Lin, Shou-De and Wang, Hsin-Min},
doi = {10.1109/TKDE.2013.112},
file = {:home/jan/Documents/Mendeley Desktop/Lo, Lin, Wang/IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING/Lo, Lin, Wang - 2013 - Generalized k-Labelsets Ensemble for Multi-Label and Cost-Sensitive Classification.pdf:pdf},
journal = {IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING},
keywords = {Index Terms—Multi-label classification,cost-sensitive learning,ensemble method,hypergraph,labelset,social tag,tag count},
number = {X},
title = {{Generalized k-Labelsets Ensemble for Multi-Label and Cost-Sensitive Classification}},
year = {2013}
}
@article{Rice2013,
author = {Rice, Douglas R. and Zorn, Christopher},
file = {:home/jan/Documents/Mendeley Desktop/Rice, Zorn/Proceedings of NDATAD/Rice, Zorn - 2013 - Corpus-Based Dictionaries for Sentiment Analysis of Specialized Vocabularies.pdf:pdf},
journal = {Proceedings of NDATAD},
keywords = {Sentiment Analysis,Sentiment Dictionaries},
mendeley-tags = {Sentiment Analysis,Sentiment Dictionaries},
pages = {1--17},
title = {{Corpus-Based Dictionaries for Sentiment Analysis of Specialized Vocabularies}},
year = {2013}
}
@article{Tsoumakasb,
abstract = {Binary relevance (BR) learns a single binary model for each different label of multi-label data. It has linear complexity with respect to the number of labels, but does not take into account label correlations and may fail to accurately predict label combinations and rank labels according to relevance with a new instance. Stacking the models of BR in order to learn a model that associates their output to the true value of each label is a way to alleviate this problem. In this paper we propose the pruning of the models participating in the stacking process, by explicitly measuring the degree of label correlation using the phi coefficient. Exploratory analysis of phi shows that the correlations detected are meaningful and useful. Empirical evaluation of the pruning approach shows that it leads to substantial reduction of the computational cost of stacking and occasional improvements in predictive performance.},
author = {Tsoumakas, Grigorios and Dimou, Anastasios and Spyromitros, Eleftherios and Mezaris, Vasileios and Kompatsiaris, Ioannis and Vlahavas, Ioannis},
file = {:home/jan/Documents/Mendeley Desktop/Tsoumakas et al/Proceedings of the Workshop on Learning from Multi-Label Data (MLD'09)/Tsoumakas et al. - 2009 - Correlation-based pruning of stacked binary relevance models for multi-label learning.pdf:pdf},
issn = {1475-925X},
journal = {Proceedings of the Workshop on Learning from Multi-Label Data (MLD'09)},
pages = {101--116},
title = {{Correlation-based pruning of stacked binary relevance models for multi-label learning}},
url = {http://www.ecmlpkdd2009.net/wp-content/uploads/2008/09/learning-from-multi-label-data.pdf{\#}page=102},
year = {2009}
}
@article{Pachet2009,
abstract = {—This paper addresses the problem of automatically ex-tracting perceptive information from acoustic signals, in a super-vised classification context. Global labels, i.e., atomic information describing a music title in its entirety, such as its genre, mood, main instruments, or type of vocals, are entered by humans. Classifiers are trained to map audio features to these labels. However, the per-formances of these classifiers on individual labels are rarely satis-factory. In the case we have to predict several labels simultane-ously, we introduce a correction scheme to improve these perfor-mances. In this scheme—an instance of the classifier fusion par-adigm—an extra layer of classifiers is built to exploit redundan-cies between labels and correct some of the errors coming from the individual acoustic classifiers. We describe a series of experi-ments aiming at validating this approach on a large-scale database of music and metadata (about 30 000 titles and 600 labels per title). The experiments show that the approach brings statistically signif-icant improvements.},
author = {Pachet, Fran{\c{c}}ois and Roy, Pierre},
doi = {10.1109/TASL.2008.2008734},
file = {:home/jan/Documents/Mendeley Desktop/Pachet, Roy/IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING/Pachet, Roy - 2009 - Improving Multilabel Analysis of Music Titles A Large-Scale Validation of the Correction Approach.pdf:pdf},
journal = {IEEE TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING},
keywords = {Index Terms—Feature extraction,learning systems,music,pat-tern classification},
number = {2},
title = {{Improving Multilabel Analysis of Music Titles: A Large-Scale Validation of the Correction Approach}},
volume = {17},
year = {2009}
}
@article{Elisseeff,
abstract = {This article presents a Support Vector Machine (SVM) like learning sys-tem to handle multi-label problems. Such problems are usually decom-posed into many two-class problems but the expressive power of such a system can be weak [5, 7]. We explore a new direct approach. It is based on a large margin ranking system that shares a lot of common proper-ties with SVMs. We tested it on a Yeast gene functional classification problem with positive results.},
author = {Elisseeff, Andr{\'{e}} and Weston, Jason},
file = {:home/jan/Documents/Mendeley Desktop/Elisseeff, Weston/Unknown/Elisseeff, Weston - Unknown - A kernel method for multi-labelled classification.pdf:pdf},
title = {{A kernel method for multi-labelled classification}},
url = {https://pdfs.semanticscholar.org/e925/33e4adca54e31d7b3726088469b493c2282e.pdf?{\_}ga=1.194163567.379343330.1490351020},
year = {2001}
}
@article{Qiao2017,
abstract = {JID: NEUCOM [m5G; February 14, 2017;20:37 ] Neurocomputing 0 0 0 (2017) 1–7 a b s t r a c t An instance is often represented from different aspects (views or modalities), which leads to high-dimensional features and even multiple labels. In this paper, we focus on the feature selection problem in multi-label classification, for which a trivial solution is handling the labels dividedly. Obviously, such a scheme may not work well by leaving the label relationship out of consideration. Recently, several re-search works conduct feature selection directly under a multi-label framework by implicitly or explicitly modeling label relationship. However, these works assume that all labels share the same feature subset or subspace, which is not reasonable enough for some scenarios since different labels tend to convey different semantics. To address this problem, we develop a novel approach in this paper to select label-dependent features for multi-label classification. Specifically, we (1) formulate a convex model based on a more general and practical assumption that different labels convey different semantics with specific fea-tures; (2) design an alternating optimization algorithm based on Nesterov's method and L 1 -ball projection for efficiently finding the optimal solution, which can realize multi-label classification, feature selection, and label relationship estimation simultaneously. Finally, experiments on publicly available datasets show that the proposed algorithm achieves better performance than several related methods.},
author = {Qiao, Lishan and Zhang, Limei and Sun, Zhonggui and Liu, Xueyan},
doi = {10.1016/j.neucom.2016.08.122},
file = {:home/jan/Documents/Mendeley Desktop/Qiao et al/Neurocomputing/Qiao et al. - 2016 - ARTICLE IN PRESS Selecting label-dependent features for multi-label classification.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Feature selection,Label dependency,Multi-label classification,Nesterov's method},
title = {{Selecting Label-dependent Features for Multi-label Classification}},
url = {http://ac.els-cdn.com.ez.sun.ac.za/S0925231217302321/1-s2.0-S0925231217302321-main.pdf?{\_}tid=accb469e-15f9-11e7-ba35-00000aab0f27{\&}acdnat=1490954955{\_}6f85d9237b965dfa7948a47fde97b589 http://linkinghub.elsevier.com/retrieve/pii/S0925231217302321},
year = {2017}
}
@article{Tsoumakasf,
abstract = {Binary relevance (BR) learns a single binary model for each different label of multi-label data. It has linear complexity with respect to the number of labels, but does not take into account label correlations and may fail to accurately predict label combinations and rank labels according to relevance with a new instance. Stacking the models of BR in order to learn a model that associates their output to the true value of each label is a way to alleviate this problem. In this paper we propose the pruning of the models participating in the stacking process, by explicitly measuring the degree of label correlation using the phi coefficient. Exploratory analysis of phi shows that the correlations detected are meaningful and useful. Empirical evaluation of the pruning approach shows that it leads to substantial reduction of the computational cost of stacking and occasional improvements in predictive performance.},
author = {Tsoumakas, Grigorios and Dimou, Anastasios and Spyromitros, Eleftherios and Mezaris, Vasileios and Kompatsiaris, Ioannis and Vlahavas, Ioannis},
file = {:home/jan/Documents/Mendeley Desktop/Tsoumakas et al/Proceedings of the Workshop on Learning from Multi-Label Data (MLD'09)/Tsoumakas et al. - 2009 - Correlation-based pruning of stacked binary relevance models for multi-label learning.pdf:pdf},
issn = {1475-925X},
journal = {Proceedings of the Workshop on Learning from Multi-Label Data (MLD'09)},
pages = {101--116},
title = {{Correlation-based pruning of stacked binary relevance models for multi-label learning}},
url = {http://lpis.csd.auth.gr/publications/tsoumakas-mld09.pdf http://www.ecmlpkdd2009.net/wp-content/uploads/2008/09/learning-from-multi-label-data.pdf{\#}page=102},
year = {2009}
}
@article{Marchetti2007,
abstract = {By analysing the current structure and the usage patterns of collaborative tagging systems, we can ﬁnd out many im- portant aspects which still need to be improved. Problems related to synonymy, polysemy, diﬀerent lexical forms, mis- pelling errors or alternate spellings, diﬀerent levels of preci- sion and diﬀerent kinds of tag-to-resource association cause inconsistencies and reduce the eﬃciency of content search and the eﬀectiveness of the tag space structuring and orga- nization. They are mainly caused by the lack of semantic information inclusion in the tagging process. We propose a new way to describe resources: the semantic tagging. It allows user to state semantic assertions: each of them ex- presses a deﬁned characteristic of a resource associating it with a concept. We present SemKey, a semantic collabora- tive tagging system, describing its global architecture and functioning along with the most relevant organizational is- sues faced. We explore the adequacy of the support oﬀered by the entries of Wikipedia and WordNet in order to access to and reference concepts.},
annote = {Discusses the limitations of collaborative tagging},
author = {Marchetti, Andrea and Rosella, Marco},
file = {:home/jan/Documents/Mendeley Desktop/Marchetti, Rosella/Proceedings of the 16th international conference on World Wide Web - WWW '07/Marchetti, Rosella - 2007 - SemKey A Semantic Collaborative Tagging System.pdf:pdf},
journal = {Proceedings of the 16th international conference on World Wide Web - WWW '07},
keywords = {19,are,collaborative tagging,in groups with common,interests,resources and tags,resources may be related,semantics,users,users may be connected},
pages = {8--12},
title = {{SemKey : A Semantic Collaborative Tagging System}},
volume = {7},
year = {2007}
}
@article{Boutell2004,
abstract = {In classic pattern recognition problems, classes are mutually exclusive by definition. Classification errors occur when the classes overlap in the feature space. We examine a different situation, occurring when the classes are, by definition, not mutually exclusive. Such problems arise in semantic scene and document classification and in medical diagnosis. We present a framework to handle such problems and apply it to the problem of semantic scene classification, where a natural scene may contain multiple objects such that the scene can be described by multiple class labels (e.g., a field scene with a mountain in the background). Such a problem poses challenges to the classic pattern recognition paradigm and demands a different treatment. We discuss approaches for training and testing in this scenario and introduce new metrics for evaluating individual examples, class recall and precision, and overall accuracy. Experiments show that our methods are suitable for scene classification; furthermore, our work appears to generalize to other classification problems of the same nature. {\textcopyright} 2004 Pattern Recognition Society. Published by Elsevier Ltd. All rights reserved.},
author = {Boutell, Matthew R. and Luo, Jiebo and Shen, Xipeng and Brown, Christopher M.},
doi = {10.1016/j.patcog.2004.03.009},
file = {:home/jan/Documents/Mendeley Desktop/Boutell et al/Pattern Recognition/Boutell et al. - 2004 - Learning multi-label scene classi{\"{y}}cation.pdf:pdf},
isbn = {0031-3203},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Cross-training,Image organization,Image understanding,Jaccard similarity,Multi-label classification,Multi-label evaluation,Multi-label training,Semantic scene classification},
number = {9},
pages = {1757--1771},
title = {{Learning multi-label scene classification}},
volume = {37},
year = {2004}
}
@article{Read,
abstract = {—The area of multi-label classification has rapidly developed in recent years. It has become widely known that the baseline binary relevance approach can easily be outperformed by methods which learn labels together. A number of methods have grown around the label powerset approach, which models label combinations together as class values in a multi-class problem. We describe the label-powerset-based solutions under a general framework of meta-labels and provide some theoret-ical justification for this framework which has been lacking; explaining how meta-labels essentially allow a random projection into a space where non-linearities can easily be tackled with established linear learning algorithms. The proposed framework enables comparison and combination of related approaches to different multi-label problems. We present a novel model in the framework and evaluate it empirically against several high-performing methods, with respect to predictive performance and scalability, on a number of datasets and evaluation metrics. This deployment obtains competitive accuracy for a fraction of the computation required by the current meta-label methods for multi-label classification.},
author = {Read, Jesse and Puurula, Antti and Bifet, Albert},
doi = {10.1109/ICDM.2014.38},
file = {:home/jan/Documents/Mendeley Desktop/Read, Puurula, Bifet/2014 IEEE International Conference on Data Mining/Read, Puurula, Bifet - 2014 - Multi-label Classification with Meta-Labels.pdf:pdf},
isbn = {978-1-4799-4302-9},
journal = {2014 IEEE International Conference on Data Mining},
pages = {941--946},
title = {{Multi-label Classification with Meta-Labels}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7023427},
year = {2014}
}
@article{Furnkranz2008,
abstract = {Label ranking studies the problem of learning a mapping from instances to rank-ings over a predefined set of labels. Hitherto existing approaches to label ranking implicitly operate on an underlying (utility) scale which is not calibrated in the sense that it lacks a natural zero point. We propose a suitable extension of label ranking that incorporates the calibrated scenario and substantially extends the expressive power of these approaches. In particular, our extension suggests a conceptually novel technique for extending the common learning by pairwise comparison approach to the multilabel scenario, a setting previously not being amenable to the pairwise decomposition technique. The key idea of the approach is to introduce an artificial calibration label that, in each example, separates the relevant from the irrelevant labels. We show that this technique can be viewed as a combination of pair-wise preference learning and the conventional relevance classification technique, where a separate classifier is trained to predict whether a label is relevant or not. Empirical results in the area of text categorization, image classification and gene analysis underscore the merits of the calibrated model in comparison to state-of-the-art multilabel learning methods.},
author = {F{\"{u}}rnkranz, Johannes and H{\"{u}}llermeier, Eyke and {Loza Menc{\'{i}}a}, Eneldo and Brinker, Klaus and {Fawcett F{\"{u}}rnkranz}, Tom J and {Loza Menc{\'{i}}a}, E and H{\"{u}}llermeier, E and Brinker, K},
doi = {10.1007/s10994-008-5064-8},
file = {:home/jan/Documents/Mendeley Desktop/F{\"{u}}rnkranz et al/Mach Learn/F{\"{u}}rnkranz et al. - 2008 - Multilabel classification via calibrated label ranking.pdf:pdf},
journal = {Mach Learn},
keywords = {Multi-label classification {\textperiodcentered},Preference learning {\textperiodcentered},Ranking},
number = {73},
pages = {133--153},
title = {{Multilabel classification via calibrated label ranking}},
url = {http://download.springer.com/static/pdf/878/art{\%}253A10.1007{\%}252Fs10994-008-5064-8.pdf?originUrl=http{\%}3A{\%}2F{\%}2Flink.springer.com{\%}2Farticle{\%}2F10.1007{\%}2Fs10994-008-5064-8{\&}token2=exp=1490609445{~}acl={\%}2Fstatic{\%}2Fpdf{\%}2F878{\%}2Fart{\%}25253A10.1007{\%}25252Fs10994-008-506},
volume = {73},
year = {2008}
}
@article{Tsoumakas2007,
author = {Tsoumakas, Grigorios and Katakis, Ioannis},
doi = {10.4018/978-1-60566-058-5.ch021},
file = {:home/jan/Documents/Mendeley Desktop/Tsoumakas, Katakis/Database Technologies/Tsoumakas, Katakis - 2007 - Multi-Label Classification.pdf:pdf},
isbn = {978-1-4244-1065-1},
journal = {Database Technologies},
keywords = {Multi-Label,data forecasting,data mining,decision models,knowledge discovery,text data-},
mendeley-tags = {Multi-Label},
number = {3},
pages = {309--319},
title = {{Multi-Label Classification}},
url = {http://services.igi-global.com/resolvedoi/resolve.aspx?doi=10.4018/978-1-60566-058-5.ch021},
volume = {3},
year = {2007}
}
